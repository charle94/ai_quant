#!/usr/bin/env python3
"""
Rill Dataé›†æˆæ¨¡å— - å°†ç»©æ•ˆåˆ†ææ•°æ®å¯¼å…¥åˆ°Rill Dataçœ‹æ¿
"""
import json
import os
import sys
from datetime import datetime, timedelta
from pathlib import Path
import logging
import pandas as pd
import yaml
from typing import Dict, List, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from performance_analysis.performance_analyzer import PerformanceAnalyzer, create_sample_performance_data

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RillDataIntegration:
    """Rill Dataé›†æˆç±»"""
    
    def __init__(self, project_path: str = "/workspace/rill_project"):
        self.project_path = Path(project_path)
        self.data_path = self.project_path / "data"
        self.models_path = self.project_path / "models"
        self.dashboards_path = self.project_path / "dashboards"
        
        # åˆ›å»ºé¡¹ç›®ç›®å½•ç»“æ„
        self._init_project_structure()
    
    def _init_project_structure(self):
        """åˆå§‹åŒ–Rillé¡¹ç›®ç»“æ„"""
        logger.info("åˆå§‹åŒ–Rill Dataé¡¹ç›®ç»“æ„...")
        
        # åˆ›å»ºç›®å½•
        self.project_path.mkdir(exist_ok=True)
        self.data_path.mkdir(exist_ok=True)
        self.models_path.mkdir(exist_ok=True)
        self.dashboards_path.mkdir(exist_ok=True)
        
        # åˆ›å»ºrill.yamlé…ç½®æ–‡ä»¶
        self._create_rill_config()
        
        logger.info(f"Rillé¡¹ç›®ç»“æ„å·²åˆ›å»º: {self.project_path}")
    
    def _create_rill_config(self):
        """åˆ›å»ºRillé…ç½®æ–‡ä»¶"""
        config = {
            'compiler': {
                'duckdb_path': str(self.project_path / "stage.db")
            },
            'runtime': {
                'host': '0.0.0.0',
                'port': 9009,
                'instance_id': 'quant_analysis'
            },
            'connectors': [
                {
                    'name': 'performance_db',
                    'type': 'duckdb',
                    'config': {
                        'dsn': str(self.project_path / "performance.db")
                    }
                }
            ]
        }
        
        config_file = self.project_path / "rill.yaml"
        with open(config_file, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
        
        logger.info(f"Rillé…ç½®æ–‡ä»¶å·²åˆ›å»º: {config_file}")
    
    def export_performance_data(self, analyzer: PerformanceAnalyzer) -> Dict[str, str]:
        """å¯¼å‡ºç»©æ•ˆæ•°æ®åˆ°Rillæ ¼å¼"""
        logger.info("å¯¼å‡ºç»©æ•ˆæ•°æ®åˆ°Rillæ ¼å¼...")
        
        exported_files = {}
        
        try:
            # 1. å¯¼å‡ºæ—¥æ”¶ç›Šç‡æ•°æ®
            if analyzer.daily_returns:
                daily_returns_df = pd.DataFrame([
                    {
                        'date': dr.date,
                        'portfolio_value': dr.portfolio_value,
                        'daily_return': dr.daily_return,
                        'cumulative_return': dr.cumulative_return,
                        'benchmark_return': dr.benchmark_return,
                        'excess_return': dr.excess_return
                    }
                    for dr in analyzer.daily_returns
                ])
                
                daily_returns_file = self.data_path / "daily_returns.parquet"
                daily_returns_df.to_parquet(daily_returns_file, index=False)
                exported_files['daily_returns'] = str(daily_returns_file)
                logger.info(f"æ—¥æ”¶ç›Šç‡æ•°æ®å·²å¯¼å‡º: {daily_returns_file}")
            
            # 2. å¯¼å‡ºäº¤æ˜“è®°å½•
            if analyzer.trades:
                trades_df = pd.DataFrame([
                    {
                        'trade_id': trade.id,
                        'timestamp': trade.timestamp,
                        'symbol': trade.symbol,
                        'side': trade.side,
                        'quantity': trade.quantity,
                        'price': trade.price,
                        'commission': trade.commission,
                        'pnl': trade.pnl,
                        'cumulative_pnl': trade.cumulative_pnl
                    }
                    for trade in analyzer.trades
                ])
                
                trades_file = self.data_path / "trades.parquet"
                trades_df.to_parquet(trades_file, index=False)
                exported_files['trades'] = str(trades_file)
                logger.info(f"äº¤æ˜“è®°å½•å·²å¯¼å‡º: {trades_file}")
            
            # 3. å¯¼å‡ºç»©æ•ˆæŒ‡æ ‡
            metrics = analyzer.calculate_performance_metrics()
            metrics_data = {
                'metric_name': [
                    'total_return', 'annualized_return', 'volatility', 'max_drawdown',
                    'sharpe_ratio', 'sortino_ratio', 'calmar_ratio', 'win_rate',
                    'total_trades', 'profit_factor', 'var_95', 'cvar_95'
                ],
                'metric_value': [
                    metrics.total_return, metrics.annualized_return, metrics.volatility,
                    metrics.max_drawdown, metrics.sharpe_ratio, metrics.sortino_ratio,
                    metrics.calmar_ratio, metrics.win_rate, metrics.total_trades,
                    metrics.profit_factor, metrics.var_95, metrics.cvar_95
                ],
                'category': [
                    'returns', 'returns', 'risk', 'risk',
                    'risk_adjusted', 'risk_adjusted', 'risk_adjusted', 'trading',
                    'trading', 'trading', 'risk', 'risk'
                ],
                'updated_at': [datetime.now()] * 12
            }
            
            metrics_df = pd.DataFrame(metrics_data)
            metrics_file = self.data_path / "performance_metrics.parquet"
            metrics_df.to_parquet(metrics_file, index=False)
            exported_files['metrics'] = str(metrics_file)
            logger.info(f"ç»©æ•ˆæŒ‡æ ‡å·²å¯¼å‡º: {metrics_file}")
            
            # 4. å¯¼å‡ºå›æ’¤æ•°æ®
            drawdowns = analyzer.calculate_drawdown_periods()
            if drawdowns:
                drawdown_data = []
                for i, dd in enumerate(drawdowns):
                    drawdown_data.append({
                        'drawdown_id': i + 1,
                        'start_date': dd.start_date,
                        'end_date': dd.end_date,
                        'peak_value': dd.peak_value,
                        'trough_value': dd.trough_value,
                        'drawdown_pct': dd.drawdown_pct,
                        'duration_days': dd.duration_days,
                        'recovered': dd.recovery_date is not None,
                        'recovery_date': dd.recovery_date
                    })
                
                drawdowns_df = pd.DataFrame(drawdown_data)
                drawdowns_file = self.data_path / "drawdowns.parquet"
                drawdowns_df.to_parquet(drawdowns_file, index=False)
                exported_files['drawdowns'] = str(drawdowns_file)
                logger.info(f"å›æ’¤æ•°æ®å·²å¯¼å‡º: {drawdowns_file}")
            
            return exported_files
            
        except Exception as e:
            logger.error(f"å¯¼å‡ºæ•°æ®æ—¶å‡ºé”™: {e}")
            return {}
    
    def create_rill_models(self):
        """åˆ›å»ºRillæ•°æ®æ¨¡å‹"""
        logger.info("åˆ›å»ºRillæ•°æ®æ¨¡å‹...")
        
        # 1. æ—¥æ”¶ç›Šç‡æ¨¡å‹
        daily_returns_model = """
-- Daily Returns Model
-- @materialize: true

SELECT 
    date,
    portfolio_value,
    daily_return,
    cumulative_return,
    benchmark_return,
    excess_return,
    -- è®¡ç®—ç§»åŠ¨å¹³å‡
    AVG(daily_return) OVER (
        ORDER BY date 
        ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
    ) as ma_7_return,
    AVG(daily_return) OVER (
        ORDER BY date 
        ROWS BETWEEN 29 PRECEDING AND CURRENT ROW
    ) as ma_30_return,
    -- è®¡ç®—æ»šåŠ¨æ³¢åŠ¨ç‡
    STDDEV(daily_return) OVER (
        ORDER BY date 
        ROWS BETWEEN 29 PRECEDING AND CURRENT ROW
    ) as rolling_volatility_30d,
    -- è®¡ç®—æ»šåŠ¨å¤æ™®æ¯”ç‡
    AVG(excess_return) OVER (
        ORDER BY date 
        ROWS BETWEEN 29 PRECEDING AND CURRENT ROW
    ) / NULLIF(STDDEV(excess_return) OVER (
        ORDER BY date 
        ROWS BETWEEN 29 PRECEDING AND CURRENT ROW
    ), 0) as rolling_sharpe_30d
FROM read_parquet('data/daily_returns.parquet')
ORDER BY date
"""
        
        model_file = self.models_path / "daily_returns.sql"
        with open(model_file, 'w', encoding='utf-8') as f:
            f.write(daily_returns_model)
        
        # 2. äº¤æ˜“åˆ†ææ¨¡å‹
        trades_model = """
-- Trades Analysis Model
-- @materialize: true

WITH trade_stats AS (
    SELECT 
        *,
        -- æŒ‰æœˆåˆ†ç»„
        DATE_TRUNC('month', timestamp) as trade_month,
        -- æŒ‰å‘¨åˆ†ç»„
        DATE_TRUNC('week', timestamp) as trade_week,
        -- ç›ˆäºåˆ†ç±»
        CASE 
            WHEN pnl > 0 THEN 'Win'
            WHEN pnl < 0 THEN 'Loss'
            ELSE 'Breakeven'
        END as trade_result,
        -- äº¤æ˜“è§„æ¨¡åˆ†ç±»
        CASE 
            WHEN ABS(pnl) > 1000 THEN 'Large'
            WHEN ABS(pnl) > 500 THEN 'Medium'
            ELSE 'Small'
        END as trade_size
    FROM read_parquet('data/trades.parquet')
)

SELECT 
    *,
    -- è®¡ç®—ç´¯è®¡ç»Ÿè®¡
    COUNT(*) OVER (ORDER BY timestamp) as trade_number,
    AVG(pnl) OVER (ORDER BY timestamp ROWS UNBOUNDED PRECEDING) as avg_pnl_cumulative,
    SUM(CASE WHEN pnl > 0 THEN 1 ELSE 0 END) OVER (ORDER BY timestamp ROWS UNBOUNDED PRECEDING) as wins_cumulative,
    COUNT(*) OVER (ORDER BY timestamp ROWS UNBOUNDED PRECEDING) as total_trades_cumulative,
    -- èƒœç‡
    CAST(SUM(CASE WHEN pnl > 0 THEN 1 ELSE 0 END) OVER (ORDER BY timestamp ROWS UNBOUNDED PRECEDING) AS FLOAT) / 
    COUNT(*) OVER (ORDER BY timestamp ROWS UNBOUNDED PRECEDING) as win_rate_cumulative
FROM trade_stats
ORDER BY timestamp
"""
        
        model_file = self.models_path / "trades_analysis.sql"
        with open(model_file, 'w', encoding='utf-8') as f:
            f.write(trades_model)
        
        # 3. ç»©æ•ˆæŒ‡æ ‡æ¨¡å‹
        metrics_model = """
-- Performance Metrics Model
-- @materialize: true

SELECT 
    metric_name,
    metric_value,
    category,
    updated_at,
    -- æ·»åŠ æ ¼å¼åŒ–æ˜¾ç¤º
    CASE 
        WHEN metric_name IN ('total_return', 'annualized_return', 'volatility', 'max_drawdown', 'win_rate', 'var_95', 'cvar_95') 
        THEN CONCAT(ROUND(metric_value * 100, 2), '%')
        ELSE ROUND(metric_value, 2)::VARCHAR
    END as formatted_value,
    -- æ·»åŠ åŸºå‡†æ¯”è¾ƒ
    CASE 
        WHEN metric_name = 'sharpe_ratio' AND metric_value > 1 THEN 'Good'
        WHEN metric_name = 'sharpe_ratio' AND metric_value > 0.5 THEN 'Fair'
        WHEN metric_name = 'sharpe_ratio' THEN 'Poor'
        WHEN metric_name = 'win_rate' AND metric_value > 0.6 THEN 'Excellent'
        WHEN metric_name = 'win_rate' AND metric_value > 0.5 THEN 'Good'
        WHEN metric_name = 'win_rate' THEN 'Needs Improvement'
        WHEN metric_name = 'max_drawdown' AND metric_value < 0.1 THEN 'Low Risk'
        WHEN metric_name = 'max_drawdown' AND metric_value < 0.2 THEN 'Medium Risk'
        WHEN metric_name = 'max_drawdown' THEN 'High Risk'
        ELSE 'Neutral'
    END as performance_grade
FROM read_parquet('data/performance_metrics.parquet')
"""
        
        model_file = self.models_path / "performance_metrics.sql"
        with open(model_file, 'w', encoding='utf-8') as f:
            f.write(metrics_model)
        
        logger.info("Rillæ•°æ®æ¨¡å‹å·²åˆ›å»º")
    
    def create_rill_dashboards(self):
        """åˆ›å»ºRillçœ‹æ¿"""
        logger.info("åˆ›å»ºRillçœ‹æ¿...")
        
        # 1. ä¸»çœ‹æ¿é…ç½®
        main_dashboard = {
            'kind': 'rill.runtime.v1.Dashboard',
            'metadata': {
                'name': 'quant_performance_overview'
            },
            'spec': {
                'title': 'é‡åŒ–ç­–ç•¥ç»©æ•ˆæ¦‚è§ˆ',
                'model': 'daily_returns',
                'default_time_range': 'P30D',
                'available_time_ranges': [
                    {'range': 'P7D', 'label': 'æœ€è¿‘7å¤©'},
                    {'range': 'P30D', 'label': 'æœ€è¿‘30å¤©'},
                    {'range': 'P90D', 'label': 'æœ€è¿‘90å¤©'},
                    {'range': 'P1Y', 'label': 'æœ€è¿‘1å¹´'}
                ],
                'time_column': 'date',
                'measures': [
                    {
                        'name': 'portfolio_value',
                        'label': 'ç»„åˆä»·å€¼',
                        'expression': 'portfolio_value',
                        'format_preset': 'currency_usd'
                    },
                    {
                        'name': 'daily_return',
                        'label': 'æ—¥æ”¶ç›Šç‡',
                        'expression': 'daily_return',
                        'format_preset': 'percentage_2'
                    },
                    {
                        'name': 'cumulative_return',
                        'label': 'ç´¯è®¡æ”¶ç›Šç‡',
                        'expression': 'cumulative_return',
                        'format_preset': 'percentage_2'
                    },
                    {
                        'name': 'rolling_sharpe_30d',
                        'label': '30æ—¥æ»šåŠ¨å¤æ™®æ¯”ç‡',
                        'expression': 'rolling_sharpe_30d',
                        'format_preset': 'number_2'
                    }
                ],
                'dimensions': [
                    {
                        'name': 'date',
                        'label': 'æ—¥æœŸ',
                        'expression': 'date',
                        'format_preset': 'date'
                    }
                ]
            }
        }
        
        dashboard_file = self.dashboards_path / "main_dashboard.yaml"
        with open(dashboard_file, 'w', encoding='utf-8') as f:
            yaml.dump(main_dashboard, f, default_flow_style=False, allow_unicode=True)
        
        # 2. äº¤æ˜“åˆ†æçœ‹æ¿
        trading_dashboard = {
            'kind': 'rill.runtime.v1.Dashboard',
            'metadata': {
                'name': 'trading_analysis'
            },
            'spec': {
                'title': 'äº¤æ˜“åˆ†æçœ‹æ¿',
                'model': 'trades_analysis',
                'default_time_range': 'P30D',
                'time_column': 'timestamp',
                'measures': [
                    {
                        'name': 'pnl',
                        'label': 'ç›ˆäº',
                        'expression': 'pnl',
                        'format_preset': 'currency_usd'
                    },
                    {
                        'name': 'cumulative_pnl',
                        'label': 'ç´¯è®¡ç›ˆäº',
                        'expression': 'cumulative_pnl',
                        'format_preset': 'currency_usd'
                    },
                    {
                        'name': 'win_rate_cumulative',
                        'label': 'ç´¯è®¡èƒœç‡',
                        'expression': 'win_rate_cumulative',
                        'format_preset': 'percentage_2'
                    },
                    {
                        'name': 'trade_count',
                        'label': 'äº¤æ˜“æ¬¡æ•°',
                        'expression': 'COUNT(*)',
                        'format_preset': 'number_0'
                    }
                ],
                'dimensions': [
                    {
                        'name': 'symbol',
                        'label': 'äº¤æ˜“å¯¹',
                        'expression': 'symbol'
                    },
                    {
                        'name': 'side',
                        'label': 'äº¤æ˜“æ–¹å‘',
                        'expression': 'side'
                    },
                    {
                        'name': 'trade_result',
                        'label': 'äº¤æ˜“ç»“æœ',
                        'expression': 'trade_result'
                    },
                    {
                        'name': 'trade_size',
                        'label': 'äº¤æ˜“è§„æ¨¡',
                        'expression': 'trade_size'
                    }
                ]
            }
        }
        
        dashboard_file = self.dashboards_path / "trading_dashboard.yaml"
        with open(dashboard_file, 'w', encoding='utf-8') as f:
            yaml.dump(trading_dashboard, f, default_flow_style=False, allow_unicode=True)
        
        # 3. é£é™©åˆ†æçœ‹æ¿
        risk_dashboard = {
            'kind': 'rill.runtime.v1.Dashboard',
            'metadata': {
                'name': 'risk_analysis'
            },
            'spec': {
                'title': 'é£é™©åˆ†æçœ‹æ¿',
                'model': 'performance_metrics',
                'measures': [
                    {
                        'name': 'metric_value',
                        'label': 'æŒ‡æ ‡å€¼',
                        'expression': 'metric_value'
                    }
                ],
                'dimensions': [
                    {
                        'name': 'metric_name',
                        'label': 'æŒ‡æ ‡åç§°',
                        'expression': 'metric_name'
                    },
                    {
                        'name': 'category',
                        'label': 'æŒ‡æ ‡ç±»åˆ«',
                        'expression': 'category'
                    },
                    {
                        'name': 'performance_grade',
                        'label': 'è¡¨ç°è¯„çº§',
                        'expression': 'performance_grade'
                    }
                ]
            }
        }
        
        dashboard_file = self.dashboards_path / "risk_dashboard.yaml"
        with open(dashboard_file, 'w', encoding='utf-8') as f:
            yaml.dump(risk_dashboard, f, default_flow_style=False, allow_unicode=True)
        
        logger.info("Rillçœ‹æ¿é…ç½®å·²åˆ›å»º")
    
    def create_startup_script(self):
        """åˆ›å»ºRillå¯åŠ¨è„šæœ¬"""
        startup_script = f"""#!/bin/bash

# Rill Data å¯åŠ¨è„šæœ¬

echo "ğŸš€ å¯åŠ¨Rill Dataçœ‹æ¿æœåŠ¡..."

# æ£€æŸ¥Rillæ˜¯å¦å·²å®‰è£…
if ! command -v rill &> /dev/null; then
    echo "âš ï¸  Rillæœªå®‰è£…ï¼Œæ­£åœ¨å®‰è£…..."
    
    # å®‰è£…Rill (æ ¹æ®ç³»ç»Ÿé€‰æ‹©åˆé€‚çš„å®‰è£…æ–¹æ³•)
    if [[ "$OSTYPE" == "darwin"* ]]; then
        # macOS
        brew install rilldata/tap/rill
    else
        # Linux
        curl -s https://cdn.rilldata.com/install.sh | bash
    fi
fi

# è¿›å…¥é¡¹ç›®ç›®å½•
cd {self.project_path}

# å¯åŠ¨Rillå¼€å‘æœåŠ¡å™¨
echo "ğŸ“Š å¯åŠ¨Rillçœ‹æ¿ï¼Œè®¿é—®åœ°å€: http://localhost:9009"
echo "ğŸ”„ æŒ‰ Ctrl+C åœæ­¢æœåŠ¡"

rill start --verbose
"""
        
        script_file = self.project_path / "start_rill.sh"
        with open(script_file, 'w', encoding='utf-8') as f:
            f.write(startup_script)
        
        # è®¾ç½®æ‰§è¡Œæƒé™
        os.chmod(script_file, 0o755)
        
        logger.info(f"Rillå¯åŠ¨è„šæœ¬å·²åˆ›å»º: {script_file}")
    
    def integrate_with_performance_analyzer(self, analyzer: PerformanceAnalyzer):
        """ä¸ç»©æ•ˆåˆ†æå™¨é›†æˆ"""
        logger.info("å¼€å§‹é›†æˆç»©æ•ˆåˆ†ææ•°æ®åˆ°Rill Data...")
        
        try:
            # 1. å¯¼å‡ºæ•°æ®
            exported_files = self.export_performance_data(analyzer)
            
            if not exported_files:
                logger.error("æ•°æ®å¯¼å‡ºå¤±è´¥")
                return False
            
            # 2. åˆ›å»ºæ¨¡å‹
            self.create_rill_models()
            
            # 3. åˆ›å»ºçœ‹æ¿
            self.create_rill_dashboards()
            
            # 4. åˆ›å»ºå¯åŠ¨è„šæœ¬
            self.create_startup_script()
            
            logger.info("Rill Dataé›†æˆå®Œæˆï¼")
            logger.info(f"é¡¹ç›®è·¯å¾„: {self.project_path}")
            logger.info(f"å¯åŠ¨å‘½ä»¤: {self.project_path}/start_rill.sh")
            logger.info("çœ‹æ¿è®¿é—®åœ°å€: http://localhost:9009")
            
            return True
            
        except Exception as e:
            logger.error(f"é›†æˆè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return False
    
    def update_data(self, analyzer: PerformanceAnalyzer):
        """æ›´æ–°Rill Dataä¸­çš„æ•°æ®"""
        logger.info("æ›´æ–°Rill Dataæ•°æ®...")
        
        try:
            # é‡æ–°å¯¼å‡ºæ•°æ®
            exported_files = self.export_performance_data(analyzer)
            
            if exported_files:
                logger.info("æ•°æ®æ›´æ–°æˆåŠŸ")
                return True
            else:
                logger.error("æ•°æ®æ›´æ–°å¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"æ›´æ–°æ•°æ®æ—¶å‡ºé”™: {e}")
            return False

def main():
    """æµ‹è¯•Rill Dataé›†æˆ"""
    print("ğŸ§ª å¼€å§‹Rill Dataé›†æˆæµ‹è¯•...")
    
    # åˆ›å»ºç¤ºä¾‹ç»©æ•ˆæ•°æ®
    analyzer = create_sample_performance_data()
    print(f"ğŸ“Š åˆ›å»ºäº† {len(analyzer.daily_returns)} å¤©çš„ç»©æ•ˆæ•°æ®")
    print(f"ğŸ“Š åˆ›å»ºäº† {len(analyzer.trades)} ç¬”äº¤æ˜“è®°å½•")
    
    # åˆ›å»ºRillé›†æˆå®ä¾‹
    rill_integration = RillDataIntegration()
    
    # æ‰§è¡Œé›†æˆ
    success = rill_integration.integrate_with_performance_analyzer(analyzer)
    
    if success:
        print("\nğŸ‰ Rill Dataé›†æˆæˆåŠŸï¼")
        print(f"ğŸ“ é¡¹ç›®è·¯å¾„: {rill_integration.project_path}")
        print("ğŸš€ å¯åŠ¨æ­¥éª¤:")
        print("   1. å®‰è£…Rill Data (å¦‚æœå°šæœªå®‰è£…)")
        print("   2. è¿è¡Œå¯åŠ¨è„šæœ¬:")
        print(f"      cd {rill_integration.project_path}")
        print("      ./start_rill.sh")
        print("   3. è®¿é—®çœ‹æ¿: http://localhost:9009")
        print("\nğŸ“Š å¯ç”¨çœ‹æ¿:")
        print("   â€¢ é‡åŒ–ç­–ç•¥ç»©æ•ˆæ¦‚è§ˆ")
        print("   â€¢ äº¤æ˜“åˆ†æçœ‹æ¿")
        print("   â€¢ é£é™©åˆ†æçœ‹æ¿")
    else:
        print("âŒ Rill Dataé›†æˆå¤±è´¥")
    
    return success

if __name__ == "__main__":
    main()