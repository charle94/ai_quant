[0m15:26:19.841838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97fc323770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97fd985a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97fb193d90>]}


============================== 15:26:19.844214 | 2f9e1454-b404-41fc-b1e0-45b93a99f7c8 ==============================
[0m15:26:19.844214 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:26:19.844552 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'static_parser': 'True', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'quiet': 'False', 'log_format': 'default', 'log_cache_events': 'False', 'profiles_dir': '/workspace/dbt_project', 'cache_selected_only': 'False', 'use_colors': 'True', 'target_path': 'None', 'introspect': 'True', 'write_json': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'warn_error': 'None', 'partial_parse': 'True', 'log_path': '/workspace/dbt_project/logs', 'invocation_command': 'dbt seed', 'indirect_selection': 'eager', 'empty': 'None', 'version_check': 'True', 'fail_fast': 'False', 'no_print': 'None'}
[0m15:26:19.978648 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2f9e1454-b404-41fc-b1e0-45b93a99f7c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97fbeaa650>]}
[0m15:26:20.020083 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2f9e1454-b404-41fc-b1e0-45b93a99f7c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97fb28ebe0>]}
[0m15:26:20.021116 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:26:20.049913 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:26:20.050328 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m15:26:20.050536 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '2f9e1454-b404-41fc-b1e0-45b93a99f7c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97f7aa4e50>]}
[0m15:26:21.075943 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `accepted_values`. Arguments to generic tests
should be nested under the `arguments` property.`
[0m15:26:21.076257 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '2f9e1454-b404-41fc-b1e0-45b93a99f7c8', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97f6e45220>]}
[0m15:26:21.279041 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2f9e1454-b404-41fc-b1e0-45b93a99f7c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97f617dd30>]}
[0m15:26:21.344765 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:26:21.345874 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:26:21.356022 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2f9e1454-b404-41fc-b1e0-45b93a99f7c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97f62f8ae0>]}
[0m15:26:21.356352 [info ] [MainThread]: Found 15 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:26:21.356587 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2f9e1454-b404-41fc-b1e0-45b93a99f7c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97f6dd3710>]}
[0m15:26:21.358081 [info ] [MainThread]: 
[0m15:26:21.358347 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:26:21.358517 [info ] [MainThread]: 
[0m15:26:21.358797 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:26:21.362031 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features'
[0m15:26:21.376764 [debug] [ThreadPool]: Using duckdb connection "list_quant_features"
[0m15:26:21.377026 [debug] [ThreadPool]: On list_quant_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"quant_features"'
    
  
  
[0m15:26:21.377203 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:26:21.393160 [debug] [ThreadPool]: SQL status: OK in 0.016 seconds
[0m15:26:21.394033 [debug] [ThreadPool]: On list_quant_features: Close
[0m15:26:21.394722 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_quant_features, now create_quant_features_main)
[0m15:26:21.395039 [debug] [ThreadPool]: Creating schema "database: "quant_features"
schema: "main"
"
[0m15:26:21.399138 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:26:21.399391 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='quant_features'
        and type='sqlite'
    
  
[0m15:26:21.399568 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:26:21.400251 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:26:21.401105 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:26:21.401310 [debug] [ThreadPool]: On create_quant_features_main: BEGIN
[0m15:26:21.401604 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:26:21.401769 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:26:21.401926 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
    
        create schema if not exists "quant_features"."main"
    
[0m15:26:21.402211 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:26:21.402722 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:26:21.402913 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:26:21.403093 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:26:21.403425 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:26:21.403604 [debug] [ThreadPool]: On create_quant_features_main: Close
[0m15:26:21.405348 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_quant_features_main, now list_quant_features_main)
[0m15:26:21.409211 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:26:21.409448 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:26:21.409603 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:26:21.410052 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:26:21.410245 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:26:21.410402 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:26:21.416486 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m15:26:21.417267 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:26:21.417847 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:26:21.418070 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:26:21.418894 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2f9e1454-b404-41fc-b1e0-45b93a99f7c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97f6067c20>]}
[0m15:26:21.419398 [debug] [MainThread]: Using duckdb connection "master"
[0m15:26:21.419593 [debug] [MainThread]: On master: BEGIN
[0m15:26:21.419747 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:26:21.420175 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:26:21.420379 [debug] [MainThread]: On master: COMMIT
[0m15:26:21.420533 [debug] [MainThread]: Using duckdb connection "master"
[0m15:26:21.420677 [debug] [MainThread]: On master: COMMIT
[0m15:26:21.420938 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:26:21.421125 [debug] [MainThread]: On master: Close
[0m15:26:21.423527 [debug] [Thread-1 (]: Began running node seed.quant_features.market_data
[0m15:26:21.423932 [debug] [Thread-2 (]: Began running node seed.quant_features.raw_stock_prices
[0m15:26:21.424437 [info ] [Thread-1 (]: 1 of 2 START seed file main.market_data ........................................ [RUN]
[0m15:26:21.424950 [info ] [Thread-2 (]: 2 of 2 START seed file main.raw_stock_prices ................................... [RUN]
[0m15:26:21.425467 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now seed.quant_features.market_data)
[0m15:26:21.426034 [debug] [Thread-2 (]: Acquiring new duckdb connection 'seed.quant_features.raw_stock_prices'
[0m15:26:21.426564 [debug] [Thread-1 (]: Began compiling node seed.quant_features.market_data
[0m15:26:21.426921 [debug] [Thread-2 (]: Began compiling node seed.quant_features.raw_stock_prices
[0m15:26:21.427275 [debug] [Thread-1 (]: Began executing node seed.quant_features.market_data
[0m15:26:21.427579 [debug] [Thread-2 (]: Began executing node seed.quant_features.raw_stock_prices
[0m15:26:21.447246 [debug] [Thread-1 (]: Using duckdb connection "seed.quant_features.market_data"
[0m15:26:21.447517 [debug] [Thread-1 (]: On seed.quant_features.market_data: BEGIN
[0m15:26:21.447701 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:26:21.449650 [debug] [Thread-2 (]: Using duckdb connection "seed.quant_features.raw_stock_prices"
[0m15:26:21.450009 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:26:21.450385 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: BEGIN
[0m15:26:21.450734 [debug] [Thread-1 (]: Using duckdb connection "seed.quant_features.market_data"
[0m15:26:21.451037 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:26:21.451302 [debug] [Thread-1 (]: On seed.quant_features.market_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "seed.quant_features.market_data"} */

    create table "quant_features"."main"."market_data" ("date" date,"symbol" text,"market_cap" integer,"pe_ratio" float8,"dividend_yield" float8,"sector" text)
  
[0m15:26:21.451917 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:26:21.452172 [debug] [Thread-2 (]: Using duckdb connection "seed.quant_features.raw_stock_prices"
[0m15:26:21.452421 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:26:21.452676 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "seed.quant_features.raw_stock_prices"} */

    create table "quant_features"."main"."raw_stock_prices" ("date" date,"symbol" text,"open" float8,"high" float8,"low" float8,"close" float8,"volume" integer)
  
[0m15:26:21.459497 [debug] [Thread-1 (]: Using duckdb connection "seed.quant_features.market_data"
[0m15:26:21.459940 [debug] [Thread-1 (]: On seed.quant_features.market_data: 
          COPY "quant_features"."main"."market_data" FROM '/workspace/dbt_project/seeds/market_data.csv' (FORMAT CSV, HEADER TRUE, DELIMITER ',')
        ...
[0m15:26:21.460315 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:26:21.461158 [debug] [Thread-2 (]: Using duckdb connection "seed.quant_features.raw_stock_prices"
[0m15:26:21.461381 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: 
          COPY "quant_features"."main"."raw_stock_prices" FROM '/workspace/dbt_project/seeds/raw_stock_prices.csv' (FORMAT CSV, HEADER TRUE, DELIMITER ',')
        ...
[0m15:26:21.464417 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m15:26:21.464849 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: 
          COPY "quant_features"."main"."market_data" FROM '/workspace/dbt_project/seeds/market_data.csv' (FORMAT CSV, HEADER TRUE, DELIMITER ',')
        
[0m15:26:21.468535 [debug] [Thread-2 (]: Writing runtime SQL for node "seed.quant_features.raw_stock_prices"
[0m15:26:21.468882 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m15:26:21.469762 [debug] [Thread-1 (]: On seed.quant_features.market_data: ROLLBACK
[0m15:26:21.479561 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: COMMIT
[0m15:26:21.479845 [debug] [Thread-2 (]: Using duckdb connection "seed.quant_features.raw_stock_prices"
[0m15:26:21.480057 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: COMMIT
[0m15:26:21.482852 [debug] [Thread-1 (]: Failed to rollback 'seed.quant_features.market_data'
[0m15:26:21.483137 [debug] [Thread-1 (]: On seed.quant_features.market_data: Close
[0m15:26:21.485823 [debug] [Thread-1 (]: Runtime Error in seed market_data (seeds/market_data.csv)
  Conversion Error: CSV Error on Line: 2
  Original Line: 2024-01-01,AAPL,3000000000000,28.5,0.44,Technology
  Error when converting column "market_cap". Could not convert string "3000000000000" to 'INTEGER'
  
  Column market_cap is being converted as type INTEGER
  This type was either manually set or derived from an existing table. Select a different type to correctly parse this column.
  * Check whether the null string value is set correctly (e.g., nullstr = 'N/A')
  
    file = /workspace/dbt_project/seeds/market_data.csv
    delimiter = , (Set By User)
    quote = (empty) (Auto-Detected)
    escape = (empty) (Auto-Detected)
    new_line = \n (Auto-Detected)
    header = true (Set By User)
    skip_rows = 0 (Auto-Detected)
    comment = (empty) (Auto-Detected)
    strict_mode = true (Auto-Detected)
    date_format =  (Auto-Detected)
    timestamp_format =  (Auto-Detected)
    null_padding = 0
    sample_size = 20480
    ignore_errors = false
    all_varchar = 0
  The Column types set by the user do not match the ones found by the sniffer. 
  Column at position: 2 Set type: INTEGER Sniffed type: BIGINT
  
  
[0m15:26:21.486875 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2f9e1454-b404-41fc-b1e0-45b93a99f7c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97fbe6cb90>]}
[0m15:26:21.487234 [debug] [Thread-2 (]: SQL status: OK in 0.007 seconds
[0m15:26:21.487697 [error] [Thread-1 (]: 1 of 2 ERROR loading seed file main.market_data ................................ [[31mERROR[0m in 0.06s]
[0m15:26:21.489035 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: Close
[0m15:26:21.489724 [debug] [Thread-1 (]: Finished running node seed.quant_features.market_data
[0m15:26:21.490259 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2f9e1454-b404-41fc-b1e0-45b93a99f7c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97f54e1550>]}
[0m15:26:21.490868 [debug] [Thread-7 (]: Marking all children of 'seed.quant_features.market_data' to be skipped because of status 'error'.  Reason: Runtime Error in seed market_data (seeds/market_data.csv)
  Conversion Error: CSV Error on Line: 2
  Original Line: 2024-01-01,AAPL,3000000000000,28.5,0.44,Technology
  Error when converting column "market_cap". Could not convert string "3000000000000" to 'INTEGER'
  
  Column market_cap is being converted as type INTEGER
  This type was either manually set or derived from an existing table. Select a different type to correctly parse this column.
  * Check whether the null string value is set correctly (e.g., nullstr = 'N/A')
  
    file = /workspace/dbt_project/seeds/market_data.csv
    delimiter = , (Set By User)
    quote = (empty) (Auto-Detected)
    escape = (empty) (Auto-Detected)
    new_line = \n (Auto-Detected)
    header = true (Set By User)
    skip_rows = 0 (Auto-Detected)
    comment = (empty) (Auto-Detected)
    strict_mode = true (Auto-Detected)
    date_format =  (Auto-Detected)
    timestamp_format =  (Auto-Detected)
    null_padding = 0
    sample_size = 20480
    ignore_errors = false
    all_varchar = 0
  The Column types set by the user do not match the ones found by the sniffer. 
  Column at position: 2 Set type: INTEGER Sniffed type: BIGINT
  
  .
[0m15:26:21.491301 [info ] [Thread-2 (]: 2 of 2 OK loaded seed file main.raw_stock_prices ............................... [[32mINSERT 32[0m in 0.06s]
[0m15:26:21.492173 [debug] [Thread-2 (]: Finished running node seed.quant_features.raw_stock_prices
[0m15:26:21.493687 [debug] [MainThread]: Using duckdb connection "master"
[0m15:26:21.494762 [debug] [MainThread]: On master: BEGIN
[0m15:26:21.494952 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:26:21.495342 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:26:21.495533 [debug] [MainThread]: On master: COMMIT
[0m15:26:21.495696 [debug] [MainThread]: Using duckdb connection "master"
[0m15:26:21.495846 [debug] [MainThread]: On master: COMMIT
[0m15:26:21.496106 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:26:21.496270 [debug] [MainThread]: On master: Close
[0m15:26:21.496549 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:26:21.496717 [debug] [MainThread]: Connection 'seed.quant_features.market_data' was properly closed.
[0m15:26:21.496859 [debug] [MainThread]: Connection 'seed.quant_features.raw_stock_prices' was properly closed.
[0m15:26:21.497042 [info ] [MainThread]: 
[0m15:26:21.497219 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 0.14 seconds (0.14s).
[0m15:26:21.497683 [debug] [MainThread]: Command end result
[0m15:26:21.519309 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:26:21.520404 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:26:21.523885 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:26:21.524122 [info ] [MainThread]: 
[0m15:26:21.524336 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:26:21.524536 [info ] [MainThread]: 
[0m15:26:21.524748 [error] [MainThread]: [31mFailure in seed market_data (seeds/market_data.csv)[0m
[0m15:26:21.524985 [error] [MainThread]:   Runtime Error in seed market_data (seeds/market_data.csv)
  Conversion Error: CSV Error on Line: 2
  Original Line: 2024-01-01,AAPL,3000000000000,28.5,0.44,Technology
  Error when converting column "market_cap". Could not convert string "3000000000000" to 'INTEGER'
  
  Column market_cap is being converted as type INTEGER
  This type was either manually set or derived from an existing table. Select a different type to correctly parse this column.
  * Check whether the null string value is set correctly (e.g., nullstr = 'N/A')
  
    file = /workspace/dbt_project/seeds/market_data.csv
    delimiter = , (Set By User)
    quote = (empty) (Auto-Detected)
    escape = (empty) (Auto-Detected)
    new_line = \n (Auto-Detected)
    header = true (Set By User)
    skip_rows = 0 (Auto-Detected)
    comment = (empty) (Auto-Detected)
    strict_mode = true (Auto-Detected)
    date_format =  (Auto-Detected)
    timestamp_format =  (Auto-Detected)
    null_padding = 0
    sample_size = 20480
    ignore_errors = false
    all_varchar = 0
  The Column types set by the user do not match the ones found by the sniffer. 
  Column at position: 2 Set type: INTEGER Sniffed type: BIGINT
  
  
[0m15:26:21.525193 [info ] [MainThread]: 
[0m15:26:21.525368 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=2
[0m15:26:21.525653 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 12 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m15:26:21.526202 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": false, "command_wall_clock_time": 1.7209142, "process_in_blocks": "0", "process_kernel_time": 0.163583, "process_mem_max_rss": "163496", "process_out_blocks": "4920", "process_user_time": 2.429802}
[0m15:26:21.526485 [debug] [MainThread]: Command `dbt seed` failed at 15:26:21.526427 after 1.72 seconds
[0m15:26:21.526698 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97fb1bdda0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97ff3a5400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97f54cd250>]}
[0m15:26:21.526905 [debug] [MainThread]: Flushing usage events
[0m15:26:21.578850 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:26:34.299965 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f8338f770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f849e5a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f821cbd90>]}


============================== 15:26:34.302378 | 5807ed42-b7c8-43f4-9682-a91126bcf7fc ==============================
[0m15:26:34.302378 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:26:34.302711 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'version_check': 'True', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'profiles_dir': '/workspace/dbt_project', 'cache_selected_only': 'False', 'debug': 'False', 'static_parser': 'True', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'use_colors': 'True', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'log_path': '/workspace/dbt_project/logs', 'introspect': 'True', 'empty': 'None', 'log_format': 'default', 'printer_width': '80', 'write_json': 'True', 'warn_error': 'None', 'invocation_command': 'dbt seed --full-refresh'}
[0m15:26:34.437089 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5807ed42-b7c8-43f4-9682-a91126bcf7fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f82f16650>]}
[0m15:26:34.478235 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5807ed42-b7c8-43f4-9682-a91126bcf7fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f822cabe0>]}
[0m15:26:34.479248 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:26:34.508354 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:26:34.595504 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m15:26:34.595967 [debug] [MainThread]: Partial parsing: added file: quant_features://seeds/schema.yml
[0m15:26:34.693065 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5807ed42-b7c8-43f4-9682-a91126bcf7fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f7defe350>]}
[0m15:26:34.788652 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:26:34.789801 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:26:34.799721 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5807ed42-b7c8-43f4-9682-a91126bcf7fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f7de654f0>]}
[0m15:26:34.800056 [info ] [MainThread]: Found 15 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:26:34.800266 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5807ed42-b7c8-43f4-9682-a91126bcf7fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f821ad2b0>]}
[0m15:26:34.801738 [info ] [MainThread]: 
[0m15:26:34.802042 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:26:34.802234 [info ] [MainThread]: 
[0m15:26:34.802544 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:26:34.805709 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features'
[0m15:26:34.820392 [debug] [ThreadPool]: Using duckdb connection "list_quant_features"
[0m15:26:34.820649 [debug] [ThreadPool]: On list_quant_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"quant_features"'
    
  
  
[0m15:26:34.820825 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:26:34.830716 [debug] [ThreadPool]: SQL status: OK in 0.010 seconds
[0m15:26:34.831607 [debug] [ThreadPool]: On list_quant_features: Close
[0m15:26:34.832166 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_quant_features, now create_quant_features_main)
[0m15:26:34.832481 [debug] [ThreadPool]: Creating schema "database: "quant_features"
schema: "main"
"
[0m15:26:34.836591 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:26:34.836847 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='quant_features'
        and type='sqlite'
    
  
[0m15:26:34.837027 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:26:34.837677 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:26:34.838519 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:26:34.838735 [debug] [ThreadPool]: On create_quant_features_main: BEGIN
[0m15:26:34.839056 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:26:34.839230 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:26:34.839386 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
    
        create schema if not exists "quant_features"."main"
    
[0m15:26:34.839678 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:26:34.840207 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:26:34.840421 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:26:34.840581 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:26:34.840864 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:26:34.841064 [debug] [ThreadPool]: On create_quant_features_main: Close
[0m15:26:34.842916 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_quant_features_main, now list_quant_features_main)
[0m15:26:34.846840 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:26:34.847096 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:26:34.847259 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:26:34.847684 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:26:34.847888 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:26:34.848055 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:26:34.854194 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m15:26:34.855054 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:26:34.855654 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:26:34.855866 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:26:34.857044 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5807ed42-b7c8-43f4-9682-a91126bcf7fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f7d158bb0>]}
[0m15:26:34.857439 [debug] [MainThread]: Using duckdb connection "master"
[0m15:26:34.857634 [debug] [MainThread]: On master: BEGIN
[0m15:26:34.857792 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:26:34.858210 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:26:34.858402 [debug] [MainThread]: On master: COMMIT
[0m15:26:34.858555 [debug] [MainThread]: Using duckdb connection "master"
[0m15:26:34.858702 [debug] [MainThread]: On master: COMMIT
[0m15:26:34.859132 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:26:34.859326 [debug] [MainThread]: On master: Close
[0m15:26:34.861375 [debug] [Thread-2 (]: Began running node seed.quant_features.raw_stock_prices
[0m15:26:34.861767 [debug] [Thread-1 (]: Began running node seed.quant_features.market_data
[0m15:26:34.862275 [info ] [Thread-2 (]: 2 of 2 START seed file main.raw_stock_prices ................................... [RUN]
[0m15:26:34.862787 [info ] [Thread-1 (]: 1 of 2 START seed file main.market_data ........................................ [RUN]
[0m15:26:34.863224 [debug] [Thread-2 (]: Acquiring new duckdb connection 'seed.quant_features.raw_stock_prices'
[0m15:26:34.863583 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now seed.quant_features.market_data)
[0m15:26:34.863867 [debug] [Thread-2 (]: Began compiling node seed.quant_features.raw_stock_prices
[0m15:26:34.864141 [debug] [Thread-1 (]: Began compiling node seed.quant_features.market_data
[0m15:26:34.864429 [debug] [Thread-2 (]: Began executing node seed.quant_features.raw_stock_prices
[0m15:26:34.864725 [debug] [Thread-1 (]: Began executing node seed.quant_features.market_data
[0m15:26:34.884816 [debug] [Thread-1 (]: Using duckdb connection "seed.quant_features.market_data"
[0m15:26:34.885102 [debug] [Thread-1 (]: On seed.quant_features.market_data: BEGIN
[0m15:26:34.885297 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:26:34.885797 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:26:34.886046 [debug] [Thread-1 (]: Using duckdb connection "seed.quant_features.market_data"
[0m15:26:34.886238 [debug] [Thread-1 (]: On seed.quant_features.market_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "seed.quant_features.market_data"} */

    create table "quant_features"."main"."market_data" ("date" date,"symbol" text,"market_cap" integer,"pe_ratio" float8,"dividend_yield" float8,"sector" text)
  
[0m15:26:34.893615 [debug] [Thread-2 (]: Using duckdb connection "seed.quant_features.raw_stock_prices"
[0m15:26:34.893915 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "seed.quant_features.raw_stock_prices"} */

      drop table if exists "quant_features"."main"."raw_stock_prices" cascade
    
[0m15:26:34.894121 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:26:34.894585 [debug] [Thread-1 (]: SQL status: OK in 0.008 seconds
[0m15:26:34.901658 [debug] [Thread-1 (]: Using duckdb connection "seed.quant_features.market_data"
[0m15:26:34.902000 [debug] [Thread-2 (]: SQL status: OK in 0.008 seconds
[0m15:26:34.902436 [debug] [Thread-1 (]: On seed.quant_features.market_data: 
          COPY "quant_features"."main"."market_data" FROM '/workspace/dbt_project/seeds/market_data.csv' (FORMAT CSV, HEADER TRUE, DELIMITER ',')
        ...
[0m15:26:34.903634 [debug] [Thread-2 (]: Using duckdb connection "seed.quant_features.raw_stock_prices"
[0m15:26:34.904045 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: BEGIN
[0m15:26:34.904459 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:26:34.904672 [debug] [Thread-2 (]: Using duckdb connection "seed.quant_features.raw_stock_prices"
[0m15:26:34.904854 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "seed.quant_features.raw_stock_prices"} */

    create table "quant_features"."main"."raw_stock_prices" ("date" date,"symbol" text,"open" float8,"high" float8,"low" float8,"close" float8,"volume" integer)
  
[0m15:26:34.905337 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:26:34.906069 [debug] [Thread-2 (]: Using duckdb connection "seed.quant_features.raw_stock_prices"
[0m15:26:34.906296 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: 
          COPY "quant_features"."main"."raw_stock_prices" FROM '/workspace/dbt_project/seeds/raw_stock_prices.csv' (FORMAT CSV, HEADER TRUE, DELIMITER ',')
        ...
[0m15:26:34.907924 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: 
          COPY "quant_features"."main"."market_data" FROM '/workspace/dbt_project/seeds/market_data.csv' (FORMAT CSV, HEADER TRUE, DELIMITER ',')
        
[0m15:26:34.908194 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m15:26:34.908470 [debug] [Thread-1 (]: On seed.quant_features.market_data: ROLLBACK
[0m15:26:34.909585 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m15:26:34.913142 [debug] [Thread-2 (]: Writing runtime SQL for node "seed.quant_features.raw_stock_prices"
[0m15:26:34.923403 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: COMMIT
[0m15:26:34.923679 [debug] [Thread-2 (]: Using duckdb connection "seed.quant_features.raw_stock_prices"
[0m15:26:34.923888 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: COMMIT
[0m15:26:34.926895 [debug] [Thread-1 (]: Failed to rollback 'seed.quant_features.market_data'
[0m15:26:34.927201 [debug] [Thread-1 (]: On seed.quant_features.market_data: Close
[0m15:26:34.927537 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m15:26:34.928996 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: Close
[0m15:26:34.930896 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5807ed42-b7c8-43f4-9682-a91126bcf7fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f7d2456d0>]}
[0m15:26:34.933030 [debug] [Thread-1 (]: Runtime Error in seed market_data (seeds/market_data.csv)
  Conversion Error: CSV Error on Line: 2
  Original Line: 2024-01-01,AAPL,3000000000000,28.5,0.44,Technology
  Error when converting column "market_cap". Could not convert string "3000000000000" to 'INTEGER'
  
  Column market_cap is being converted as type INTEGER
  This type was either manually set or derived from an existing table. Select a different type to correctly parse this column.
  * Check whether the null string value is set correctly (e.g., nullstr = 'N/A')
  
    file = /workspace/dbt_project/seeds/market_data.csv
    delimiter = , (Set By User)
    quote = (empty) (Auto-Detected)
    escape = (empty) (Auto-Detected)
    new_line = \n (Auto-Detected)
    header = true (Set By User)
    skip_rows = 0 (Auto-Detected)
    comment = (empty) (Auto-Detected)
    strict_mode = true (Auto-Detected)
    date_format =  (Auto-Detected)
    timestamp_format =  (Auto-Detected)
    null_padding = 0
    sample_size = 20480
    ignore_errors = false
    all_varchar = 0
  The Column types set by the user do not match the ones found by the sniffer. 
  Column at position: 2 Set type: INTEGER Sniffed type: BIGINT
  
  
[0m15:26:34.933618 [info ] [Thread-2 (]: 2 of 2 OK loaded seed file main.raw_stock_prices ............................... [[32mCREATE 32[0m in 0.07s]
[0m15:26:34.934143 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5807ed42-b7c8-43f4-9682-a91126bcf7fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f7dc826d0>]}
[0m15:26:34.934693 [debug] [Thread-2 (]: Finished running node seed.quant_features.raw_stock_prices
[0m15:26:34.935166 [error] [Thread-1 (]: 1 of 2 ERROR loading seed file main.market_data ................................ [[31mERROR[0m in 0.07s]
[0m15:26:34.935870 [debug] [Thread-1 (]: Finished running node seed.quant_features.market_data
[0m15:26:34.936325 [debug] [Thread-7 (]: Marking all children of 'seed.quant_features.market_data' to be skipped because of status 'error'.  Reason: Runtime Error in seed market_data (seeds/market_data.csv)
  Conversion Error: CSV Error on Line: 2
  Original Line: 2024-01-01,AAPL,3000000000000,28.5,0.44,Technology
  Error when converting column "market_cap". Could not convert string "3000000000000" to 'INTEGER'
  
  Column market_cap is being converted as type INTEGER
  This type was either manually set or derived from an existing table. Select a different type to correctly parse this column.
  * Check whether the null string value is set correctly (e.g., nullstr = 'N/A')
  
    file = /workspace/dbt_project/seeds/market_data.csv
    delimiter = , (Set By User)
    quote = (empty) (Auto-Detected)
    escape = (empty) (Auto-Detected)
    new_line = \n (Auto-Detected)
    header = true (Set By User)
    skip_rows = 0 (Auto-Detected)
    comment = (empty) (Auto-Detected)
    strict_mode = true (Auto-Detected)
    date_format =  (Auto-Detected)
    timestamp_format =  (Auto-Detected)
    null_padding = 0
    sample_size = 20480
    ignore_errors = false
    all_varchar = 0
  The Column types set by the user do not match the ones found by the sniffer. 
  Column at position: 2 Set type: INTEGER Sniffed type: BIGINT
  
  .
[0m15:26:34.938611 [debug] [MainThread]: Using duckdb connection "master"
[0m15:26:34.938860 [debug] [MainThread]: On master: BEGIN
[0m15:26:34.939043 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:26:34.939396 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:26:34.939573 [debug] [MainThread]: On master: COMMIT
[0m15:26:34.939728 [debug] [MainThread]: Using duckdb connection "master"
[0m15:26:34.939872 [debug] [MainThread]: On master: COMMIT
[0m15:26:34.940188 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:26:34.940387 [debug] [MainThread]: On master: Close
[0m15:26:34.940661 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:26:34.940823 [debug] [MainThread]: Connection 'seed.quant_features.market_data' was properly closed.
[0m15:26:34.940971 [debug] [MainThread]: Connection 'seed.quant_features.raw_stock_prices' was properly closed.
[0m15:26:34.941140 [info ] [MainThread]: 
[0m15:26:34.941310 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 0.14 seconds (0.14s).
[0m15:26:34.941776 [debug] [MainThread]: Command end result
[0m15:26:34.963683 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:26:34.964831 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:26:34.968372 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:26:34.968600 [info ] [MainThread]: 
[0m15:26:34.968811 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:26:34.968994 [info ] [MainThread]: 
[0m15:26:34.969204 [error] [MainThread]: [31mFailure in seed market_data (seeds/market_data.csv)[0m
[0m15:26:34.969418 [error] [MainThread]:   Runtime Error in seed market_data (seeds/market_data.csv)
  Conversion Error: CSV Error on Line: 2
  Original Line: 2024-01-01,AAPL,3000000000000,28.5,0.44,Technology
  Error when converting column "market_cap". Could not convert string "3000000000000" to 'INTEGER'
  
  Column market_cap is being converted as type INTEGER
  This type was either manually set or derived from an existing table. Select a different type to correctly parse this column.
  * Check whether the null string value is set correctly (e.g., nullstr = 'N/A')
  
    file = /workspace/dbt_project/seeds/market_data.csv
    delimiter = , (Set By User)
    quote = (empty) (Auto-Detected)
    escape = (empty) (Auto-Detected)
    new_line = \n (Auto-Detected)
    header = true (Set By User)
    skip_rows = 0 (Auto-Detected)
    comment = (empty) (Auto-Detected)
    strict_mode = true (Auto-Detected)
    date_format =  (Auto-Detected)
    timestamp_format =  (Auto-Detected)
    null_padding = 0
    sample_size = 20480
    ignore_errors = false
    all_varchar = 0
  The Column types set by the user do not match the ones found by the sniffer. 
  Column at position: 2 Set type: INTEGER Sniffed type: BIGINT
  
  
[0m15:26:34.969605 [info ] [MainThread]: 
[0m15:26:34.969771 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=2
[0m15:26:34.970294 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": false, "command_wall_clock_time": 0.70694566, "process_in_blocks": "0", "process_kernel_time": 0.163698, "process_mem_max_rss": "155708", "process_out_blocks": "4920", "process_user_time": 1.441296}
[0m15:26:34.970569 [debug] [MainThread]: Command `dbt seed` failed at 15:26:34.970513 after 0.71 seconds
[0m15:26:34.970771 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f7d2e7a70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f7ddcc9e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f7eb368d0>]}
[0m15:26:34.970979 [debug] [MainThread]: Flushing usage events
[0m15:26:34.998342 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:26:48.899990 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facd9667770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facdacbda90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facd849fd90>]}


============================== 15:26:48.902308 | eb21f7a4-8bd1-4ba5-9b26-576fcbe93fb2 ==============================
[0m15:26:48.902308 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:26:48.902635 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/workspace/dbt_project', 'target_path': 'None', 'log_path': '/workspace/dbt_project/logs', 'warn_error': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_cache_events': 'False', 'introspect': 'True', 'debug': 'False', 'use_experimental_parser': 'False', 'invocation_command': 'dbt seed --full-refresh', 'use_colors': 'True', 'empty': 'None', 'write_json': 'True', 'fail_fast': 'False', 'version_check': 'True', 'log_format': 'default', 'partial_parse': 'True', 'printer_width': '80', 'no_print': 'None', 'indirect_selection': 'eager'}
[0m15:26:49.042437 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'eb21f7a4-8bd1-4ba5-9b26-576fcbe93fb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facd91ee650>]}
[0m15:26:49.083719 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'eb21f7a4-8bd1-4ba5-9b26-576fcbe93fb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facd859ebe0>]}
[0m15:26:49.084724 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:26:49.113876 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:26:49.192220 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m15:26:49.192527 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'eb21f7a4-8bd1-4ba5-9b26-576fcbe93fb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facd856f550>]}
[0m15:26:50.181177 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `accepted_values`. Arguments to generic tests
should be nested under the `arguments` property.`
[0m15:26:50.181490 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'eb21f7a4-8bd1-4ba5-9b26-576fcbe93fb2', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facd373f980>]}
[0m15:26:50.390067 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'eb21f7a4-8bd1-4ba5-9b26-576fcbe93fb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facd34fd8d0>]}
[0m15:26:50.453451 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:26:50.454641 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:26:50.465155 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'eb21f7a4-8bd1-4ba5-9b26-576fcbe93fb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facd33d7520>]}
[0m15:26:50.465490 [info ] [MainThread]: Found 15 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:26:50.465696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'eb21f7a4-8bd1-4ba5-9b26-576fcbe93fb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facd42ecb90>]}
[0m15:26:50.467245 [info ] [MainThread]: 
[0m15:26:50.467523 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:26:50.467695 [info ] [MainThread]: 
[0m15:26:50.467998 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:26:50.471441 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features'
[0m15:26:50.485948 [debug] [ThreadPool]: Using duckdb connection "list_quant_features"
[0m15:26:50.486203 [debug] [ThreadPool]: On list_quant_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"quant_features"'
    
  
  
[0m15:26:50.486385 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:26:50.496227 [debug] [ThreadPool]: SQL status: OK in 0.010 seconds
[0m15:26:50.497162 [debug] [ThreadPool]: On list_quant_features: Close
[0m15:26:50.497881 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_quant_features, now create_quant_features_main)
[0m15:26:50.498174 [debug] [ThreadPool]: Creating schema "database: "quant_features"
schema: "main"
"
[0m15:26:50.502318 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:26:50.502575 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='quant_features'
        and type='sqlite'
    
  
[0m15:26:50.502745 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:26:50.503411 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:26:50.504262 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:26:50.504487 [debug] [ThreadPool]: On create_quant_features_main: BEGIN
[0m15:26:50.504788 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:26:50.504968 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:26:50.505121 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
    
        create schema if not exists "quant_features"."main"
    
[0m15:26:50.505497 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:26:50.506055 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:26:50.506244 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:26:50.506402 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:26:50.506697 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:26:50.506899 [debug] [ThreadPool]: On create_quant_features_main: Close
[0m15:26:50.508677 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_quant_features_main, now list_quant_features_main)
[0m15:26:50.512443 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:26:50.512691 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:26:50.512869 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:26:50.513267 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:26:50.513454 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:26:50.513612 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:26:50.519737 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m15:26:50.520610 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:26:50.521230 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:26:50.521440 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:26:50.522430 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'eb21f7a4-8bd1-4ba5-9b26-576fcbe93fb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facd33936a0>]}
[0m15:26:50.522789 [debug] [MainThread]: Using duckdb connection "master"
[0m15:26:50.522988 [debug] [MainThread]: On master: BEGIN
[0m15:26:50.523139 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:26:50.523520 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:26:50.523705 [debug] [MainThread]: On master: COMMIT
[0m15:26:50.523872 [debug] [MainThread]: Using duckdb connection "master"
[0m15:26:50.524017 [debug] [MainThread]: On master: COMMIT
[0m15:26:50.524276 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:26:50.524486 [debug] [MainThread]: On master: Close
[0m15:26:50.527101 [debug] [Thread-1 (]: Began running node seed.quant_features.market_data
[0m15:26:50.527518 [debug] [Thread-2 (]: Began running node seed.quant_features.raw_stock_prices
[0m15:26:50.528052 [info ] [Thread-1 (]: 1 of 2 START seed file main.market_data ........................................ [RUN]
[0m15:26:50.528529 [info ] [Thread-2 (]: 2 of 2 START seed file main.raw_stock_prices ................................... [RUN]
[0m15:26:50.529111 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now seed.quant_features.market_data)
[0m15:26:50.529620 [debug] [Thread-2 (]: Acquiring new duckdb connection 'seed.quant_features.raw_stock_prices'
[0m15:26:50.529974 [debug] [Thread-1 (]: Began compiling node seed.quant_features.market_data
[0m15:26:50.530283 [debug] [Thread-2 (]: Began compiling node seed.quant_features.raw_stock_prices
[0m15:26:50.530585 [debug] [Thread-1 (]: Began executing node seed.quant_features.market_data
[0m15:26:50.530870 [debug] [Thread-2 (]: Began executing node seed.quant_features.raw_stock_prices
[0m15:26:50.563685 [debug] [Thread-1 (]: Using duckdb connection "seed.quant_features.market_data"
[0m15:26:50.566174 [debug] [Thread-2 (]: Using duckdb connection "seed.quant_features.raw_stock_prices"
[0m15:26:50.566654 [debug] [Thread-1 (]: On seed.quant_features.market_data: BEGIN
[0m15:26:50.567026 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "seed.quant_features.raw_stock_prices"} */

      drop table if exists "quant_features"."main"."raw_stock_prices" cascade
    
[0m15:26:50.567699 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:26:50.567382 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:26:50.568613 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:26:50.569074 [debug] [Thread-1 (]: Using duckdb connection "seed.quant_features.market_data"
[0m15:26:50.569420 [debug] [Thread-1 (]: On seed.quant_features.market_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "seed.quant_features.market_data"} */

    create table "quant_features"."main"."market_data" ("date" date,"symbol" text,"market_cap" BIGINT,"pe_ratio" DECIMAL(10,2),"dividend_yield" DECIMAL(5,2),"sector" text)
  
[0m15:26:50.570004 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m15:26:50.570992 [debug] [Thread-2 (]: Using duckdb connection "seed.quant_features.raw_stock_prices"
[0m15:26:50.571235 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: BEGIN
[0m15:26:50.571617 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:26:50.571822 [debug] [Thread-2 (]: Using duckdb connection "seed.quant_features.raw_stock_prices"
[0m15:26:50.572017 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "seed.quant_features.raw_stock_prices"} */

    create table "quant_features"."main"."raw_stock_prices" ("date" date,"symbol" text,"open" DECIMAL(10,2),"high" DECIMAL(10,2),"low" DECIMAL(10,2),"close" DECIMAL(10,2),"volume" BIGINT)
  
[0m15:26:50.572519 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m15:26:50.579426 [debug] [Thread-1 (]: Using duckdb connection "seed.quant_features.market_data"
[0m15:26:50.579720 [debug] [Thread-1 (]: On seed.quant_features.market_data: 
          COPY "quant_features"."main"."market_data" FROM '/workspace/dbt_project/seeds/market_data.csv' (FORMAT CSV, HEADER TRUE, DELIMITER ',')
        ...
[0m15:26:50.580176 [debug] [Thread-2 (]: SQL status: OK in 0.008 seconds
[0m15:26:50.581003 [debug] [Thread-2 (]: Using duckdb connection "seed.quant_features.raw_stock_prices"
[0m15:26:50.581240 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: 
          COPY "quant_features"."main"."raw_stock_prices" FROM '/workspace/dbt_project/seeds/raw_stock_prices.csv' (FORMAT CSV, HEADER TRUE, DELIMITER ',')
        ...
[0m15:26:50.584001 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m15:26:50.587542 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.quant_features.market_data"
[0m15:26:50.588037 [debug] [Thread-2 (]: SQL status: OK in 0.007 seconds
[0m15:26:50.593712 [debug] [Thread-2 (]: Writing runtime SQL for node "seed.quant_features.raw_stock_prices"
[0m15:26:50.597242 [debug] [Thread-1 (]: On seed.quant_features.market_data: COMMIT
[0m15:26:50.597764 [debug] [Thread-1 (]: Using duckdb connection "seed.quant_features.market_data"
[0m15:26:50.598026 [debug] [Thread-1 (]: On seed.quant_features.market_data: COMMIT
[0m15:26:50.600516 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: COMMIT
[0m15:26:50.600800 [debug] [Thread-2 (]: Using duckdb connection "seed.quant_features.raw_stock_prices"
[0m15:26:50.601006 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: COMMIT
[0m15:26:50.601341 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m15:26:50.602754 [debug] [Thread-1 (]: On seed.quant_features.market_data: Close
[0m15:26:50.603241 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m15:26:50.603800 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: Close
[0m15:26:50.605049 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'eb21f7a4-8bd1-4ba5-9b26-576fcbe93fb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facd91b0b90>]}
[0m15:26:50.605427 [info ] [Thread-1 (]: 1 of 2 OK loaded seed file main.market_data .................................... [[32mCREATE 32[0m in 0.08s]
[0m15:26:50.605890 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'eb21f7a4-8bd1-4ba5-9b26-576fcbe93fb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facd28c0850>]}
[0m15:26:50.606320 [debug] [Thread-1 (]: Finished running node seed.quant_features.market_data
[0m15:26:50.606716 [info ] [Thread-2 (]: 2 of 2 OK loaded seed file main.raw_stock_prices ............................... [[32mCREATE 32[0m in 0.08s]
[0m15:26:50.607575 [debug] [Thread-2 (]: Finished running node seed.quant_features.raw_stock_prices
[0m15:26:50.608973 [debug] [MainThread]: Using duckdb connection "master"
[0m15:26:50.609230 [debug] [MainThread]: On master: BEGIN
[0m15:26:50.609399 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:26:50.609801 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:26:50.610009 [debug] [MainThread]: On master: COMMIT
[0m15:26:50.610168 [debug] [MainThread]: Using duckdb connection "master"
[0m15:26:50.610315 [debug] [MainThread]: On master: COMMIT
[0m15:26:50.610568 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:26:50.610740 [debug] [MainThread]: On master: Close
[0m15:26:50.611016 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:26:50.611173 [debug] [MainThread]: Connection 'seed.quant_features.market_data' was properly closed.
[0m15:26:50.611302 [debug] [MainThread]: Connection 'seed.quant_features.raw_stock_prices' was properly closed.
[0m15:26:50.611468 [info ] [MainThread]: 
[0m15:26:50.611654 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 0.14 seconds (0.14s).
[0m15:26:50.612102 [debug] [MainThread]: Command end result
[0m15:26:50.632560 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:26:50.633698 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:26:50.637276 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:26:50.637513 [info ] [MainThread]: 
[0m15:26:50.637732 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:26:50.637910 [info ] [MainThread]: 
[0m15:26:50.638086 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m15:26:50.638372 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 12 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m15:26:50.638961 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 1.7758723, "process_in_blocks": "0", "process_kernel_time": 0.151495, "process_mem_max_rss": "163276", "process_out_blocks": "4896", "process_user_time": 2.503625}
[0m15:26:50.639278 [debug] [MainThread]: Command `dbt seed` succeeded at 15:26:50.639211 after 1.78 seconds
[0m15:26:50.639477 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facd84cdda0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facdc6e5400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facd348a330>]}
[0m15:26:50.639688 [debug] [MainThread]: Flushing usage events
[0m15:26:50.686796 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:26:56.448294 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05b31f3770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05b482da90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05b206fd90>]}


============================== 15:26:56.450578 | b9de5e66-0c9a-407b-8ac2-3599d5e61949 ==============================
[0m15:26:56.450578 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:26:56.451338 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'no_print': 'None', 'log_cache_events': 'False', 'invocation_command': 'dbt run', 'debug': 'False', 'indirect_selection': 'eager', 'profiles_dir': '/workspace/dbt_project', 'empty': 'False', 'printer_width': '80', 'target_path': 'None', 'quiet': 'False', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'log_path': '/workspace/dbt_project/logs', 'log_format': 'default', 'cache_selected_only': 'False', 'write_json': 'True', 'version_check': 'True', 'introspect': 'True', 'warn_error': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'use_colors': 'True'}
[0m15:26:56.594082 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b9de5e66-0c9a-407b-8ac2-3599d5e61949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05b2d86650>]}
[0m15:26:56.635228 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b9de5e66-0c9a-407b-8ac2-3599d5e61949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05b216abe0>]}
[0m15:26:56.636306 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:26:56.665055 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:26:56.751531 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:26:56.751775 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:26:56.788097 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b9de5e66-0c9a-407b-8ac2-3599d5e61949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05ae139450>]}
[0m15:26:56.857650 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:26:56.858834 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:26:56.869282 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b9de5e66-0c9a-407b-8ac2-3599d5e61949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05ae16d220>]}
[0m15:26:56.869630 [info ] [MainThread]: Found 15 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:26:56.869856 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b9de5e66-0c9a-407b-8ac2-3599d5e61949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05ae1ac130>]}
[0m15:26:56.871454 [info ] [MainThread]: 
[0m15:26:56.871714 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:26:56.871904 [info ] [MainThread]: 
[0m15:26:56.872186 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:26:56.875782 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features'
[0m15:26:56.893799 [debug] [ThreadPool]: Using duckdb connection "list_quant_features"
[0m15:26:56.894069 [debug] [ThreadPool]: On list_quant_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"quant_features"'
    
  
  
[0m15:26:56.894251 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:26:56.905772 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m15:26:56.906684 [debug] [ThreadPool]: On list_quant_features: Close
[0m15:26:56.907398 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_quant_features, now create_quant_features_main)
[0m15:26:56.907708 [debug] [ThreadPool]: Creating schema "database: "quant_features"
schema: "main"
"
[0m15:26:56.912007 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:26:56.912265 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='quant_features'
        and type='sqlite'
    
  
[0m15:26:56.912466 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:26:56.913153 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:26:56.913996 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:26:56.914210 [debug] [ThreadPool]: On create_quant_features_main: BEGIN
[0m15:26:56.914572 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:26:56.914752 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:26:56.914921 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
    
        create schema if not exists "quant_features"."main"
    
[0m15:26:56.915251 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:26:56.915772 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:26:56.915989 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:26:56.916153 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:26:56.916467 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:26:56.916651 [debug] [ThreadPool]: On create_quant_features_main: Close
[0m15:26:56.918736 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_quant_features_main, now list_quant_features_main)
[0m15:26:56.953161 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:26:56.953411 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:26:56.953581 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:26:56.954003 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:26:56.954211 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:26:56.954378 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:26:56.960875 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m15:26:56.961726 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:26:56.962325 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:26:56.962528 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:26:56.963759 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b9de5e66-0c9a-407b-8ac2-3599d5e61949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05b212b520>]}
[0m15:26:56.964220 [debug] [MainThread]: Using duckdb connection "master"
[0m15:26:56.964428 [debug] [MainThread]: On master: BEGIN
[0m15:26:56.964582 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:26:56.964979 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:26:56.965169 [debug] [MainThread]: On master: COMMIT
[0m15:26:56.965315 [debug] [MainThread]: Using duckdb connection "master"
[0m15:26:56.965452 [debug] [MainThread]: On master: COMMIT
[0m15:26:56.965708 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:26:56.965900 [debug] [MainThread]: On master: Close
[0m15:26:56.968114 [debug] [Thread-1 (]: Began running node model.quant_features.stg_market_data
[0m15:26:56.968488 [debug] [Thread-2 (]: Began running node model.quant_features.stg_ohlc_data
[0m15:26:56.968960 [debug] [Thread-3 (]: Began running node model.quant_features.stg_stock_prices
[0m15:26:56.969504 [info ] [Thread-1 (]: 1 of 15 START sql view model main.stg_market_data .............................. [RUN]
[0m15:26:56.970031 [info ] [Thread-2 (]: 2 of 15 START sql view model main.stg_ohlc_data ................................ [RUN]
[0m15:26:56.970559 [info ] [Thread-3 (]: 3 of 15 START sql view model main.stg_stock_prices ............................. [RUN]
[0m15:26:56.971119 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.stg_market_data)
[0m15:26:56.971718 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.quant_features.stg_ohlc_data'
[0m15:26:56.972189 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.quant_features.stg_stock_prices'
[0m15:26:56.972485 [debug] [Thread-1 (]: Began compiling node model.quant_features.stg_market_data
[0m15:26:56.972766 [debug] [Thread-2 (]: Began compiling node model.quant_features.stg_ohlc_data
[0m15:26:56.973255 [debug] [Thread-3 (]: Began compiling node model.quant_features.stg_stock_prices
[0m15:26:56.978076 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.stg_market_data"
[0m15:26:56.980913 [debug] [Thread-2 (]: Writing injected SQL for node "model.quant_features.stg_ohlc_data"
[0m15:26:56.983137 [debug] [Thread-3 (]: Writing injected SQL for node "model.quant_features.stg_stock_prices"
[0m15:26:56.984175 [debug] [Thread-2 (]: Began executing node model.quant_features.stg_ohlc_data
[0m15:26:56.996465 [debug] [Thread-1 (]: Began executing node model.quant_features.stg_market_data
[0m15:26:57.010009 [debug] [Thread-1 (]: Writing runtime sql for node "model.quant_features.stg_market_data"
[0m15:26:57.011624 [debug] [Thread-2 (]: Writing runtime sql for node "model.quant_features.stg_ohlc_data"
[0m15:26:57.012118 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:26:57.012407 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: BEGIN
[0m15:26:57.012621 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:26:57.013239 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stg_ohlc_data"
[0m15:26:57.013517 [debug] [Thread-2 (]: On model.quant_features.stg_ohlc_data: BEGIN
[0m15:26:57.013704 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:26:57.014131 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:26:57.014464 [debug] [Thread-3 (]: Began executing node model.quant_features.stg_stock_prices
[0m15:26:57.014802 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stg_ohlc_data"
[0m15:26:57.015156 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:26:57.016994 [debug] [Thread-3 (]: Writing runtime sql for node "model.quant_features.stg_stock_prices"
[0m15:26:57.017521 [debug] [Thread-2 (]: On model.quant_features.stg_ohlc_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_ohlc_data"} */

  
  create view "quant_features"."main"."stg_ohlc_data__dbt_tmp" as (
    

with raw_ohlc as (
    select 
        symbol,
        timestamp,
        open,
        high,
        low,
        close,
        volume,
        -- 数据清洗和验证
        case 
            when open <= 0 or high <= 0 or low <= 0 or close <= 0 then null
            when high < greatest(open, close, low) then null
            when low > least(open, close, high) then null
            else timestamp
        end as valid_timestamp
    from "quant_features"."raw"."ohlc_data"
),

cleaned_ohlc as (
    select 
        symbol,
        timestamp,
        open,
        high,
        low,
        close,
        volume,
        -- 计算基础指标
        (high + low + close) / 3 as typical_price,
        (high - low) as daily_range,
        case when open != 0 then (close - open) / open else 0 end as daily_return,
        case when close != 0 then volume / close else 0 end as volume_price_ratio
    from raw_ohlc
    where valid_timestamp is not null
      and timestamp >= '2020-01-01'
      and timestamp <= '2024-12-31'
)

select * from cleaned_ohlc
  );

[0m15:26:57.017953 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:26:57.018902 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_market_data"} */

  
  create view "quant_features"."main"."stg_market_data__dbt_tmp" as (
    

with source_data as (
    select
        date,
        symbol,
        market_cap,
        pe_ratio,
        dividend_yield,
        sector,
        -- 添加计算字段
        case 
            when pe_ratio > 0 then market_cap / pe_ratio 
            else null 
        end as estimated_earnings,
        case 
            when dividend_yield > 0 then market_cap * dividend_yield / 100 
            else 0 
        end as estimated_dividend_payout
    from "quant_features"."main"."market_data"
    where date is not null
      and symbol is not null
      and market_cap > 0
)

select * from source_data
  );

[0m15:26:57.019335 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:26:57.019753 [debug] [Thread-3 (]: On model.quant_features.stg_stock_prices: BEGIN
[0m15:26:57.020125 [debug] [Thread-2 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_ohlc_data"} */

  
  create view "quant_features"."main"."stg_ohlc_data__dbt_tmp" as (
    

with raw_ohlc as (
    select 
        symbol,
        timestamp,
        open,
        high,
        low,
        close,
        volume,
        -- 数据清洗和验证
        case 
            when open <= 0 or high <= 0 or low <= 0 or close <= 0 then null
            when high < greatest(open, close, low) then null
            when low > least(open, close, high) then null
            else timestamp
        end as valid_timestamp
    from "quant_features"."raw"."ohlc_data"
),

cleaned_ohlc as (
    select 
        symbol,
        timestamp,
        open,
        high,
        low,
        close,
        volume,
        -- 计算基础指标
        (high + low + close) / 3 as typical_price,
        (high - low) as daily_range,
        case when open != 0 then (close - open) / open else 0 end as daily_return,
        case when close != 0 then volume / close else 0 end as volume_price_ratio
    from raw_ohlc
    where valid_timestamp is not null
      and timestamp >= '2020-01-01'
      and timestamp <= '2024-12-31'
)

select * from cleaned_ohlc
  );

[0m15:26:57.020452 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:26:57.020764 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:26:57.021280 [debug] [Thread-2 (]: DuckDB adapter: Rolling back transaction.
[0m15:26:57.025646 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:26:57.026148 [debug] [Thread-2 (]: On model.quant_features.stg_ohlc_data: ROLLBACK
[0m15:26:57.026473 [debug] [Thread-3 (]: SQL status: OK in 0.006 seconds
[0m15:26:57.026857 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_market_data"} */
alter view "quant_features"."main"."stg_market_data__dbt_tmp" rename to "stg_market_data"
[0m15:26:57.027925 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:26:57.028621 [debug] [Thread-3 (]: On model.quant_features.stg_stock_prices: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_stock_prices"} */

  
  create view "quant_features"."main"."stg_stock_prices__dbt_tmp" as (
    

with source_data as (
    select
        date,
        symbol,
        open,
        high,
        low,
        close,
        volume,
        -- 计算基础技术指标
        (high - low) as daily_range,
        (close - open) as daily_change,
        (close - open) / open as daily_return,
        -- 添加数据质量检查
        case 
            when high >= low and high >= open and high >= close 
                 and low <= open and low <= close 
            then true 
            else false 
        end as is_valid_ohlc
    from "quant_features"."main"."raw_stock_prices"
    where date is not null
      and symbol is not null
      and open > 0
      and high > 0
      and low > 0
      and close > 0
      and volume >= 0
)

select * from source_data
where is_valid_ohlc = true
  );

[0m15:26:57.029208 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:26:57.034733 [debug] [Thread-3 (]: SQL status: OK in 0.006 seconds
[0m15:26:57.036589 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:26:57.037124 [debug] [Thread-3 (]: On model.quant_features.stg_stock_prices: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_stock_prices"} */
alter view "quant_features"."main"."stg_stock_prices__dbt_tmp" rename to "stg_stock_prices"
[0m15:26:57.038697 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: COMMIT
[0m15:26:57.039577 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:26:57.040047 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: COMMIT
[0m15:26:57.040537 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m15:26:57.041441 [debug] [Thread-3 (]: On model.quant_features.stg_stock_prices: COMMIT
[0m15:26:57.041678 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:26:57.041941 [debug] [Thread-3 (]: On model.quant_features.stg_stock_prices: COMMIT
[0m15:26:57.042359 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:26:57.045881 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:26:57.046159 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_market_data"} */

      drop view if exists "quant_features"."main"."stg_market_data__dbt_backup" cascade
    
[0m15:26:57.046529 [debug] [Thread-3 (]: SQL status: OK in 0.004 seconds
[0m15:26:57.047988 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:26:57.048276 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:26:57.048631 [debug] [Thread-3 (]: On model.quant_features.stg_stock_prices: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_stock_prices"} */

      drop view if exists "quant_features"."main"."stg_stock_prices__dbt_backup" cascade
    
[0m15:26:57.050931 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: Close
[0m15:26:57.051812 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:26:57.052913 [debug] [Thread-3 (]: On model.quant_features.stg_stock_prices: Close
[0m15:26:57.054499 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b9de5e66-0c9a-407b-8ac2-3599d5e61949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05ac4947d0>]}
[0m15:26:57.054825 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b9de5e66-0c9a-407b-8ac2-3599d5e61949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05adb552e0>]}
[0m15:26:57.056884 [debug] [Thread-2 (]: Failed to rollback 'model.quant_features.stg_ohlc_data'
[0m15:26:57.057550 [info ] [Thread-3 (]: 3 of 15 OK created sql view model main.stg_stock_prices ........................ [[32mOK[0m in 0.08s]
[0m15:26:57.058125 [info ] [Thread-1 (]: 1 of 15 OK created sql view model main.stg_market_data ......................... [[32mOK[0m in 0.08s]
[0m15:26:57.058548 [debug] [Thread-2 (]: On model.quant_features.stg_ohlc_data: Close
[0m15:26:57.059023 [debug] [Thread-3 (]: Finished running node model.quant_features.stg_stock_prices
[0m15:26:57.059524 [debug] [Thread-1 (]: Finished running node model.quant_features.stg_market_data
[0m15:26:57.062310 [debug] [Thread-2 (]: Runtime Error in model stg_ohlc_data (models/staging/stg_ohlc_data.sql)
  Catalog Error: Table with name ohlc_data does not exist!
  Did you mean "pg_catalog.pg_database"?
  
  LINE 23:     from "quant_features"."raw"."ohlc_data"
                    ^
[0m15:26:57.062636 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b9de5e66-0c9a-407b-8ac2-3599d5e61949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05ad0e9450>]}
[0m15:26:57.063084 [error] [Thread-2 (]: 2 of 15 ERROR creating sql view model main.stg_ohlc_data ....................... [[31mERROR[0m in 0.09s]
[0m15:26:57.063430 [debug] [Thread-2 (]: Finished running node model.quant_features.stg_ohlc_data
[0m15:26:57.063848 [debug] [Thread-7 (]: Marking all children of 'model.quant_features.stg_ohlc_data' to be skipped because of status 'error'.  Reason: Runtime Error in model stg_ohlc_data (models/staging/stg_ohlc_data.sql)
  Catalog Error: Table with name ohlc_data does not exist!
  Did you mean "pg_catalog.pg_database"?
  
  LINE 23:     from "quant_features"."raw"."ohlc_data"
                    ^.
[0m15:26:57.064720 [debug] [Thread-4 (]: Began running node model.quant_features.daily_stock_summary
[0m15:26:57.065116 [info ] [Thread-4 (]: 4 of 15 START sql table model main.daily_stock_summary ......................... [RUN]
[0m15:26:57.065652 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.quant_features.daily_stock_summary'
[0m15:26:57.065987 [debug] [Thread-4 (]: Began compiling node model.quant_features.daily_stock_summary
[0m15:26:57.066238 [debug] [Thread-3 (]: Began running node model.quant_features.alpha_base_data
[0m15:26:57.068309 [debug] [Thread-4 (]: Writing injected SQL for node "model.quant_features.daily_stock_summary"
[0m15:26:57.068679 [debug] [Thread-1 (]: Began running node model.quant_features.mart_technical_indicators
[0m15:26:57.069079 [info ] [Thread-3 (]: 5 of 15 SKIP relation main.alpha_base_data ..................................... [[33mSKIP[0m]
[0m15:26:57.069607 [info ] [Thread-1 (]: 6 of 15 SKIP relation main.mart_technical_indicators ........................... [[33mSKIP[0m]
[0m15:26:57.070036 [debug] [Thread-3 (]: Finished running node model.quant_features.alpha_base_data
[0m15:26:57.070453 [debug] [Thread-1 (]: Finished running node model.quant_features.mart_technical_indicators
[0m15:26:57.070994 [debug] [Thread-3 (]: Began running node model.quant_features.alpha_factors_021_050
[0m15:26:57.071240 [info ] [Thread-3 (]: 8 of 15 SKIP relation main.alpha_factors_021_050 ............................... [[33mSKIP[0m]
[0m15:26:57.071535 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_factors_051_075
[0m15:26:57.071894 [debug] [Thread-2 (]: Began running node model.quant_features.alpha_factors_001_020
[0m15:26:57.072194 [debug] [Thread-3 (]: Finished running node model.quant_features.alpha_factors_021_050
[0m15:26:57.072532 [debug] [Thread-4 (]: Began executing node model.quant_features.daily_stock_summary
[0m15:26:57.072936 [info ] [Thread-1 (]: 9 of 15 SKIP relation main.alpha_factors_051_075 ............................... [[33mSKIP[0m]
[0m15:26:57.073355 [info ] [Thread-2 (]: 7 of 15 SKIP relation main.alpha_factors_001_020 ............................... [[33mSKIP[0m]
[0m15:26:57.073985 [debug] [Thread-3 (]: Began running node model.quant_features.alpha_factors_076_101
[0m15:26:57.084594 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_factors_051_075
[0m15:26:57.087120 [debug] [Thread-4 (]: Writing runtime sql for node "model.quant_features.daily_stock_summary"
[0m15:26:57.087669 [debug] [Thread-2 (]: Finished running node model.quant_features.alpha_factors_001_020
[0m15:26:57.088056 [info ] [Thread-3 (]: 10 of 15 SKIP relation main.alpha_factors_076_101 .............................. [[33mSKIP[0m]
[0m15:26:57.088423 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_factors_advanced
[0m15:26:57.088903 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:26:57.089413 [debug] [Thread-2 (]: Began running node model.quant_features.features_ohlc_technical
[0m15:26:57.089753 [debug] [Thread-3 (]: Finished running node model.quant_features.alpha_factors_076_101
[0m15:26:57.090340 [info ] [Thread-1 (]: 11 of 15 SKIP relation main.alpha_factors_advanced ............................. [[33mSKIP[0m]
[0m15:26:57.090710 [debug] [Thread-4 (]: On model.quant_features.daily_stock_summary: BEGIN
[0m15:26:57.091061 [info ] [Thread-2 (]: 12 of 15 SKIP relation main.features_ohlc_technical ............................ [[33mSKIP[0m]
[0m15:26:57.091521 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_factors_advanced
[0m15:26:57.091839 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m15:26:57.092123 [debug] [Thread-2 (]: Finished running node model.quant_features.features_ohlc_technical
[0m15:26:57.092561 [debug] [Thread-3 (]: Began running node model.quant_features.alpha101_complete
[0m15:26:57.092827 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_factors_final
[0m15:26:57.093574 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m15:26:57.093915 [info ] [Thread-3 (]: 13 of 15 SKIP relation main.alpha101_complete .................................. [[33mSKIP[0m]
[0m15:26:57.094250 [info ] [Thread-1 (]: 14 of 15 SKIP relation main.alpha_factors_final ................................ [[33mSKIP[0m]
[0m15:26:57.094688 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:26:57.095068 [debug] [Thread-3 (]: Finished running node model.quant_features.alpha101_complete
[0m15:26:57.095451 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_factors_final
[0m15:26:57.095897 [debug] [Thread-4 (]: On model.quant_features.daily_stock_summary: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.daily_stock_summary"} */

  
    
    

    create  table
      "quant_features"."main"."daily_stock_summary__dbt_tmp"
  
    as (
      

with price_data as (
    select * from "quant_features"."main"."stg_stock_prices"
),

market_data as (
    select * from "quant_features"."main"."stg_market_data"
),

combined_data as (
    select
        p.date,
        p.symbol,
        p.open,
        p.high,
        p.low,
        p.close,
        p.volume,
        p.daily_range,
        p.daily_change,
        p.daily_return,
        m.market_cap,
        m.pe_ratio,
        m.dividend_yield,
        m.sector,
        m.estimated_earnings,
        m.estimated_dividend_payout,
        -- 计算额外的技术指标
        (p.high + p.low + p.close) / 3 as typical_price,
        p.volume * p.close as dollar_volume,
        case 
            when p.daily_return > 0.05 then 'Strong Up'
            when p.daily_return > 0.02 then 'Up'
            when p.daily_return > -0.02 then 'Flat'
            when p.daily_return > -0.05 then 'Down'
            else 'Strong Down'
        end as price_movement_category
    from price_data p
    left join market_data m
        on p.date = m.date
        and p.symbol = m.symbol
)

select * from combined_data
    );
  
  
[0m15:26:57.100755 [debug] [Thread-4 (]: SQL status: OK in 0.004 seconds
[0m15:26:57.102478 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:26:57.102747 [debug] [Thread-4 (]: On model.quant_features.daily_stock_summary: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.daily_stock_summary"} */
alter table "quant_features"."main"."daily_stock_summary__dbt_tmp" rename to "daily_stock_summary"
[0m15:26:57.103219 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m15:26:57.106267 [debug] [Thread-4 (]: On model.quant_features.daily_stock_summary: COMMIT
[0m15:26:57.106542 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:26:57.106733 [debug] [Thread-4 (]: On model.quant_features.daily_stock_summary: COMMIT
[0m15:26:57.108696 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m15:26:57.110402 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:26:57.110668 [debug] [Thread-4 (]: On model.quant_features.daily_stock_summary: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.daily_stock_summary"} */

      drop table if exists "quant_features"."main"."daily_stock_summary__dbt_backup" cascade
    
[0m15:26:57.111086 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m15:26:57.111915 [debug] [Thread-4 (]: On model.quant_features.daily_stock_summary: Close
[0m15:26:57.112392 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b9de5e66-0c9a-407b-8ac2-3599d5e61949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05b206b550>]}
[0m15:26:57.112774 [info ] [Thread-4 (]: 4 of 15 OK created sql table model main.daily_stock_summary .................... [[32mOK[0m in 0.05s]
[0m15:26:57.113141 [debug] [Thread-4 (]: Finished running node model.quant_features.daily_stock_summary
[0m15:26:57.113624 [debug] [Thread-2 (]: Began running node model.quant_features.stock_features
[0m15:26:57.113924 [info ] [Thread-2 (]: 15 of 15 START sql table model main.stock_features ............................. [RUN]
[0m15:26:57.114253 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.quant_features.stg_ohlc_data, now model.quant_features.stock_features)
[0m15:26:57.114462 [debug] [Thread-2 (]: Began compiling node model.quant_features.stock_features
[0m15:26:57.116280 [debug] [Thread-2 (]: Writing injected SQL for node "model.quant_features.stock_features"
[0m15:26:57.116757 [debug] [Thread-2 (]: Began executing node model.quant_features.stock_features
[0m15:26:57.118763 [debug] [Thread-2 (]: Writing runtime sql for node "model.quant_features.stock_features"
[0m15:26:57.119225 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:26:57.119445 [debug] [Thread-2 (]: On model.quant_features.stock_features: BEGIN
[0m15:26:57.119626 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:26:57.120020 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:26:57.120226 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:26:57.120469 [debug] [Thread-2 (]: On model.quant_features.stock_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stock_features"} */

  
    
    

    create  table
      "quant_features"."main"."stock_features__dbt_tmp"
  
    as (
      

with daily_data as (
    select * from "quant_features"."main"."daily_stock_summary"
),

windowed_features as (
    select
        *,
        -- 移动平均
        avg(close) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        ) as ma_5d,
        
        avg(close) over (
            partition by symbol 
            order by date 
            rows between 9 preceding and current row
        ) as ma_10d,
        
        -- 波动率（标准差）
        stddev(daily_return) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        ) as volatility_5d,
        
        -- 价格相对位置
        (close - min(low) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        )) / (max(high) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        ) - min(low) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        )) as price_position_5d,
        
        -- 成交量相对强度
        volume / avg(volume) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        ) as volume_ratio_5d
        
    from daily_data
),

final_features as (
    select
        *,
        -- 技术信号
        case 
            when close > ma_5d and ma_5d > ma_10d then 'Bullish'
            when close < ma_5d and ma_5d < ma_10d then 'Bearish'
            else 'Neutral'
        end as trend_signal,
        
        case 
            when volatility_5d > 0.03 then 'High'
            when volatility_5d > 0.01 then 'Medium'
            else 'Low'
        end as volatility_category
        
    from windowed_features
)

select * from final_features
    );
  
  
[0m15:26:57.125899 [debug] [Thread-2 (]: SQL status: OK in 0.005 seconds
[0m15:26:57.127592 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:26:57.127864 [debug] [Thread-2 (]: On model.quant_features.stock_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stock_features"} */
alter table "quant_features"."main"."stock_features__dbt_tmp" rename to "stock_features"
[0m15:26:57.128348 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:26:57.129195 [debug] [Thread-2 (]: On model.quant_features.stock_features: COMMIT
[0m15:26:57.129440 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:26:57.129635 [debug] [Thread-2 (]: On model.quant_features.stock_features: COMMIT
[0m15:26:57.131312 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:26:57.132683 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:26:57.132943 [debug] [Thread-2 (]: On model.quant_features.stock_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stock_features"} */

      drop table if exists "quant_features"."main"."stock_features__dbt_backup" cascade
    
[0m15:26:57.133348 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:26:57.134170 [debug] [Thread-2 (]: On model.quant_features.stock_features: Close
[0m15:26:57.134813 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b9de5e66-0c9a-407b-8ac2-3599d5e61949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05ac56f9a0>]}
[0m15:26:57.135208 [info ] [Thread-2 (]: 15 of 15 OK created sql table model main.stock_features ........................ [[32mOK[0m in 0.02s]
[0m15:26:57.135513 [debug] [Thread-2 (]: Finished running node model.quant_features.stock_features
[0m15:26:57.136653 [debug] [MainThread]: Using duckdb connection "master"
[0m15:26:57.136926 [debug] [MainThread]: On master: BEGIN
[0m15:26:57.137089 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:26:57.137439 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:26:57.137616 [debug] [MainThread]: On master: COMMIT
[0m15:26:57.137772 [debug] [MainThread]: Using duckdb connection "master"
[0m15:26:57.137934 [debug] [MainThread]: On master: COMMIT
[0m15:26:57.138188 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:26:57.138357 [debug] [MainThread]: On master: Close
[0m15:26:57.138631 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:26:57.138790 [debug] [MainThread]: Connection 'model.quant_features.stg_market_data' was properly closed.
[0m15:26:57.138941 [debug] [MainThread]: Connection 'model.quant_features.stock_features' was properly closed.
[0m15:26:57.139072 [debug] [MainThread]: Connection 'model.quant_features.stg_stock_prices' was properly closed.
[0m15:26:57.139204 [debug] [MainThread]: Connection 'model.quant_features.daily_stock_summary' was properly closed.
[0m15:26:57.139419 [info ] [MainThread]: 
[0m15:26:57.139618 [info ] [MainThread]: Finished running 12 table models, 3 view models in 0 hours 0 minutes and 0.27 seconds (0.27s).
[0m15:26:57.140395 [debug] [MainThread]: Command end result
[0m15:26:57.163073 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:26:57.164191 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:26:57.168089 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:26:57.168322 [info ] [MainThread]: 
[0m15:26:57.168544 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:26:57.168715 [info ] [MainThread]: 
[0m15:26:57.168931 [error] [MainThread]: [31mFailure in model stg_ohlc_data (models/staging/stg_ohlc_data.sql)[0m
[0m15:26:57.169129 [error] [MainThread]:   Runtime Error in model stg_ohlc_data (models/staging/stg_ohlc_data.sql)
  Catalog Error: Table with name ohlc_data does not exist!
  Did you mean "pg_catalog.pg_database"?
  
  LINE 23:     from "quant_features"."raw"."ohlc_data"
                    ^
[0m15:26:57.169279 [info ] [MainThread]: 
[0m15:26:57.169469 [info ] [MainThread]:   compiled code at target/compiled/quant_features/models/staging/stg_ohlc_data.sql
[0m15:26:57.169618 [info ] [MainThread]: 
[0m15:26:57.169780 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=1 SKIP=10 NO-OP=0 TOTAL=15
[0m15:26:57.170320 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.757745, "process_in_blocks": "0", "process_kernel_time": 0.148485, "process_mem_max_rss": "160840", "process_out_blocks": "3680", "process_user_time": 1.504961}
[0m15:26:57.170611 [debug] [MainThread]: Command `dbt run` failed at 15:26:57.170551 after 0.76 seconds
[0m15:26:57.170830 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05adb3c0b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05b2166570>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05ae03fdd0>]}
[0m15:26:57.171039 [debug] [MainThread]: Flushing usage events
[0m15:26:57.212220 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:27:19.361235 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90cfad3770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90d1105a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90ce947d90>]}


============================== 15:27:19.363613 | 546a7400-bea5-4d62-9693-c27be28811e4 ==============================
[0m15:27:19.363613 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:27:19.363934 [debug] [MainThread]: running dbt with arguments {'log_path': '/workspace/dbt_project/logs', 'static_parser': 'True', 'fail_fast': 'False', 'no_print': 'None', 'version_check': 'True', 'use_colors': 'True', 'introspect': 'True', 'target_path': 'None', 'use_experimental_parser': 'False', 'invocation_command': 'dbt run', 'cache_selected_only': 'False', 'debug': 'False', 'indirect_selection': 'eager', 'warn_error': 'None', 'profiles_dir': '/workspace/dbt_project', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'log_cache_events': 'False', 'empty': 'False', 'printer_width': '80', 'write_json': 'True', 'log_format': 'default', 'quiet': 'False', 'partial_parse': 'True'}
[0m15:27:19.499701 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '546a7400-bea5-4d62-9693-c27be28811e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90cf65e650>]}
[0m15:27:19.541214 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '546a7400-bea5-4d62-9693-c27be28811e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90cea42be0>]}
[0m15:27:19.542209 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:27:19.571543 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:27:19.657453 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:27:19.657918 [debug] [MainThread]: Partial parsing: updated file: quant_features://models/staging/stg_ohlc_data.sql
[0m15:27:19.977364 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '546a7400-bea5-4d62-9693-c27be28811e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90ca51e250>]}
[0m15:27:20.046399 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:27:20.047614 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:27:20.058917 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '546a7400-bea5-4d62-9693-c27be28811e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90cb28d220>]}
[0m15:27:20.059292 [info ] [MainThread]: Found 15 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:27:20.059507 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '546a7400-bea5-4d62-9693-c27be28811e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90ca99ab30>]}
[0m15:27:20.061329 [info ] [MainThread]: 
[0m15:27:20.061604 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:27:20.061773 [info ] [MainThread]: 
[0m15:27:20.062049 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:27:20.066050 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features'
[0m15:27:20.081967 [debug] [ThreadPool]: Using duckdb connection "list_quant_features"
[0m15:27:20.082237 [debug] [ThreadPool]: On list_quant_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"quant_features"'
    
  
  
[0m15:27:20.082417 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:27:20.092613 [debug] [ThreadPool]: SQL status: OK in 0.010 seconds
[0m15:27:20.093480 [debug] [ThreadPool]: On list_quant_features: Close
[0m15:27:20.094266 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_quant_features, now create_quant_features_main)
[0m15:27:20.094651 [debug] [ThreadPool]: Creating schema "database: "quant_features"
schema: "main"
"
[0m15:27:20.098933 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:27:20.099206 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='quant_features'
        and type='sqlite'
    
  
[0m15:27:20.099388 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:27:20.100314 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:27:20.101150 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:27:20.101370 [debug] [ThreadPool]: On create_quant_features_main: BEGIN
[0m15:27:20.101689 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:27:20.101851 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:27:20.101997 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
    
        create schema if not exists "quant_features"."main"
    
[0m15:27:20.102327 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:27:20.102866 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:27:20.103090 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:27:20.103312 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:27:20.103715 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:27:20.103905 [debug] [ThreadPool]: On create_quant_features_main: Close
[0m15:27:20.105485 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_quant_features_main, now list_quant_features_main)
[0m15:27:20.109256 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:27:20.109509 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:27:20.109684 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:27:20.110095 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:27:20.110300 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:27:20.110472 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:27:20.117055 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m15:27:20.117986 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:27:20.118624 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:27:20.118838 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:27:20.120386 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '546a7400-bea5-4d62-9693-c27be28811e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90c9137790>]}
[0m15:27:20.120813 [debug] [MainThread]: Using duckdb connection "master"
[0m15:27:20.121011 [debug] [MainThread]: On master: BEGIN
[0m15:27:20.121166 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:27:20.121589 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:27:20.121783 [debug] [MainThread]: On master: COMMIT
[0m15:27:20.121940 [debug] [MainThread]: Using duckdb connection "master"
[0m15:27:20.122090 [debug] [MainThread]: On master: COMMIT
[0m15:27:20.122404 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:27:20.122604 [debug] [MainThread]: On master: Close
[0m15:27:20.124560 [debug] [Thread-3 (]: Began running node model.quant_features.stg_stock_prices
[0m15:27:20.124925 [info ] [Thread-3 (]: 3 of 15 START sql view model main.stg_stock_prices ............................. [RUN]
[0m15:27:20.125253 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.quant_features.stg_stock_prices'
[0m15:27:20.125459 [debug] [Thread-3 (]: Began compiling node model.quant_features.stg_stock_prices
[0m15:27:20.125826 [debug] [Thread-2 (]: Began running node model.quant_features.stg_ohlc_data
[0m15:27:20.130600 [debug] [Thread-3 (]: Writing injected SQL for node "model.quant_features.stg_stock_prices"
[0m15:27:20.131020 [debug] [Thread-1 (]: Began running node model.quant_features.stg_market_data
[0m15:27:20.131594 [info ] [Thread-2 (]: 2 of 15 START sql view model main.stg_ohlc_data ................................ [RUN]
[0m15:27:20.132248 [info ] [Thread-1 (]: 1 of 15 START sql view model main.stg_market_data .............................. [RUN]
[0m15:27:20.132824 [debug] [Thread-3 (]: Began executing node model.quant_features.stg_stock_prices
[0m15:27:20.133424 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.quant_features.stg_ohlc_data'
[0m15:27:20.133934 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.stg_market_data)
[0m15:27:20.144896 [debug] [Thread-2 (]: Began compiling node model.quant_features.stg_ohlc_data
[0m15:27:20.154445 [debug] [Thread-3 (]: Writing runtime sql for node "model.quant_features.stg_stock_prices"
[0m15:27:20.154963 [debug] [Thread-1 (]: Began compiling node model.quant_features.stg_market_data
[0m15:27:20.157774 [debug] [Thread-2 (]: Writing injected SQL for node "model.quant_features.stg_ohlc_data"
[0m15:27:20.158265 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:27:20.160312 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.stg_market_data"
[0m15:27:20.160967 [debug] [Thread-2 (]: Began executing node model.quant_features.stg_ohlc_data
[0m15:27:20.161395 [debug] [Thread-3 (]: On model.quant_features.stg_stock_prices: BEGIN
[0m15:27:20.162053 [debug] [Thread-1 (]: Began executing node model.quant_features.stg_market_data
[0m15:27:20.164518 [debug] [Thread-2 (]: Writing runtime sql for node "model.quant_features.stg_ohlc_data"
[0m15:27:20.165045 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:27:20.167085 [debug] [Thread-1 (]: Writing runtime sql for node "model.quant_features.stg_market_data"
[0m15:27:20.167716 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stg_ohlc_data"
[0m15:27:20.168256 [debug] [Thread-2 (]: On model.quant_features.stg_ohlc_data: BEGIN
[0m15:27:20.168480 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:27:20.168930 [debug] [Thread-3 (]: SQL status: OK in 0.004 seconds
[0m15:27:20.169229 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:27:20.169462 [debug] [Thread-3 (]: On model.quant_features.stg_stock_prices: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_stock_prices"} */

  
  create view "quant_features"."main"."stg_stock_prices__dbt_tmp" as (
    

with source_data as (
    select
        date,
        symbol,
        open,
        high,
        low,
        close,
        volume,
        -- 计算基础技术指标
        (high - low) as daily_range,
        (close - open) as daily_change,
        (close - open) / open as daily_return,
        -- 添加数据质量检查
        case 
            when high >= low and high >= open and high >= close 
                 and low <= open and low <= close 
            then true 
            else false 
        end as is_valid_ohlc
    from "quant_features"."main"."raw_stock_prices"
    where date is not null
      and symbol is not null
      and open > 0
      and high > 0
      and low > 0
      and close > 0
      and volume >= 0
)

select * from source_data
where is_valid_ohlc = true
  );

[0m15:27:20.169953 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:27:20.170325 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m15:27:20.170661 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: BEGIN
[0m15:27:20.171022 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m15:27:20.171342 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stg_ohlc_data"
[0m15:27:20.171613 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:27:20.175825 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:27:20.176390 [debug] [Thread-2 (]: On model.quant_features.stg_ohlc_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_ohlc_data"} */

  
  create view "quant_features"."main"."stg_ohlc_data__dbt_tmp" as (
    

with raw_ohlc as (
    select 
        symbol,
        date as timestamp,
        open,
        high,
        low,
        close,
        volume,
        -- 数据清洗和验证
        case 
            when open <= 0 or high <= 0 or low <= 0 or close <= 0 then null
            when high < greatest(open, close, low) then null
            when low > least(open, close, high) then null
            else date
        end as valid_timestamp
    from "quant_features"."main"."raw_stock_prices"
),

cleaned_ohlc as (
    select 
        symbol,
        timestamp,
        open,
        high,
        low,
        close,
        volume,
        -- 计算基础指标
        (high + low + close) / 3 as typical_price,
        (high - low) as daily_range,
        case when open != 0 then (close - open) / open else 0 end as daily_return,
        case when close != 0 then volume / close else 0 end as volume_price_ratio
    from raw_ohlc
    where valid_timestamp is not null
      and timestamp >= '2020-01-01'
      and timestamp <= '2024-12-31'
)

select * from cleaned_ohlc
  );

[0m15:27:20.177210 [debug] [Thread-3 (]: On model.quant_features.stg_stock_prices: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_stock_prices"} */
alter view "quant_features"."main"."stg_stock_prices" rename to "stg_stock_prices__dbt_backup"
[0m15:27:20.177747 [debug] [Thread-1 (]: SQL status: OK in 0.006 seconds
[0m15:27:20.178489 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:27:20.178818 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_market_data"} */

  
  create view "quant_features"."main"."stg_market_data__dbt_tmp" as (
    

with source_data as (
    select
        date,
        symbol,
        market_cap,
        pe_ratio,
        dividend_yield,
        sector,
        -- 添加计算字段
        case 
            when pe_ratio > 0 then market_cap / pe_ratio 
            else null 
        end as estimated_earnings,
        case 
            when dividend_yield > 0 then market_cap * dividend_yield / 100 
            else 0 
        end as estimated_dividend_payout
    from "quant_features"."main"."market_data"
    where date is not null
      and symbol is not null
      and market_cap > 0
)

select * from source_data
  );

[0m15:27:20.179124 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m15:27:20.179464 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:27:20.181248 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:27:20.183879 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stg_ohlc_data"
[0m15:27:20.184214 [debug] [Thread-3 (]: On model.quant_features.stg_stock_prices: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_stock_prices"} */
alter view "quant_features"."main"."stg_stock_prices__dbt_tmp" rename to "stg_stock_prices"
[0m15:27:20.184510 [debug] [Thread-2 (]: On model.quant_features.stg_ohlc_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_ohlc_data"} */
alter view "quant_features"."main"."stg_ohlc_data__dbt_tmp" rename to "stg_ohlc_data"
[0m15:27:20.185035 [debug] [Thread-1 (]: SQL status: OK in 0.006 seconds
[0m15:27:20.186012 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:27:20.188116 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:27:20.188495 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m15:27:20.196317 [debug] [Thread-3 (]: On model.quant_features.stg_stock_prices: COMMIT
[0m15:27:20.196830 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_market_data"} */
alter view "quant_features"."main"."stg_market_data" rename to "stg_market_data__dbt_backup"
[0m15:27:20.197910 [debug] [Thread-2 (]: On model.quant_features.stg_ohlc_data: COMMIT
[0m15:27:20.198382 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:27:20.199157 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stg_ohlc_data"
[0m15:27:20.199665 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:27:20.200144 [debug] [Thread-3 (]: On model.quant_features.stg_stock_prices: COMMIT
[0m15:27:20.200656 [debug] [Thread-2 (]: On model.quant_features.stg_ohlc_data: COMMIT
[0m15:27:20.202875 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:27:20.203709 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_market_data"} */
alter view "quant_features"."main"."stg_market_data__dbt_tmp" rename to "stg_market_data"
[0m15:27:20.204289 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:27:20.205121 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: COMMIT
[0m15:27:20.205463 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m15:27:20.205842 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:27:20.206256 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m15:27:20.209777 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:27:20.210325 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: COMMIT
[0m15:27:20.212346 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stg_ohlc_data"
[0m15:27:20.212913 [debug] [Thread-3 (]: On model.quant_features.stg_stock_prices: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_stock_prices"} */

      drop view if exists "quant_features"."main"."stg_stock_prices__dbt_backup" cascade
    
[0m15:27:20.213583 [debug] [Thread-2 (]: On model.quant_features.stg_ohlc_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_ohlc_data"} */

      drop view if exists "quant_features"."main"."stg_ohlc_data__dbt_backup" cascade
    
[0m15:27:20.214765 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:27:20.216161 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:27:20.216507 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m15:27:20.216818 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_market_data"} */

      drop view if exists "quant_features"."main"."stg_market_data__dbt_backup" cascade
    
[0m15:27:20.218644 [debug] [Thread-3 (]: On model.quant_features.stg_stock_prices: Close
[0m15:27:20.219117 [debug] [Thread-2 (]: SQL status: OK in 0.005 seconds
[0m15:27:20.220217 [debug] [Thread-2 (]: On model.quant_features.stg_ohlc_data: Close
[0m15:27:20.221364 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '546a7400-bea5-4d62-9693-c27be28811e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90c90b2210>]}
[0m15:27:20.221786 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m15:27:20.222337 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '546a7400-bea5-4d62-9693-c27be28811e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90ca4dad00>]}
[0m15:27:20.222823 [info ] [Thread-3 (]: 3 of 15 OK created sql view model main.stg_stock_prices ........................ [[32mOK[0m in 0.10s]
[0m15:27:20.223817 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: Close
[0m15:27:20.224236 [info ] [Thread-2 (]: 2 of 15 OK created sql view model main.stg_ohlc_data ........................... [[32mOK[0m in 0.09s]
[0m15:27:20.224671 [debug] [Thread-3 (]: Finished running node model.quant_features.stg_stock_prices
[0m15:27:20.225073 [debug] [Thread-2 (]: Finished running node model.quant_features.stg_ohlc_data
[0m15:27:20.225422 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '546a7400-bea5-4d62-9693-c27be28811e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90cb2dfc50>]}
[0m15:27:20.226378 [debug] [Thread-4 (]: Began running node model.quant_features.alpha_base_data
[0m15:27:20.226769 [info ] [Thread-1 (]: 1 of 15 OK created sql view model main.stg_market_data ......................... [[32mOK[0m in 0.09s]
[0m15:27:20.227295 [debug] [Thread-3 (]: Began running node model.quant_features.mart_technical_indicators
[0m15:27:20.227700 [info ] [Thread-4 (]: 4 of 15 START sql table model main.alpha_base_data ............................. [RUN]
[0m15:27:20.228072 [debug] [Thread-1 (]: Finished running node model.quant_features.stg_market_data
[0m15:27:20.228836 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.quant_features.alpha_base_data'
[0m15:27:20.229220 [debug] [Thread-4 (]: Began compiling node model.quant_features.alpha_base_data
[0m15:27:20.228493 [info ] [Thread-3 (]: 5 of 15 START sql table model main.mart_technical_indicators ................... [RUN]
[0m15:27:20.240134 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.quant_features.stg_stock_prices, now model.quant_features.mart_technical_indicators)
[0m15:27:20.240474 [debug] [Thread-3 (]: Began compiling node model.quant_features.mart_technical_indicators
[0m15:27:20.242500 [debug] [Thread-3 (]: Writing injected SQL for node "model.quant_features.mart_technical_indicators"
[0m15:27:20.252238 [debug] [Thread-4 (]: Writing injected SQL for node "model.quant_features.alpha_base_data"
[0m15:27:20.252607 [debug] [Thread-3 (]: Began executing node model.quant_features.mart_technical_indicators
[0m15:27:20.258414 [debug] [Thread-2 (]: Began running node model.quant_features.daily_stock_summary
[0m15:27:20.258819 [info ] [Thread-2 (]: 6 of 15 START sql table model main.daily_stock_summary ......................... [RUN]
[0m15:27:20.266248 [debug] [Thread-3 (]: Writing runtime sql for node "model.quant_features.mart_technical_indicators"
[0m15:27:20.266795 [debug] [Thread-4 (]: Began executing node model.quant_features.alpha_base_data
[0m15:27:20.267315 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.quant_features.stg_ohlc_data, now model.quant_features.daily_stock_summary)
[0m15:27:20.269501 [debug] [Thread-4 (]: Writing runtime sql for node "model.quant_features.alpha_base_data"
[0m15:27:20.270020 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.mart_technical_indicators"
[0m15:27:20.270592 [debug] [Thread-2 (]: Began compiling node model.quant_features.daily_stock_summary
[0m15:27:20.271076 [debug] [Thread-3 (]: On model.quant_features.mart_technical_indicators: BEGIN
[0m15:27:20.275681 [debug] [Thread-2 (]: Writing injected SQL for node "model.quant_features.daily_stock_summary"
[0m15:27:20.276155 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:27:20.276618 [debug] [Thread-2 (]: Began executing node model.quant_features.daily_stock_summary
[0m15:27:20.278814 [debug] [Thread-2 (]: Writing runtime sql for node "model.quant_features.daily_stock_summary"
[0m15:27:20.279331 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.alpha_base_data"
[0m15:27:20.279736 [debug] [Thread-3 (]: SQL status: OK in 0.004 seconds
[0m15:27:20.280092 [debug] [Thread-4 (]: On model.quant_features.alpha_base_data: BEGIN
[0m15:27:20.280437 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:27:20.280859 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.mart_technical_indicators"
[0m15:27:20.281227 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m15:27:20.281725 [debug] [Thread-2 (]: On model.quant_features.daily_stock_summary: BEGIN
[0m15:27:20.282233 [debug] [Thread-3 (]: On model.quant_features.mart_technical_indicators: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.mart_technical_indicators"} */

  
    
    

    create  table
      "quant_features"."main"."mart_technical_indicators__dbt_tmp"
  
    as (
      

with base_data as (
    select * from "quant_features"."main"."stg_ohlc_data"
),

technical_indicators as (
    select 
        symbol,
        timestamp,
        open,
        high,
        low,
        close,
        volume,
        typical_price,
        daily_range,
        daily_return,
        volume_price_ratio,
        
        -- 移动平均线
        avg(close) over (
            partition by symbol 
            order by timestamp 
            rows between 4 preceding and current row
        ) as ma_5,
        
        avg(close) over (
            partition by symbol 
            order by timestamp 
            rows between 9 preceding and current row
        ) as ma_10,
        
        avg(close) over (
            partition by symbol 
            order by timestamp 
            rows between 19 preceding and current row
        ) as ma_20,
        
        -- 波动率 (标准差)
        stddev(daily_return) over (
            partition by symbol 
            order by timestamp 
            rows between 19 preceding and current row
        ) as volatility_20d,
        
        -- RSI相关计算
        case when daily_return > 0 then daily_return else 0 end as gain,
        case when daily_return < 0 then abs(daily_return) else 0 end as loss,
        
        -- 价格位置指标
        (close - min(low) over (
            partition by symbol 
            order by timestamp 
            rows between 13 preceding and current row
        )) / nullif((max(high) over (
            partition by symbol 
            order by timestamp 
            rows between 13 preceding and current row
        ) - min(low) over (
            partition by symbol 
            order by timestamp 
            rows between 13 preceding and current row
        )), 0) as stoch_k_14,
        
        -- 成交量指标
        avg(volume) over (
            partition by symbol 
            order by timestamp 
            rows between 19 preceding and current row
        ) as avg_volume_20d
        
    from base_data
),

rsi_calculation as (
    select *,
        -- RSI计算
        avg(gain) over (
            partition by symbol 
            order by timestamp 
            rows between 13 preceding and current row
        ) as avg_gain_14,
        
        avg(loss) over (
            partition by symbol 
            order by timestamp 
            rows between 13 preceding and current row
        ) as avg_loss_14
    from technical_indicators
),

final_indicators as (
    select *,
        case 
            when avg_loss_14 = 0 then 100
            when avg_gain_14 = 0 then 0
            else 100 - (100 / (1 + (avg_gain_14 / avg_loss_14)))
        end as rsi_14,
        
        -- 布林带
        ma_20 + (2 * stddev(close) over (
            partition by symbol 
            order by timestamp 
            rows between 19 preceding and current row
        )) as bollinger_upper,
        
        ma_20 - (2 * stddev(close) over (
            partition by symbol 
            order by timestamp 
            rows between 19 preceding and current row
        )) as bollinger_lower,
        
        -- 价格动量
        case when lag(close, 5) over (partition by symbol order by timestamp) != 0 
            then (close - lag(close, 5) over (partition by symbol order by timestamp)) / 
                 lag(close, 5) over (partition by symbol order by timestamp)
            else 0 
        end as momentum_5d,
        
        case when lag(close, 10) over (partition by symbol order by timestamp) != 0 
            then (close - lag(close, 10) over (partition by symbol order by timestamp)) / 
                 lag(close, 10) over (partition by symbol order by timestamp)
            else 0 
        end as momentum_10d
        
    from rsi_calculation
)

select * from final_indicators
    );
  
  
[0m15:27:20.282922 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:27:20.283291 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m15:27:20.284098 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.alpha_base_data"
[0m15:27:20.284587 [debug] [Thread-4 (]: On model.quant_features.alpha_base_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_base_data"} */

  
    
    

    create  table
      "quant_features"."main"."alpha_base_data__dbt_tmp"
  
    as (
      

-- Alpha 101 基础数据准备
-- 为Alpha因子计算准备所有必要的基础数据

WITH base_ohlc AS (
    SELECT 
        symbol,
        timestamp,
        open,
        high,
        low,
        close,
        volume,
        -- 计算VWAP (简化版本，假设等权重)
        (high + low + close) / 3 AS vwap,
        -- 计算returns
        CASE 
            WHEN LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp) IS NOT NULL
            THEN (close - LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp)) / 
                 LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp)
            ELSE 0
        END AS returns
    FROM "quant_features"."main"."stg_ohlc_data"
    WHERE timestamp >= '2020-01-01'
      AND timestamp <= '2024-12-31'
),

enhanced_data AS (
    SELECT 
        *,
        -- 计算ADV (Average Daily Volume)
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )

 AS adv20,
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )

 AS adv10,
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )

 AS adv5,
        
        -- 预计算一些常用的时间序列指标
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_ma5,
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS close_ma10,
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_ma20,
        
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS volume_ma20,
        
        -- 预计算滚动标准差
        
    STDDEV(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_std20,
        
    STDDEV(returns) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS returns_std20,
        
        -- 预计算一些延迟项
        
    LAG(close, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag1,
        
    LAG(close, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag2,
        
    LAG(close, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag5,
        
    LAG(close, 10) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag10,
        
    LAG(close, 20) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag20,
        
    LAG(volume, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS volume_lag1,
        
    LAG(high, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS high_lag1,
        
    LAG(low, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS low_lag1,
        
    LAG(vwap, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS vwap_lag5,
        
        -- 预计算一些差值项
        
    close - 
    LAG(close, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta1,
        
    close - 
    LAG(close, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta2,
        
    close - 
    LAG(close, 3) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta3,
        
    close - 
    LAG(close, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta5,
        
    close - 
    LAG(close, 7) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta7,
        
    close - 
    LAG(close, 10) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta10,
        
    volume - 
    LAG(volume, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS volume_delta1,
        
    volume - 
    LAG(volume, 3) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS volume_delta3,
        
    high - 
    LAG(high, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS high_delta2,
        
        -- 预计算一些排序项
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY close
    )
 AS close_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY volume
    )
 AS volume_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY high
    )
 AS high_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY low
    )
 AS low_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY vwap
    )
 AS vwap_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY returns
    )
 AS returns_rank,
        
        -- 预计算时间序列排序
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY close
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS close_ts_rank10,
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY volume
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_ts_rank5,
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY high
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS high_ts_rank5,
        
        -- 预计算一些最值项
        
    MIN(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 99 PRECEDING AND CURRENT ROW
    )
 AS close_min100,
        
    MAX(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS close_max3,
        
    MIN(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_min5,
        
    MAX(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_max5,
        
    MIN(low) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS low_min5,
        
    MAX(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS high_max3,
        
        -- 预计算一些求和项
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_sum5,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 7 PRECEDING AND CURRENT ROW
    )
 AS close_sum8,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_sum20,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 99 PRECEDING AND CURRENT ROW
    )
 AS close_sum100,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 199 PRECEDING AND CURRENT ROW
    )
 AS close_sum200,
        
    SUM(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_sum5,
        
    SUM(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS volume_sum20,
        
    SUM(returns) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 249 PRECEDING AND CURRENT ROW
    )
 AS returns_sum250,
        
    SUM(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS high_sum5,
        
    SUM(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS high_sum20,
        
        -- 预计算一些相关性
        
    -- 使用DuckDB的CORR窗口函数
    CORR(close, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS corr_close_volume_10,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(open, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS corr_open_volume_10,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(high, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS corr_high_volume_5,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(vwap, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 5 PRECEDING AND CURRENT ROW
    )
 AS corr_vwap_volume_6,
        
        -- 预计算一些协方差
        
    COVAR_SAMP(close, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS cov_close_volume_5,
        
    COVAR_SAMP(high, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS cov_high_volume_5
        
    FROM base_ohlc
    WHERE timestamp >= '2020-01-01' - INTERVAL '250 days'  -- 扩展时间范围以确保有足够的历史数据
),

-- 过滤回原始时间范围
final_data AS (
    SELECT *
    FROM enhanced_data
    WHERE timestamp >= '2020-01-01'
      AND timestamp <= '2024-12-31'
)

SELECT * FROM final_data
    );
  
  
[0m15:27:20.285755 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m15:27:20.286068 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:27:20.286382 [debug] [Thread-2 (]: On model.quant_features.daily_stock_summary: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.daily_stock_summary"} */

  
    
    

    create  table
      "quant_features"."main"."daily_stock_summary__dbt_tmp"
  
    as (
      

with price_data as (
    select * from "quant_features"."main"."stg_stock_prices"
),

market_data as (
    select * from "quant_features"."main"."stg_market_data"
),

combined_data as (
    select
        p.date,
        p.symbol,
        p.open,
        p.high,
        p.low,
        p.close,
        p.volume,
        p.daily_range,
        p.daily_change,
        p.daily_return,
        m.market_cap,
        m.pe_ratio,
        m.dividend_yield,
        m.sector,
        m.estimated_earnings,
        m.estimated_dividend_payout,
        -- 计算额外的技术指标
        (p.high + p.low + p.close) / 3 as typical_price,
        p.volume * p.close as dollar_volume,
        case 
            when p.daily_return > 0.05 then 'Strong Up'
            when p.daily_return > 0.02 then 'Up'
            when p.daily_return > -0.02 then 'Flat'
            when p.daily_return > -0.05 then 'Down'
            else 'Strong Down'
        end as price_movement_category
    from price_data p
    left join market_data m
        on p.date = m.date
        and p.symbol = m.symbol
)

select * from combined_data
    );
  
  
[0m15:27:20.287316 [debug] [Thread-4 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_base_data"} */

  
    
    

    create  table
      "quant_features"."main"."alpha_base_data__dbt_tmp"
  
    as (
      

-- Alpha 101 基础数据准备
-- 为Alpha因子计算准备所有必要的基础数据

WITH base_ohlc AS (
    SELECT 
        symbol,
        timestamp,
        open,
        high,
        low,
        close,
        volume,
        -- 计算VWAP (简化版本，假设等权重)
        (high + low + close) / 3 AS vwap,
        -- 计算returns
        CASE 
            WHEN LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp) IS NOT NULL
            THEN (close - LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp)) / 
                 LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp)
            ELSE 0
        END AS returns
    FROM "quant_features"."main"."stg_ohlc_data"
    WHERE timestamp >= '2020-01-01'
      AND timestamp <= '2024-12-31'
),

enhanced_data AS (
    SELECT 
        *,
        -- 计算ADV (Average Daily Volume)
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )

 AS adv20,
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )

 AS adv10,
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )

 AS adv5,
        
        -- 预计算一些常用的时间序列指标
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_ma5,
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS close_ma10,
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_ma20,
        
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS volume_ma20,
        
        -- 预计算滚动标准差
        
    STDDEV(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_std20,
        
    STDDEV(returns) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS returns_std20,
        
        -- 预计算一些延迟项
        
    LAG(close, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag1,
        
    LAG(close, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag2,
        
    LAG(close, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag5,
        
    LAG(close, 10) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag10,
        
    LAG(close, 20) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag20,
        
    LAG(volume, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS volume_lag1,
        
    LAG(high, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS high_lag1,
        
    LAG(low, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS low_lag1,
        
    LAG(vwap, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS vwap_lag5,
        
        -- 预计算一些差值项
        
    close - 
    LAG(close, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta1,
        
    close - 
    LAG(close, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta2,
        
    close - 
    LAG(close, 3) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta3,
        
    close - 
    LAG(close, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta5,
        
    close - 
    LAG(close, 7) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta7,
        
    close - 
    LAG(close, 10) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta10,
        
    volume - 
    LAG(volume, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS volume_delta1,
        
    volume - 
    LAG(volume, 3) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS volume_delta3,
        
    high - 
    LAG(high, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS high_delta2,
        
        -- 预计算一些排序项
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY close
    )
 AS close_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY volume
    )
 AS volume_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY high
    )
 AS high_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY low
    )
 AS low_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY vwap
    )
 AS vwap_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY returns
    )
 AS returns_rank,
        
        -- 预计算时间序列排序
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY close
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS close_ts_rank10,
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY volume
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_ts_rank5,
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY high
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS high_ts_rank5,
        
        -- 预计算一些最值项
        
    MIN(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 99 PRECEDING AND CURRENT ROW
    )
 AS close_min100,
        
    MAX(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS close_max3,
        
    MIN(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_min5,
        
    MAX(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_max5,
        
    MIN(low) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS low_min5,
        
    MAX(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS high_max3,
        
        -- 预计算一些求和项
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_sum5,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 7 PRECEDING AND CURRENT ROW
    )
 AS close_sum8,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_sum20,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 99 PRECEDING AND CURRENT ROW
    )
 AS close_sum100,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 199 PRECEDING AND CURRENT ROW
    )
 AS close_sum200,
        
    SUM(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_sum5,
        
    SUM(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS volume_sum20,
        
    SUM(returns) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 249 PRECEDING AND CURRENT ROW
    )
 AS returns_sum250,
        
    SUM(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS high_sum5,
        
    SUM(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS high_sum20,
        
        -- 预计算一些相关性
        
    -- 使用DuckDB的CORR窗口函数
    CORR(close, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS corr_close_volume_10,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(open, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS corr_open_volume_10,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(high, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS corr_high_volume_5,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(vwap, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 5 PRECEDING AND CURRENT ROW
    )
 AS corr_vwap_volume_6,
        
        -- 预计算一些协方差
        
    COVAR_SAMP(close, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS cov_close_volume_5,
        
    COVAR_SAMP(high, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS cov_high_volume_5
        
    FROM base_ohlc
    WHERE timestamp >= '2020-01-01' - INTERVAL '250 days'  -- 扩展时间范围以确保有足够的历史数据
),

-- 过滤回原始时间范围
final_data AS (
    SELECT *
    FROM enhanced_data
    WHERE timestamp >= '2020-01-01'
      AND timestamp <= '2024-12-31'
)

SELECT * FROM final_data
    );
  
  
[0m15:27:20.287984 [debug] [Thread-4 (]: DuckDB adapter: Rolling back transaction.
[0m15:27:20.288307 [debug] [Thread-4 (]: On model.quant_features.alpha_base_data: ROLLBACK
[0m15:27:20.291607 [debug] [Thread-4 (]: Failed to rollback 'model.quant_features.alpha_base_data'
[0m15:27:20.291931 [debug] [Thread-4 (]: On model.quant_features.alpha_base_data: Close
[0m15:27:20.292472 [debug] [Thread-2 (]: SQL status: OK in 0.006 seconds
[0m15:27:20.295281 [debug] [Thread-4 (]: Runtime Error in model alpha_base_data (models/alpha101/alpha_base_data.sql)
  Binder Error: Could not choose a best candidate function for the function call "-(STRING_LITERAL, INTERVAL)". In order to select one, please add explicit type casts.
  	Candidate functions:
  	-(DATE, INTERVAL) -> TIMESTAMP
  	-(TIME, INTERVAL) -> TIME
  	-(TIMESTAMP, INTERVAL) -> TIMESTAMP
  	-(TIME WITH TIME ZONE, INTERVAL) -> TIME WITH TIME ZONE
  	-(TIMESTAMP WITH TIME ZONE, INTERVAL) -> TIMESTAMP WITH TIME ZONE
  	-(INTERVAL, INTERVAL) -> INTERVAL
  
  
  LINE 475:     WHERE timestamp >= '2020-01-01' - INTERVAL '250 days'  -- 扩展时间范围以确保有足够的历史数...
                                                ^
[0m15:27:20.297214 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:27:20.297779 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '546a7400-bea5-4d62-9693-c27be28811e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90c8f8c3d0>]}
[0m15:27:20.298178 [debug] [Thread-3 (]: SQL status: OK in 0.015 seconds
[0m15:27:20.298512 [debug] [Thread-2 (]: On model.quant_features.daily_stock_summary: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.daily_stock_summary"} */
alter table "quant_features"."main"."daily_stock_summary" rename to "daily_stock_summary__dbt_backup"
[0m15:27:20.299003 [error] [Thread-4 (]: 4 of 15 ERROR creating sql table model main.alpha_base_data .................... [[31mERROR[0m in 0.07s]
[0m15:27:20.301779 [debug] [Thread-4 (]: Finished running node model.quant_features.alpha_base_data
[0m15:27:20.300906 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.mart_technical_indicators"
[0m15:27:20.302099 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:27:20.302663 [debug] [Thread-7 (]: Marking all children of 'model.quant_features.alpha_base_data' to be skipped because of status 'error'.  Reason: Runtime Error in model alpha_base_data (models/alpha101/alpha_base_data.sql)
  Binder Error: Could not choose a best candidate function for the function call "-(STRING_LITERAL, INTERVAL)". In order to select one, please add explicit type casts.
  	Candidate functions:
  	-(DATE, INTERVAL) -> TIMESTAMP
  	-(TIME, INTERVAL) -> TIME
  	-(TIMESTAMP, INTERVAL) -> TIMESTAMP
  	-(TIME WITH TIME ZONE, INTERVAL) -> TIME WITH TIME ZONE
  	-(TIMESTAMP WITH TIME ZONE, INTERVAL) -> TIMESTAMP WITH TIME ZONE
  	-(INTERVAL, INTERVAL) -> INTERVAL
  
  
  LINE 475:     WHERE timestamp >= '2020-01-01' - INTERVAL '250 days'  -- 扩展时间范围以确保有足够的历史数...
                                                ^.
[0m15:27:20.303027 [debug] [Thread-3 (]: On model.quant_features.mart_technical_indicators: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.mart_technical_indicators"} */
alter table "quant_features"."main"."mart_technical_indicators__dbt_tmp" rename to "mart_technical_indicators"
[0m15:27:20.305162 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:27:20.306419 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_factors_001_020
[0m15:27:20.306818 [debug] [Thread-4 (]: Began running node model.quant_features.alpha_factors_021_050
[0m15:27:20.307225 [debug] [Thread-2 (]: On model.quant_features.daily_stock_summary: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.daily_stock_summary"} */
alter table "quant_features"."main"."daily_stock_summary__dbt_tmp" rename to "daily_stock_summary"
[0m15:27:20.307609 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m15:27:20.307945 [info ] [Thread-1 (]: 7 of 15 SKIP relation main.alpha_factors_001_020 ............................... [[33mSKIP[0m]
[0m15:27:20.308301 [info ] [Thread-4 (]: 8 of 15 SKIP relation main.alpha_factors_021_050 ............................... [[33mSKIP[0m]
[0m15:27:20.311660 [debug] [Thread-3 (]: On model.quant_features.mart_technical_indicators: COMMIT
[0m15:27:20.312098 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_factors_001_020
[0m15:27:20.312510 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m15:27:20.312814 [debug] [Thread-4 (]: Finished running node model.quant_features.alpha_factors_021_050
[0m15:27:20.313318 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.mart_technical_indicators"
[0m15:27:20.313690 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_factors_051_075
[0m15:27:20.314857 [debug] [Thread-2 (]: On model.quant_features.daily_stock_summary: COMMIT
[0m15:27:20.315184 [debug] [Thread-4 (]: Began running node model.quant_features.alpha_factors_076_101
[0m15:27:20.315470 [debug] [Thread-3 (]: On model.quant_features.mart_technical_indicators: COMMIT
[0m15:27:20.315757 [info ] [Thread-1 (]: 9 of 15 SKIP relation main.alpha_factors_051_075 ............................... [[33mSKIP[0m]
[0m15:27:20.316060 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:27:20.316333 [info ] [Thread-4 (]: 10 of 15 SKIP relation main.alpha_factors_076_101 .............................. [[33mSKIP[0m]
[0m15:27:20.316795 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_factors_051_075
[0m15:27:20.317127 [debug] [Thread-2 (]: On model.quant_features.daily_stock_summary: COMMIT
[0m15:27:20.317803 [debug] [Thread-4 (]: Finished running node model.quant_features.alpha_factors_076_101
[0m15:27:20.318057 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_factors_advanced
[0m15:27:20.318459 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m15:27:20.319017 [info ] [Thread-1 (]: 11 of 15 SKIP relation main.alpha_factors_advanced ............................. [[33mSKIP[0m]
[0m15:27:20.319452 [debug] [Thread-4 (]: Began running node model.quant_features.alpha101_complete
[0m15:27:20.320924 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.mart_technical_indicators"
[0m15:27:20.321291 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m15:27:20.321860 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_factors_advanced
[0m15:27:20.322284 [info ] [Thread-4 (]: 12 of 15 SKIP relation main.alpha101_complete .................................. [[33mSKIP[0m]
[0m15:27:20.322658 [debug] [Thread-3 (]: On model.quant_features.mart_technical_indicators: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.mart_technical_indicators"} */

      drop table if exists "quant_features"."main"."mart_technical_indicators__dbt_backup" cascade
    
[0m15:27:20.324516 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:27:20.325260 [debug] [Thread-4 (]: Finished running node model.quant_features.alpha101_complete
[0m15:27:20.325859 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_factors_final
[0m15:27:20.326219 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m15:27:20.326723 [debug] [Thread-2 (]: On model.quant_features.daily_stock_summary: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.daily_stock_summary"} */

      drop table if exists "quant_features"."main"."daily_stock_summary__dbt_backup" cascade
    
[0m15:27:20.327146 [info ] [Thread-1 (]: 13 of 15 SKIP relation main.alpha_factors_final ................................ [[33mSKIP[0m]
[0m15:27:20.328100 [debug] [Thread-3 (]: On model.quant_features.mart_technical_indicators: Close
[0m15:27:20.328711 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_factors_final
[0m15:27:20.330150 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '546a7400-bea5-4d62-9693-c27be28811e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90c810b850>]}
[0m15:27:20.330515 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m15:27:20.330997 [info ] [Thread-3 (]: 5 of 15 OK created sql table model main.mart_technical_indicators .............. [[32mOK[0m in 0.09s]
[0m15:27:20.331963 [debug] [Thread-2 (]: On model.quant_features.daily_stock_summary: Close
[0m15:27:20.332460 [debug] [Thread-3 (]: Finished running node model.quant_features.mart_technical_indicators
[0m15:27:20.333022 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '546a7400-bea5-4d62-9693-c27be28811e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90ca44c3b0>]}
[0m15:27:20.333386 [debug] [Thread-4 (]: Began running node model.quant_features.features_ohlc_technical
[0m15:27:20.334039 [info ] [Thread-2 (]: 6 of 15 OK created sql table model main.daily_stock_summary .................... [[32mOK[0m in 0.07s]
[0m15:27:20.334494 [info ] [Thread-4 (]: 14 of 15 START sql table model main.features_ohlc_technical .................... [RUN]
[0m15:27:20.334878 [debug] [Thread-2 (]: Finished running node model.quant_features.daily_stock_summary
[0m15:27:20.335305 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.quant_features.alpha_base_data, now model.quant_features.features_ohlc_technical)
[0m15:27:20.335711 [debug] [Thread-4 (]: Began compiling node model.quant_features.features_ohlc_technical
[0m15:27:20.337847 [debug] [Thread-4 (]: Writing injected SQL for node "model.quant_features.features_ohlc_technical"
[0m15:27:20.338452 [debug] [Thread-1 (]: Began running node model.quant_features.stock_features
[0m15:27:20.338796 [debug] [Thread-4 (]: Began executing node model.quant_features.features_ohlc_technical
[0m15:27:20.339187 [info ] [Thread-1 (]: 15 of 15 START sql table model main.stock_features ............................. [RUN]
[0m15:27:20.341399 [debug] [Thread-4 (]: Writing runtime sql for node "model.quant_features.features_ohlc_technical"
[0m15:27:20.341988 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.quant_features.stg_market_data, now model.quant_features.stock_features)
[0m15:27:20.342427 [debug] [Thread-1 (]: Began compiling node model.quant_features.stock_features
[0m15:27:20.344388 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.stock_features"
[0m15:27:20.344790 [debug] [Thread-1 (]: Began executing node model.quant_features.stock_features
[0m15:27:20.346808 [debug] [Thread-1 (]: Writing runtime sql for node "model.quant_features.stock_features"
[0m15:27:20.347273 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:27:20.347650 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.features_ohlc_technical"
[0m15:27:20.347957 [debug] [Thread-1 (]: On model.quant_features.stock_features: BEGIN
[0m15:27:20.348260 [debug] [Thread-4 (]: On model.quant_features.features_ohlc_technical: BEGIN
[0m15:27:20.348794 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:27:20.349286 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m15:27:20.350071 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:27:20.350376 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:27:20.350661 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m15:27:20.351028 [debug] [Thread-1 (]: On model.quant_features.stock_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stock_features"} */

  
    
    

    create  table
      "quant_features"."main"."stock_features__dbt_tmp"
  
    as (
      

with daily_data as (
    select * from "quant_features"."main"."daily_stock_summary"
),

windowed_features as (
    select
        *,
        -- 移动平均
        avg(close) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        ) as ma_5d,
        
        avg(close) over (
            partition by symbol 
            order by date 
            rows between 9 preceding and current row
        ) as ma_10d,
        
        -- 波动率（标准差）
        stddev(daily_return) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        ) as volatility_5d,
        
        -- 价格相对位置
        (close - min(low) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        )) / (max(high) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        ) - min(low) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        )) as price_position_5d,
        
        -- 成交量相对强度
        volume / avg(volume) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        ) as volume_ratio_5d
        
    from daily_data
),

final_features as (
    select
        *,
        -- 技术信号
        case 
            when close > ma_5d and ma_5d > ma_10d then 'Bullish'
            when close < ma_5d and ma_5d < ma_10d then 'Bearish'
            else 'Neutral'
        end as trend_signal,
        
        case 
            when volatility_5d > 0.03 then 'High'
            when volatility_5d > 0.01 then 'Medium'
            else 'Low'
        end as volatility_category
        
    from windowed_features
)

select * from final_features
    );
  
  
[0m15:27:20.351388 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.features_ohlc_technical"
[0m15:27:20.351927 [debug] [Thread-4 (]: On model.quant_features.features_ohlc_technical: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.features_ohlc_technical"} */

  
    
    

    create  table
      "quant_features"."main"."features_ohlc_technical__dbt_tmp"
  
    as (
      

with technical_data as (
    select * from "quant_features"."main"."mart_technical_indicators"
),

feature_engineering as (
    select 
        symbol,
        timestamp,
        
        -- 基础价格特征
        close as price,
        daily_return,
        volatility_20d,
        
        -- 趋势特征
        ma_5,
        ma_10,
        ma_20,
        case when close > ma_5 then 1 else 0 end as price_above_ma5,
        case when close > ma_10 then 1 else 0 end as price_above_ma10,
        case when close > ma_20 then 1 else 0 end as price_above_ma20,
        case when ma_5 > ma_10 then 1 else 0 end as ma5_above_ma10,
        case when ma_10 > ma_20 then 1 else 0 end as ma10_above_ma20,
        
        -- 技术指标特征
        rsi_14,
        case when rsi_14 > 70 then 1 else 0 end as rsi_overbought,
        case when rsi_14 < 30 then 1 else 0 end as rsi_oversold,
        
        stoch_k_14,
        case when stoch_k_14 > 0.8 then 1 else 0 end as stoch_overbought,
        case when stoch_k_14 < 0.2 then 1 else 0 end as stoch_oversold,
        
        -- 布林带特征
        bollinger_upper,
        bollinger_lower,
        case when close > bollinger_upper then 1 else 0 end as price_above_bb_upper,
        case when close < bollinger_lower then 1 else 0 end as price_below_bb_lower,
        case 
            when bollinger_upper - bollinger_lower != 0 
            then (close - bollinger_lower) / (bollinger_upper - bollinger_lower)
            else 0.5
        end as bb_position,
        
        -- 动量特征
        momentum_5d,
        momentum_10d,
        case when momentum_5d > 0 then 1 else 0 end as momentum_5d_positive,
        case when momentum_10d > 0 then 1 else 0 end as momentum_10d_positive,
        
        -- 成交量特征
        volume,
        avg_volume_20d,
        case when avg_volume_20d != 0 then volume / avg_volume_20d else 0 end as volume_ratio,
        case when volume > avg_volume_20d * 1.5 then 1 else 0 end as high_volume,
        
        -- 价格范围特征
        daily_range,
        case when lag(close) over (partition by symbol order by timestamp) != 0 
            then daily_range / lag(close) over (partition by symbol order by timestamp)
            else 0
        end as range_ratio,
        
        -- 组合特征
        case when rsi_14 > 70 and stoch_k_14 > 0.8 then 1 else 0 end as double_overbought,
        case when rsi_14 < 30 and stoch_k_14 < 0.2 then 1 else 0 end as double_oversold,
        
        -- 时间特征
        extract(hour from timestamp) as hour,
        extract(dow from timestamp) as day_of_week,
        extract(month from timestamp) as month,
        
        -- 标识特征用于Feast
        concat(symbol, '_', date_trunc('day', timestamp)::string) as entity_id,
        timestamp as event_timestamp
        
    from technical_data
    where timestamp >= current_date - interval '20' days
)

select * from feature_engineering
    );
  
  
[0m15:27:20.355005 [debug] [Thread-4 (]: SQL status: OK in 0.003 seconds
[0m15:27:20.356799 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.features_ohlc_technical"
[0m15:27:20.357064 [debug] [Thread-4 (]: On model.quant_features.features_ohlc_technical: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.features_ohlc_technical"} */
alter table "quant_features"."main"."features_ohlc_technical__dbt_tmp" rename to "features_ohlc_technical"
[0m15:27:20.357612 [debug] [Thread-1 (]: SQL status: OK in 0.006 seconds
[0m15:27:20.359424 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:27:20.359727 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m15:27:20.360040 [debug] [Thread-1 (]: On model.quant_features.stock_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stock_features"} */
alter table "quant_features"."main"."stock_features" rename to "stock_features__dbt_backup"
[0m15:27:20.361008 [debug] [Thread-4 (]: On model.quant_features.features_ohlc_technical: COMMIT
[0m15:27:20.361646 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.features_ohlc_technical"
[0m15:27:20.362000 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:27:20.362577 [debug] [Thread-4 (]: On model.quant_features.features_ohlc_technical: COMMIT
[0m15:27:20.364673 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:27:20.365061 [debug] [Thread-1 (]: On model.quant_features.stock_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stock_features"} */
alter table "quant_features"."main"."stock_features__dbt_tmp" rename to "stock_features"
[0m15:27:20.365807 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:27:20.366840 [debug] [Thread-1 (]: On model.quant_features.stock_features: COMMIT
[0m15:27:20.367093 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:27:20.367358 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m15:27:20.367643 [debug] [Thread-1 (]: On model.quant_features.stock_features: COMMIT
[0m15:27:20.369025 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.features_ohlc_technical"
[0m15:27:20.369438 [debug] [Thread-4 (]: On model.quant_features.features_ohlc_technical: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.features_ohlc_technical"} */

      drop table if exists "quant_features"."main"."features_ohlc_technical__dbt_backup" cascade
    
[0m15:27:20.371090 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:27:20.372742 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:27:20.373000 [debug] [Thread-1 (]: On model.quant_features.stock_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stock_features"} */

      drop table if exists "quant_features"."main"."stock_features__dbt_backup" cascade
    
[0m15:27:20.373440 [debug] [Thread-4 (]: SQL status: OK in 0.004 seconds
[0m15:27:20.374503 [debug] [Thread-4 (]: On model.quant_features.features_ohlc_technical: Close
[0m15:27:20.374821 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:27:20.376215 [debug] [Thread-1 (]: On model.quant_features.stock_features: Close
[0m15:27:20.376619 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '546a7400-bea5-4d62-9693-c27be28811e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90c9b15e50>]}
[0m15:27:20.377156 [info ] [Thread-4 (]: 14 of 15 OK created sql table model main.features_ohlc_technical ............... [[32mOK[0m in 0.04s]
[0m15:27:20.377497 [debug] [Thread-4 (]: Finished running node model.quant_features.features_ohlc_technical
[0m15:27:20.377937 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '546a7400-bea5-4d62-9693-c27be28811e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90c9b15c70>]}
[0m15:27:20.378542 [info ] [Thread-1 (]: 15 of 15 OK created sql table model main.stock_features ........................ [[32mOK[0m in 0.04s]
[0m15:27:20.378901 [debug] [Thread-1 (]: Finished running node model.quant_features.stock_features
[0m15:27:20.380396 [debug] [MainThread]: Using duckdb connection "master"
[0m15:27:20.380628 [debug] [MainThread]: On master: BEGIN
[0m15:27:20.380788 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:27:20.381147 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:27:20.381362 [debug] [MainThread]: On master: COMMIT
[0m15:27:20.381523 [debug] [MainThread]: Using duckdb connection "master"
[0m15:27:20.381671 [debug] [MainThread]: On master: COMMIT
[0m15:27:20.381951 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:27:20.382121 [debug] [MainThread]: On master: Close
[0m15:27:20.382403 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:27:20.382595 [debug] [MainThread]: Connection 'model.quant_features.stock_features' was properly closed.
[0m15:27:20.382741 [debug] [MainThread]: Connection 'model.quant_features.mart_technical_indicators' was properly closed.
[0m15:27:20.382876 [debug] [MainThread]: Connection 'model.quant_features.daily_stock_summary' was properly closed.
[0m15:27:20.383007 [debug] [MainThread]: Connection 'model.quant_features.features_ohlc_technical' was properly closed.
[0m15:27:20.383232 [info ] [MainThread]: 
[0m15:27:20.383431 [info ] [MainThread]: Finished running 12 table models, 3 view models in 0 hours 0 minutes and 0.32 seconds (0.32s).
[0m15:27:20.384370 [debug] [MainThread]: Command end result
[0m15:27:20.406455 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:27:20.407699 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:27:20.411628 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:27:20.411868 [info ] [MainThread]: 
[0m15:27:20.412075 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:27:20.412262 [info ] [MainThread]: 
[0m15:27:20.412490 [error] [MainThread]: [31mFailure in model alpha_base_data (models/alpha101/alpha_base_data.sql)[0m
[0m15:27:20.412707 [error] [MainThread]:   Runtime Error in model alpha_base_data (models/alpha101/alpha_base_data.sql)
  Binder Error: Could not choose a best candidate function for the function call "-(STRING_LITERAL, INTERVAL)". In order to select one, please add explicit type casts.
  	Candidate functions:
  	-(DATE, INTERVAL) -> TIMESTAMP
  	-(TIME, INTERVAL) -> TIME
  	-(TIMESTAMP, INTERVAL) -> TIMESTAMP
  	-(TIME WITH TIME ZONE, INTERVAL) -> TIME WITH TIME ZONE
  	-(TIMESTAMP WITH TIME ZONE, INTERVAL) -> TIMESTAMP WITH TIME ZONE
  	-(INTERVAL, INTERVAL) -> INTERVAL
  
  
  LINE 475:     WHERE timestamp >= '2020-01-01' - INTERVAL '250 days'  -- 扩展时间范围以确保有足够的历史数...
                                                ^
[0m15:27:20.412892 [info ] [MainThread]: 
[0m15:27:20.413082 [info ] [MainThread]:   compiled code at target/compiled/quant_features/models/alpha101/alpha_base_data.sql
[0m15:27:20.413249 [info ] [MainThread]: 
[0m15:27:20.413427 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=1 SKIP=7 NO-OP=0 TOTAL=15
[0m15:27:20.413977 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.0885947, "process_in_blocks": "0", "process_kernel_time": 0.156504, "process_mem_max_rss": "167864", "process_out_blocks": "5432", "process_user_time": 1.874127}
[0m15:27:20.414305 [debug] [MainThread]: Command `dbt run` failed at 15:27:20.414242 after 1.09 seconds
[0m15:27:20.414540 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90ca4a3d70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90cf6f1c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90c8fbe7b0>]}
[0m15:27:20.414757 [debug] [MainThread]: Flushing usage events
[0m15:27:20.471403 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:27:27.655178 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c61697770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c62cf1a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c6050bd90>]}


============================== 15:27:27.658659 | 35d3798b-4af2-4e77-8cc6-1830e26e304e ==============================
[0m15:27:27.658659 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:27:27.659033 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'printer_width': '80', 'static_parser': 'True', 'warn_error': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'debug': 'False', 'invocation_command': 'dbt run --select stg_stock_prices stg_market_data daily_stock_summary stock_features', 'use_colors': 'True', 'no_print': 'None', 'log_path': '/workspace/dbt_project/logs', 'empty': 'False', 'target_path': 'None', 'partial_parse': 'True', 'write_json': 'True', 'cache_selected_only': 'False', 'introspect': 'True', 'indirect_selection': 'eager', 'version_check': 'True', 'use_experimental_parser': 'False', 'log_format': 'default', 'log_cache_events': 'False', 'profiles_dir': '/workspace/dbt_project'}
[0m15:27:27.807454 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '35d3798b-4af2-4e77-8cc6-1830e26e304e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c61222650>]}
[0m15:27:27.849175 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '35d3798b-4af2-4e77-8cc6-1830e26e304e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c60606be0>]}
[0m15:27:27.850341 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:27:27.881040 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:27:27.971116 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:27:27.971371 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:27:28.008633 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '35d3798b-4af2-4e77-8cc6-1830e26e304e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c5ce1d450>]}
[0m15:27:28.081504 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:27:28.082808 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:27:28.093485 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '35d3798b-4af2-4e77-8cc6-1830e26e304e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c5ce51220>]}
[0m15:27:28.093827 [info ] [MainThread]: Found 15 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:27:28.094036 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '35d3798b-4af2-4e77-8cc6-1830e26e304e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c5ce90130>]}
[0m15:27:28.095779 [info ] [MainThread]: 
[0m15:27:28.096053 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:27:28.096229 [info ] [MainThread]: 
[0m15:27:28.096612 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:27:28.099893 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features'
[0m15:27:28.118373 [debug] [ThreadPool]: Using duckdb connection "list_quant_features"
[0m15:27:28.118644 [debug] [ThreadPool]: On list_quant_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"quant_features"'
    
  
  
[0m15:27:28.118838 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:27:28.131380 [debug] [ThreadPool]: SQL status: OK in 0.012 seconds
[0m15:27:28.132287 [debug] [ThreadPool]: On list_quant_features: Close
[0m15:27:28.133213 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_quant_features, now create_quant_features_main)
[0m15:27:28.133613 [debug] [ThreadPool]: Creating schema "database: "quant_features"
schema: "main"
"
[0m15:27:28.137921 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:27:28.138180 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='quant_features'
        and type='sqlite'
    
  
[0m15:27:28.138372 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:27:28.139410 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:27:28.140246 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:27:28.140490 [debug] [ThreadPool]: On create_quant_features_main: BEGIN
[0m15:27:28.140805 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:27:28.140979 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:27:28.141133 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
    
        create schema if not exists "quant_features"."main"
    
[0m15:27:28.141460 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:27:28.141991 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:27:28.142193 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:27:28.142365 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:27:28.142659 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:27:28.142829 [debug] [ThreadPool]: On create_quant_features_main: Close
[0m15:27:28.144566 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_quant_features_main, now list_quant_features_main)
[0m15:27:28.187382 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:27:28.187629 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:27:28.187791 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:27:28.188183 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:27:28.188392 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:27:28.188552 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:27:28.195138 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m15:27:28.196086 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:27:28.196762 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:27:28.196983 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:27:28.198925 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '35d3798b-4af2-4e77-8cc6-1830e26e304e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c5b6069c0>]}
[0m15:27:28.199375 [debug] [MainThread]: Using duckdb connection "master"
[0m15:27:28.199575 [debug] [MainThread]: On master: BEGIN
[0m15:27:28.199728 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:27:28.200124 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:27:28.200323 [debug] [MainThread]: On master: COMMIT
[0m15:27:28.200494 [debug] [MainThread]: Using duckdb connection "master"
[0m15:27:28.200639 [debug] [MainThread]: On master: COMMIT
[0m15:27:28.200896 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:27:28.201076 [debug] [MainThread]: On master: Close
[0m15:27:28.203618 [debug] [Thread-1 (]: Began running node model.quant_features.stg_market_data
[0m15:27:28.204011 [debug] [Thread-2 (]: Began running node model.quant_features.stg_stock_prices
[0m15:27:28.204497 [info ] [Thread-1 (]: 1 of 4 START sql view model main.stg_market_data ............................... [RUN]
[0m15:27:28.205087 [info ] [Thread-2 (]: 2 of 4 START sql view model main.stg_stock_prices .............................. [RUN]
[0m15:27:28.205536 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.stg_market_data)
[0m15:27:28.206038 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.quant_features.stg_stock_prices'
[0m15:27:28.206439 [debug] [Thread-1 (]: Began compiling node model.quant_features.stg_market_data
[0m15:27:28.206803 [debug] [Thread-2 (]: Began compiling node model.quant_features.stg_stock_prices
[0m15:27:28.211653 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.stg_market_data"
[0m15:27:28.214027 [debug] [Thread-2 (]: Writing injected SQL for node "model.quant_features.stg_stock_prices"
[0m15:27:28.214682 [debug] [Thread-1 (]: Began executing node model.quant_features.stg_market_data
[0m15:27:28.221785 [debug] [Thread-2 (]: Began executing node model.quant_features.stg_stock_prices
[0m15:27:28.241421 [debug] [Thread-2 (]: Writing runtime sql for node "model.quant_features.stg_stock_prices"
[0m15:27:28.242505 [debug] [Thread-1 (]: Writing runtime sql for node "model.quant_features.stg_market_data"
[0m15:27:28.243321 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:27:28.243597 [debug] [Thread-2 (]: On model.quant_features.stg_stock_prices: BEGIN
[0m15:27:28.243788 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:27:28.244334 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:27:28.244630 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: BEGIN
[0m15:27:28.244820 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:27:28.245245 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:27:28.245480 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:27:28.245692 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_market_data"} */

  
  create view "quant_features"."main"."stg_market_data__dbt_tmp" as (
    

with source_data as (
    select
        date,
        symbol,
        market_cap,
        pe_ratio,
        dividend_yield,
        sector,
        -- 添加计算字段
        case 
            when pe_ratio > 0 then market_cap / pe_ratio 
            else null 
        end as estimated_earnings,
        case 
            when dividend_yield > 0 then market_cap * dividend_yield / 100 
            else 0 
        end as estimated_dividend_payout
    from "quant_features"."main"."market_data"
    where date is not null
      and symbol is not null
      and market_cap > 0
)

select * from source_data
  );

[0m15:27:28.246105 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m15:27:28.246421 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:27:28.246668 [debug] [Thread-2 (]: On model.quant_features.stg_stock_prices: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_stock_prices"} */

  
  create view "quant_features"."main"."stg_stock_prices__dbt_tmp" as (
    

with source_data as (
    select
        date,
        symbol,
        open,
        high,
        low,
        close,
        volume,
        -- 计算基础技术指标
        (high - low) as daily_range,
        (close - open) as daily_change,
        (close - open) / open as daily_return,
        -- 添加数据质量检查
        case 
            when high >= low and high >= open and high >= close 
                 and low <= open and low <= close 
            then true 
            else false 
        end as is_valid_ohlc
    from "quant_features"."main"."raw_stock_prices"
    where date is not null
      and symbol is not null
      and open > 0
      and high > 0
      and low > 0
      and close > 0
      and volume >= 0
)

select * from source_data
where is_valid_ohlc = true
  );

[0m15:27:28.246994 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:27:28.251156 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:27:28.251445 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_market_data"} */
alter view "quant_features"."main"."stg_market_data" rename to "stg_market_data__dbt_backup"
[0m15:27:28.251825 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m15:27:28.253739 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:27:28.254027 [debug] [Thread-2 (]: On model.quant_features.stg_stock_prices: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_stock_prices"} */
alter view "quant_features"."main"."stg_stock_prices" rename to "stg_stock_prices__dbt_backup"
[0m15:27:28.254451 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m15:27:28.256527 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:27:28.256794 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_market_data"} */
alter view "quant_features"."main"."stg_market_data__dbt_tmp" rename to "stg_market_data"
[0m15:27:28.257148 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m15:27:28.259034 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:27:28.259310 [debug] [Thread-2 (]: On model.quant_features.stg_stock_prices: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_stock_prices"} */
alter view "quant_features"."main"."stg_stock_prices__dbt_tmp" rename to "stg_stock_prices"
[0m15:27:28.259671 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m15:27:28.265179 [debug] [Thread-2 (]: SQL status: OK in 0.006 seconds
[0m15:27:28.269420 [debug] [Thread-2 (]: On model.quant_features.stg_stock_prices: COMMIT
[0m15:27:28.269725 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:27:28.270802 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: COMMIT
[0m15:27:28.271626 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:27:28.271271 [debug] [Thread-2 (]: On model.quant_features.stg_stock_prices: COMMIT
[0m15:27:28.271841 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: COMMIT
[0m15:27:28.274394 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m15:27:28.278002 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:27:28.278283 [debug] [Thread-2 (]: On model.quant_features.stg_stock_prices: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_stock_prices"} */

      drop view if exists "quant_features"."main"."stg_stock_prices__dbt_backup" cascade
    
[0m15:27:28.278805 [debug] [Thread-1 (]: SQL status: OK in 0.006 seconds
[0m15:27:28.280855 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:27:28.281109 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_market_data"} */

      drop view if exists "quant_features"."main"."stg_market_data__dbt_backup" cascade
    
[0m15:27:28.281512 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m15:27:28.283145 [debug] [Thread-2 (]: On model.quant_features.stg_stock_prices: Close
[0m15:27:28.283611 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:27:28.284601 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: Close
[0m15:27:28.285774 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35d3798b-4af2-4e77-8cc6-1830e26e304e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c5b5dca10>]}
[0m15:27:28.286307 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35d3798b-4af2-4e77-8cc6-1830e26e304e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c5c038f70>]}
[0m15:27:28.286857 [info ] [Thread-1 (]: 1 of 4 OK created sql view model main.stg_market_data .......................... [[32mOK[0m in 0.08s]
[0m15:27:28.287441 [info ] [Thread-2 (]: 2 of 4 OK created sql view model main.stg_stock_prices ......................... [[32mOK[0m in 0.08s]
[0m15:27:28.287864 [debug] [Thread-1 (]: Finished running node model.quant_features.stg_market_data
[0m15:27:28.288329 [debug] [Thread-2 (]: Finished running node model.quant_features.stg_stock_prices
[0m15:27:28.289934 [debug] [Thread-3 (]: Began running node model.quant_features.daily_stock_summary
[0m15:27:28.290335 [info ] [Thread-3 (]: 3 of 4 START sql table model main.daily_stock_summary .......................... [RUN]
[0m15:27:28.290693 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.quant_features.daily_stock_summary'
[0m15:27:28.290903 [debug] [Thread-3 (]: Began compiling node model.quant_features.daily_stock_summary
[0m15:27:28.293238 [debug] [Thread-3 (]: Writing injected SQL for node "model.quant_features.daily_stock_summary"
[0m15:27:28.293662 [debug] [Thread-3 (]: Began executing node model.quant_features.daily_stock_summary
[0m15:27:28.306130 [debug] [Thread-3 (]: Writing runtime sql for node "model.quant_features.daily_stock_summary"
[0m15:27:28.306594 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:27:28.306820 [debug] [Thread-3 (]: On model.quant_features.daily_stock_summary: BEGIN
[0m15:27:28.307008 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:27:28.307482 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:27:28.307708 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:27:28.307940 [debug] [Thread-3 (]: On model.quant_features.daily_stock_summary: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.daily_stock_summary"} */

  
    
    

    create  table
      "quant_features"."main"."daily_stock_summary__dbt_tmp"
  
    as (
      

with price_data as (
    select * from "quant_features"."main"."stg_stock_prices"
),

market_data as (
    select * from "quant_features"."main"."stg_market_data"
),

combined_data as (
    select
        p.date,
        p.symbol,
        p.open,
        p.high,
        p.low,
        p.close,
        p.volume,
        p.daily_range,
        p.daily_change,
        p.daily_return,
        m.market_cap,
        m.pe_ratio,
        m.dividend_yield,
        m.sector,
        m.estimated_earnings,
        m.estimated_dividend_payout,
        -- 计算额外的技术指标
        (p.high + p.low + p.close) / 3 as typical_price,
        p.volume * p.close as dollar_volume,
        case 
            when p.daily_return > 0.05 then 'Strong Up'
            when p.daily_return > 0.02 then 'Up'
            when p.daily_return > -0.02 then 'Flat'
            when p.daily_return > -0.05 then 'Down'
            else 'Strong Down'
        end as price_movement_category
    from price_data p
    left join market_data m
        on p.date = m.date
        and p.symbol = m.symbol
)

select * from combined_data
    );
  
  
[0m15:27:28.311853 [debug] [Thread-3 (]: SQL status: OK in 0.004 seconds
[0m15:27:28.313678 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:27:28.313937 [debug] [Thread-3 (]: On model.quant_features.daily_stock_summary: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.daily_stock_summary"} */
alter table "quant_features"."main"."daily_stock_summary" rename to "daily_stock_summary__dbt_backup"
[0m15:27:28.314421 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:27:28.316137 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:27:28.316406 [debug] [Thread-3 (]: On model.quant_features.daily_stock_summary: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.daily_stock_summary"} */
alter table "quant_features"."main"."daily_stock_summary__dbt_tmp" rename to "daily_stock_summary"
[0m15:27:28.316855 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:27:28.319876 [debug] [Thread-3 (]: On model.quant_features.daily_stock_summary: COMMIT
[0m15:27:28.320152 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:27:28.320367 [debug] [Thread-3 (]: On model.quant_features.daily_stock_summary: COMMIT
[0m15:27:28.322823 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m15:27:28.329038 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:27:28.329418 [debug] [Thread-3 (]: On model.quant_features.daily_stock_summary: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.daily_stock_summary"} */

      drop table if exists "quant_features"."main"."daily_stock_summary__dbt_backup" cascade
    
[0m15:27:28.331384 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m15:27:28.332473 [debug] [Thread-3 (]: On model.quant_features.daily_stock_summary: Close
[0m15:27:28.333078 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35d3798b-4af2-4e77-8cc6-1830e26e304e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c5b5c8e10>]}
[0m15:27:28.333540 [info ] [Thread-3 (]: 3 of 4 OK created sql table model main.daily_stock_summary ..................... [[32mOK[0m in 0.04s]
[0m15:27:28.333869 [debug] [Thread-3 (]: Finished running node model.quant_features.daily_stock_summary
[0m15:27:28.334952 [debug] [Thread-4 (]: Began running node model.quant_features.stock_features
[0m15:27:28.335367 [info ] [Thread-4 (]: 4 of 4 START sql table model main.stock_features ............................... [RUN]
[0m15:27:28.335719 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.quant_features.stock_features'
[0m15:27:28.335932 [debug] [Thread-4 (]: Began compiling node model.quant_features.stock_features
[0m15:27:28.338160 [debug] [Thread-4 (]: Writing injected SQL for node "model.quant_features.stock_features"
[0m15:27:28.338600 [debug] [Thread-4 (]: Began executing node model.quant_features.stock_features
[0m15:27:28.340771 [debug] [Thread-4 (]: Writing runtime sql for node "model.quant_features.stock_features"
[0m15:27:28.341205 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:27:28.341427 [debug] [Thread-4 (]: On model.quant_features.stock_features: BEGIN
[0m15:27:28.341616 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m15:27:28.342074 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m15:27:28.342289 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:27:28.342542 [debug] [Thread-4 (]: On model.quant_features.stock_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stock_features"} */

  
    
    

    create  table
      "quant_features"."main"."stock_features__dbt_tmp"
  
    as (
      

with daily_data as (
    select * from "quant_features"."main"."daily_stock_summary"
),

windowed_features as (
    select
        *,
        -- 移动平均
        avg(close) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        ) as ma_5d,
        
        avg(close) over (
            partition by symbol 
            order by date 
            rows between 9 preceding and current row
        ) as ma_10d,
        
        -- 波动率（标准差）
        stddev(daily_return) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        ) as volatility_5d,
        
        -- 价格相对位置
        (close - min(low) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        )) / (max(high) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        ) - min(low) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        )) as price_position_5d,
        
        -- 成交量相对强度
        volume / avg(volume) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        ) as volume_ratio_5d
        
    from daily_data
),

final_features as (
    select
        *,
        -- 技术信号
        case 
            when close > ma_5d and ma_5d > ma_10d then 'Bullish'
            when close < ma_5d and ma_5d < ma_10d then 'Bearish'
            else 'Neutral'
        end as trend_signal,
        
        case 
            when volatility_5d > 0.03 then 'High'
            when volatility_5d > 0.01 then 'Medium'
            else 'Low'
        end as volatility_category
        
    from windowed_features
)

select * from final_features
    );
  
  
[0m15:27:28.349216 [debug] [Thread-4 (]: SQL status: OK in 0.006 seconds
[0m15:27:28.350948 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:27:28.351211 [debug] [Thread-4 (]: On model.quant_features.stock_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stock_features"} */
alter table "quant_features"."main"."stock_features" rename to "stock_features__dbt_backup"
[0m15:27:28.351722 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m15:27:28.353725 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:27:28.353989 [debug] [Thread-4 (]: On model.quant_features.stock_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stock_features"} */
alter table "quant_features"."main"."stock_features__dbt_tmp" rename to "stock_features"
[0m15:27:28.354474 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m15:27:28.355429 [debug] [Thread-4 (]: On model.quant_features.stock_features: COMMIT
[0m15:27:28.355688 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:27:28.355884 [debug] [Thread-4 (]: On model.quant_features.stock_features: COMMIT
[0m15:27:28.357792 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m15:27:28.359163 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:27:28.359439 [debug] [Thread-4 (]: On model.quant_features.stock_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stock_features"} */

      drop table if exists "quant_features"."main"."stock_features__dbt_backup" cascade
    
[0m15:27:28.361377 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m15:27:28.362271 [debug] [Thread-4 (]: On model.quant_features.stock_features: Close
[0m15:27:28.363016 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35d3798b-4af2-4e77-8cc6-1830e26e304e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c5b6f1950>]}
[0m15:27:28.363418 [info ] [Thread-4 (]: 4 of 4 OK created sql table model main.stock_features .......................... [[32mOK[0m in 0.03s]
[0m15:27:28.363731 [debug] [Thread-4 (]: Finished running node model.quant_features.stock_features
[0m15:27:28.365457 [debug] [MainThread]: Using duckdb connection "master"
[0m15:27:28.365710 [debug] [MainThread]: On master: BEGIN
[0m15:27:28.365876 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:27:28.366270 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:27:28.366469 [debug] [MainThread]: On master: COMMIT
[0m15:27:28.366627 [debug] [MainThread]: Using duckdb connection "master"
[0m15:27:28.366772 [debug] [MainThread]: On master: COMMIT
[0m15:27:28.367044 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:27:28.367234 [debug] [MainThread]: On master: Close
[0m15:27:28.367524 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:27:28.367689 [debug] [MainThread]: Connection 'model.quant_features.stg_market_data' was properly closed.
[0m15:27:28.367829 [debug] [MainThread]: Connection 'model.quant_features.stg_stock_prices' was properly closed.
[0m15:27:28.367965 [debug] [MainThread]: Connection 'model.quant_features.daily_stock_summary' was properly closed.
[0m15:27:28.368097 [debug] [MainThread]: Connection 'model.quant_features.stock_features' was properly closed.
[0m15:27:28.368330 [info ] [MainThread]: 
[0m15:27:28.368554 [info ] [MainThread]: Finished running 2 table models, 2 view models in 0 hours 0 minutes and 0.27 seconds (0.27s).
[0m15:27:28.369144 [debug] [MainThread]: Command end result
[0m15:27:28.392246 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:27:28.393666 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:27:28.397779 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:27:28.398020 [info ] [MainThread]: 
[0m15:27:28.398251 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:27:28.398440 [info ] [MainThread]: 
[0m15:27:28.398637 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m15:27:28.399287 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.7865642, "process_in_blocks": "0", "process_kernel_time": 0.139911, "process_mem_max_rss": "158996", "process_out_blocks": "3664", "process_user_time": 1.542903}
[0m15:27:28.399617 [debug] [MainThread]: Command `dbt run` succeeded at 15:27:28.399551 after 0.79 seconds
[0m15:27:28.399843 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c60531da0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c6472d400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c6070b650>]}
[0m15:27:28.400054 [debug] [MainThread]: Flushing usage events
[0m15:27:28.456351 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:27:34.598249 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa910623770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa911c75a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa90f493d90>]}


============================== 15:27:34.600755 | 739055df-1b17-40a3-9538-a0b06eaafc9b ==============================
[0m15:27:34.600755 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:27:34.601093 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt show --select stock_features --limit 10', 'introspect': 'True', 'use_experimental_parser': 'False', 'write_json': 'True', 'log_format': 'default', 'log_path': '/workspace/dbt_project/logs', 'profiles_dir': '/workspace/dbt_project', 'no_print': 'None', 'log_cache_events': 'False', 'static_parser': 'True', 'indirect_selection': 'eager', 'use_colors': 'True', 'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'cache_selected_only': 'False', 'partial_parse': 'True', 'version_check': 'True', 'printer_width': '80', 'fail_fast': 'False', 'target_path': 'None', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'empty': 'None', 'quiet': 'False'}
[0m15:27:34.751053 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '739055df-1b17-40a3-9538-a0b06eaafc9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa9101ae650>]}
[0m15:27:34.792715 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '739055df-1b17-40a3-9538-a0b06eaafc9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa90f592be0>]}
[0m15:27:34.801424 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:27:34.832332 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:27:34.921788 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:27:34.922032 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:27:34.959036 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '739055df-1b17-40a3-9538-a0b06eaafc9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa90b3d5650>]}
[0m15:27:35.032448 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:27:35.033708 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:27:35.038511 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '739055df-1b17-40a3-9538-a0b06eaafc9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa90b3c1310>]}
[0m15:27:35.038863 [info ] [MainThread]: Found 15 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:27:35.039074 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '739055df-1b17-40a3-9538-a0b06eaafc9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa90b44c590>]}
[0m15:27:35.040381 [info ] [MainThread]: 
[0m15:27:35.040667 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:27:35.040842 [info ] [MainThread]: 
[0m15:27:35.041145 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:27:35.044800 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features_main'
[0m15:27:35.095974 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:27:35.096231 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:27:35.096423 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:27:35.109649 [debug] [ThreadPool]: SQL status: OK in 0.013 seconds
[0m15:27:35.109907 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:27:35.110092 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:27:35.118306 [debug] [ThreadPool]: SQL status: OK in 0.008 seconds
[0m15:27:35.119308 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:27:35.119961 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:27:35.120192 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:27:35.122090 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '739055df-1b17-40a3-9538-a0b06eaafc9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa90aff0ae0>]}
[0m15:27:35.124428 [debug] [Thread-1 (]: Began running node model.quant_features.stock_features
[0m15:27:35.124773 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.stock_features)
[0m15:27:35.124987 [debug] [Thread-1 (]: Began compiling node model.quant_features.stock_features
[0m15:27:35.129774 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.stock_features"
[0m15:27:35.130231 [debug] [Thread-1 (]: Began executing node model.quant_features.stock_features
[0m15:27:35.134742 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:27:35.135090 [debug] [Thread-1 (]: On model.quant_features.stock_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stock_features"} */

  
  

with daily_data as (
    select * from "quant_features"."main"."daily_stock_summary"
),

windowed_features as (
    select
        *,
        -- 移动平均
        avg(close) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        ) as ma_5d,
        
        avg(close) over (
            partition by symbol 
            order by date 
            rows between 9 preceding and current row
        ) as ma_10d,
        
        -- 波动率（标准差）
        stddev(daily_return) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        ) as volatility_5d,
        
        -- 价格相对位置
        (close - min(low) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        )) / (max(high) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        ) - min(low) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        )) as price_position_5d,
        
        -- 成交量相对强度
        volume / avg(volume) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        ) as volume_ratio_5d
        
    from daily_data
),

final_features as (
    select
        *,
        -- 技术信号
        case 
            when close > ma_5d and ma_5d > ma_10d then 'Bullish'
            when close < ma_5d and ma_5d < ma_10d then 'Bearish'
            else 'Neutral'
        end as trend_signal,
        
        case 
            when volatility_5d > 0.03 then 'High'
            when volatility_5d > 0.01 then 'Medium'
            else 'Low'
        end as volatility_category
        
    from windowed_features
)

select * from final_features
  
  limit 10

[0m15:27:35.135372 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:27:35.140681 [debug] [Thread-1 (]: SQL status: OK in 0.005 seconds
[0m15:27:35.143092 [debug] [Thread-1 (]: On model.quant_features.stock_features: Close
[0m15:27:35.143865 [debug] [Thread-1 (]: Finished running node model.quant_features.stock_features
[0m15:27:35.145382 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:27:35.145646 [debug] [MainThread]: Connection 'model.quant_features.stock_features' was properly closed.
[0m15:27:35.146063 [debug] [MainThread]: Command end result
[0m15:27:35.168502 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:27:35.169723 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:27:35.173962 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:27:35.175024 [info ] [MainThread]: Previewing node 'stock_features':
|       date | symbol |    open |    high |     low |   close | ... |
| ---------- | ------ | ------- | ------- | ------- | ------- | --- |
| 2024-01-01 | GOOGL  | 2,800.0 | 2,850.0 | 2,780.0 | 2,820.0 | ... |
| 2024-01-02 | GOOGL  | 2,820.0 | 2,870.0 | 2,810.0 | 2,845.0 | ... |
| 2024-01-01 | MSFT   |   380.0 |   385.0 |   378.0 |   382.5 | ... |
| 2024-01-02 | MSFT   |   382.5 |   387.0 |   380.5 |   384.2 | ... |
| 2024-01-03 | MSFT   |   384.2 |   389.0 |   382.8 |   386.7 | ... |
| 2024-01-04 | MSFT   |   386.7 |   391.5 |   385.2 |   388.9 | ... |
| 2024-01-05 | MSFT   |   388.9 |   393.0 |   387.4 |   390.8 | ... |
| 2024-01-08 | MSFT   |   390.8 |   395.2 |   389.5 |   392.6 | ... |
| 2024-01-09 | MSFT   |   392.6 |   397.0 |   391.2 |   394.4 | ... |
| 2024-01-10 | MSFT   |   394.4 |   399.0 |   393.1 |   396.2 | ... |

[0m15:27:35.175668 [debug] [MainThread]: Resource report: {"command_name": "show", "command_success": true, "command_wall_clock_time": 0.61442894, "process_in_blocks": "0", "process_kernel_time": 0.18255, "process_mem_max_rss": "153040", "process_out_blocks": "3456", "process_user_time": 1.34534}
[0m15:27:35.175973 [debug] [MainThread]: Command `dbt show` succeeded at 15:27:35.175911 after 0.61 seconds
[0m15:27:35.176174 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa90f82f110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa90b179390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa90b179440>]}
[0m15:27:35.176387 [debug] [MainThread]: Flushing usage events
[0m15:27:35.241632 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:27:40.918204 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7eca53770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7ee085a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7eb8c7d90>]}


============================== 15:27:40.920538 | d014ec97-0aed-4f57-a9bc-2341530ab41c ==============================
[0m15:27:40.920538 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:27:40.920875 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'empty': 'None', 'target_path': 'None', 'log_cache_events': 'False', 'no_print': 'None', 'introspect': 'True', 'static_parser': 'True', 'debug': 'False', 'quiet': 'False', 'printer_width': '80', 'invocation_command': 'dbt show --select daily_stock_summary --limit 5', 'fail_fast': 'False', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True', 'log_path': '/workspace/dbt_project/logs', 'log_format': 'default', 'warn_error': 'None', 'use_experimental_parser': 'False', 'profiles_dir': '/workspace/dbt_project'}
[0m15:27:41.059488 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd014ec97-0aed-4f57-a9bc-2341530ab41c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7ec5de650>]}
[0m15:27:41.100541 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd014ec97-0aed-4f57-a9bc-2341530ab41c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7eb9c2be0>]}
[0m15:27:41.108482 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:27:41.137560 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:27:41.224187 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:27:41.224441 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:27:41.261069 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd014ec97-0aed-4f57-a9bc-2341530ab41c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7e77e1650>]}
[0m15:27:41.330199 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:27:41.331399 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:27:41.335551 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd014ec97-0aed-4f57-a9bc-2341530ab41c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7e77d1310>]}
[0m15:27:41.335911 [info ] [MainThread]: Found 15 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:27:41.336137 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd014ec97-0aed-4f57-a9bc-2341530ab41c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7e7858590>]}
[0m15:27:41.337339 [info ] [MainThread]: 
[0m15:27:41.337610 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:27:41.337781 [info ] [MainThread]: 
[0m15:27:41.338065 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:27:41.341747 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features_main'
[0m15:27:41.388026 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:27:41.388281 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:27:41.388505 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:27:41.398849 [debug] [ThreadPool]: SQL status: OK in 0.010 seconds
[0m15:27:41.399103 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:27:41.399279 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:27:41.406214 [debug] [ThreadPool]: SQL status: OK in 0.007 seconds
[0m15:27:41.407185 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:27:41.407800 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:27:41.408013 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:27:41.409612 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd014ec97-0aed-4f57-a9bc-2341530ab41c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7e73fcae0>]}
[0m15:27:41.412005 [debug] [Thread-1 (]: Began running node model.quant_features.daily_stock_summary
[0m15:27:41.412392 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.daily_stock_summary)
[0m15:27:41.412626 [debug] [Thread-1 (]: Began compiling node model.quant_features.daily_stock_summary
[0m15:27:41.417513 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.daily_stock_summary"
[0m15:27:41.417971 [debug] [Thread-1 (]: Began executing node model.quant_features.daily_stock_summary
[0m15:27:41.422689 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:27:41.423002 [debug] [Thread-1 (]: On model.quant_features.daily_stock_summary: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.daily_stock_summary"} */

  
  

with price_data as (
    select * from "quant_features"."main"."stg_stock_prices"
),

market_data as (
    select * from "quant_features"."main"."stg_market_data"
),

combined_data as (
    select
        p.date,
        p.symbol,
        p.open,
        p.high,
        p.low,
        p.close,
        p.volume,
        p.daily_range,
        p.daily_change,
        p.daily_return,
        m.market_cap,
        m.pe_ratio,
        m.dividend_yield,
        m.sector,
        m.estimated_earnings,
        m.estimated_dividend_payout,
        -- 计算额外的技术指标
        (p.high + p.low + p.close) / 3 as typical_price,
        p.volume * p.close as dollar_volume,
        case 
            when p.daily_return > 0.05 then 'Strong Up'
            when p.daily_return > 0.02 then 'Up'
            when p.daily_return > -0.02 then 'Flat'
            when p.daily_return > -0.05 then 'Down'
            else 'Strong Down'
        end as price_movement_category
    from price_data p
    left join market_data m
        on p.date = m.date
        and p.symbol = m.symbol
)

select * from combined_data
  
  limit 5

[0m15:27:41.423239 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:27:41.427320 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m15:27:41.429291 [debug] [Thread-1 (]: On model.quant_features.daily_stock_summary: Close
[0m15:27:41.429930 [debug] [Thread-1 (]: Finished running node model.quant_features.daily_stock_summary
[0m15:27:41.431177 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:27:41.431420 [debug] [MainThread]: Connection 'model.quant_features.daily_stock_summary' was properly closed.
[0m15:27:41.431849 [debug] [MainThread]: Command end result
[0m15:27:41.453245 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:27:41.454412 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:27:41.458584 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:27:41.459357 [info ] [MainThread]: Previewing node 'daily_stock_summary':
|       date | symbol |  open |  high |   low | close | ... |
| ---------- | ------ | ----- | ----- | ----- | ----- | --- |
| 2024-01-01 | AAPL   | 150.0 | 152.5 | 149.8 | 151.2 | ... |
| 2024-01-02 | AAPL   | 151.2 | 153.0 | 150.5 | 152.8 | ... |
| 2024-01-03 | AAPL   | 152.8 | 154.2 | 151.9 | 153.5 | ... |
| 2024-01-04 | AAPL   | 153.5 | 155.0 | 152.8 | 154.3 | ... |
| 2024-01-05 | AAPL   | 154.3 | 156.2 | 153.5 | 155.8 | ... |

[0m15:27:41.459957 [debug] [MainThread]: Resource report: {"command_name": "show", "command_success": true, "command_wall_clock_time": 0.5773786, "process_in_blocks": "0", "process_kernel_time": 0.124516, "process_mem_max_rss": "150792", "process_out_blocks": "3448", "process_user_time": 1.32949}
[0m15:27:41.460248 [debug] [MainThread]: Command `dbt show` succeeded at 15:27:41.460191 after 0.58 seconds
[0m15:27:41.460465 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7eba63110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7e753d390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7e753d440>]}
[0m15:27:41.460692 [debug] [MainThread]: Flushing usage events
[0m15:27:41.483239 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:30:03.178375 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8390b57770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f83921a5a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f838f9c7d90>]}


============================== 15:30:03.180816 | f583f5e6-b791-4144-83d2-01fc394916a1 ==============================
[0m15:30:03.180816 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:30:03.181150 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'introspect': 'True', 'log_format': 'default', 'version_check': 'True', 'log_path': '/workspace/dbt_project/logs', 'log_cache_events': 'False', 'write_json': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'warn_error': 'None', 'printer_width': '80', 'fail_fast': 'False', 'empty': 'False', 'quiet': 'False', 'debug': 'False', 'invocation_command': 'dbt run --select alpha_base_data', 'target_path': 'None', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'cache_selected_only': 'False', 'static_parser': 'True', 'profiles_dir': '/workspace/dbt_project'}
[0m15:30:03.331077 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f583f5e6-b791-4144-83d2-01fc394916a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f83906de650>]}
[0m15:30:03.372902 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f583f5e6-b791-4144-83d2-01fc394916a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f838fac2be0>]}
[0m15:30:03.373971 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:30:03.404541 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:30:03.493217 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:30:03.493728 [debug] [MainThread]: Partial parsing: updated file: quant_features://models/alpha101/alpha_base_data.sql
[0m15:30:03.755448 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `dbt_utils.accepted_range`. Arguments to
generic tests should be nested under the `arguments` property.`
[0m15:30:03.755927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'f583f5e6-b791-4144-83d2-01fc394916a1', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f838c2d9a50>]}
[0m15:30:03.866042 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f583f5e6-b791-4144-83d2-01fc394916a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f838b4d59a0>]}
[0m15:30:03.935517 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:30:03.936813 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:30:03.948350 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f583f5e6-b791-4144-83d2-01fc394916a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f838b45f850>]}
[0m15:30:03.948699 [info ] [MainThread]: Found 15 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:30:03.948913 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f583f5e6-b791-4144-83d2-01fc394916a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f838aa28c80>]}
[0m15:30:03.950153 [info ] [MainThread]: 
[0m15:30:03.950436 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:30:03.950619 [info ] [MainThread]: 
[0m15:30:03.950924 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:30:03.951903 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features'
[0m15:30:03.968917 [debug] [ThreadPool]: Using duckdb connection "list_quant_features"
[0m15:30:03.969191 [debug] [ThreadPool]: On list_quant_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"quant_features"'
    
  
  
[0m15:30:03.969382 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:30:03.983983 [debug] [ThreadPool]: SQL status: OK in 0.015 seconds
[0m15:30:03.984956 [debug] [ThreadPool]: On list_quant_features: Close
[0m15:30:03.985665 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_quant_features, now create_quant_features_main)
[0m15:30:03.985977 [debug] [ThreadPool]: Creating schema "database: "quant_features"
schema: "main"
"
[0m15:30:03.990355 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:30:03.990611 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='quant_features'
        and type='sqlite'
    
  
[0m15:30:03.990784 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:30:03.991532 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:30:03.992396 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:30:03.992611 [debug] [ThreadPool]: On create_quant_features_main: BEGIN
[0m15:30:03.992970 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:30:03.993139 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:30:03.993308 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
    
        create schema if not exists "quant_features"."main"
    
[0m15:30:03.993657 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:30:03.994184 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:30:03.994405 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:30:03.994572 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:30:03.994863 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:30:03.995040 [debug] [ThreadPool]: On create_quant_features_main: Close
[0m15:30:03.999096 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_quant_features_main, now list_quant_features_main)
[0m15:30:04.002908 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:30:04.003162 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:30:04.003332 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:30:04.003720 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:30:04.003905 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:30:04.004059 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:30:04.011096 [debug] [ThreadPool]: SQL status: OK in 0.007 seconds
[0m15:30:04.012039 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:30:04.012722 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:30:04.012938 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:30:04.014650 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f583f5e6-b791-4144-83d2-01fc394916a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f838a018410>]}
[0m15:30:04.014954 [debug] [MainThread]: Using duckdb connection "master"
[0m15:30:04.015120 [debug] [MainThread]: On master: BEGIN
[0m15:30:04.015280 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:30:04.015663 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:30:04.015868 [debug] [MainThread]: On master: COMMIT
[0m15:30:04.016036 [debug] [MainThread]: Using duckdb connection "master"
[0m15:30:04.016204 [debug] [MainThread]: On master: COMMIT
[0m15:30:04.016928 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:30:04.017236 [debug] [MainThread]: On master: Close
[0m15:30:04.019678 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_base_data
[0m15:30:04.020074 [info ] [Thread-1 (]: 1 of 1 START sql table model main.alpha_base_data .............................. [RUN]
[0m15:30:04.020386 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.alpha_base_data)
[0m15:30:04.020596 [debug] [Thread-1 (]: Began compiling node model.quant_features.alpha_base_data
[0m15:30:04.038870 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.alpha_base_data"
[0m15:30:04.039507 [debug] [Thread-1 (]: Began executing node model.quant_features.alpha_base_data
[0m15:30:04.060089 [debug] [Thread-1 (]: Writing runtime sql for node "model.quant_features.alpha_base_data"
[0m15:30:04.060662 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_base_data"
[0m15:30:04.060914 [debug] [Thread-1 (]: On model.quant_features.alpha_base_data: BEGIN
[0m15:30:04.061119 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:30:04.061596 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:30:04.061805 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_base_data"
[0m15:30:04.062246 [debug] [Thread-1 (]: On model.quant_features.alpha_base_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_base_data"} */

  
    
    

    create  table
      "quant_features"."main"."alpha_base_data__dbt_tmp"
  
    as (
      

-- Alpha 101 基础数据准备
-- 为Alpha因子计算准备所有必要的基础数据

WITH base_ohlc AS (
    SELECT 
        symbol,
        timestamp,
        open,
        high,
        low,
        close,
        volume,
        -- 计算VWAP (简化版本，假设等权重)
        (high + low + close) / 3 AS vwap,
        -- 计算returns
        CASE 
            WHEN LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp) IS NOT NULL
            THEN (close - LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp)) / 
                 LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp)
            ELSE 0
        END AS returns
    FROM "quant_features"."main"."stg_ohlc_data"
    WHERE timestamp >= CAST('2020-01-01' AS DATE)
      AND timestamp <= CAST('2024-12-31' AS DATE)
),

enhanced_data AS (
    SELECT 
        *,
        -- 计算ADV (Average Daily Volume)
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )

 AS adv20,
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )

 AS adv10,
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )

 AS adv5,
        
        -- 预计算一些常用的时间序列指标
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_ma5,
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS close_ma10,
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_ma20,
        
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS volume_ma20,
        
        -- 预计算滚动标准差
        
    STDDEV(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_std20,
        
    STDDEV(returns) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS returns_std20,
        
        -- 预计算一些延迟项
        
    LAG(close, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag1,
        
    LAG(close, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag2,
        
    LAG(close, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag5,
        
    LAG(close, 10) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag10,
        
    LAG(close, 20) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag20,
        
    LAG(volume, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS volume_lag1,
        
    LAG(high, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS high_lag1,
        
    LAG(low, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS low_lag1,
        
    LAG(vwap, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS vwap_lag5,
        
        -- 预计算一些差值项
        
    close - 
    LAG(close, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta1,
        
    close - 
    LAG(close, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta2,
        
    close - 
    LAG(close, 3) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta3,
        
    close - 
    LAG(close, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta5,
        
    close - 
    LAG(close, 7) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta7,
        
    close - 
    LAG(close, 10) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta10,
        
    volume - 
    LAG(volume, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS volume_delta1,
        
    volume - 
    LAG(volume, 3) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS volume_delta3,
        
    high - 
    LAG(high, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS high_delta2,
        
        -- 预计算一些排序项
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY close
    )
 AS close_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY volume
    )
 AS volume_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY high
    )
 AS high_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY low
    )
 AS low_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY vwap
    )
 AS vwap_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY returns
    )
 AS returns_rank,
        
        -- 预计算时间序列排序
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY close
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS close_ts_rank10,
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY volume
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_ts_rank5,
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY high
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS high_ts_rank5,
        
        -- 预计算一些最值项
        
    MIN(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 99 PRECEDING AND CURRENT ROW
    )
 AS close_min100,
        
    MAX(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS close_max3,
        
    MIN(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_min5,
        
    MAX(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_max5,
        
    MIN(low) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS low_min5,
        
    MAX(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS high_max3,
        
        -- 预计算一些求和项
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_sum5,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 7 PRECEDING AND CURRENT ROW
    )
 AS close_sum8,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_sum20,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 99 PRECEDING AND CURRENT ROW
    )
 AS close_sum100,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 199 PRECEDING AND CURRENT ROW
    )
 AS close_sum200,
        
    SUM(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_sum5,
        
    SUM(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS volume_sum20,
        
    SUM(returns) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 249 PRECEDING AND CURRENT ROW
    )
 AS returns_sum250,
        
    SUM(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS high_sum5,
        
    SUM(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS high_sum20,
        
        -- 预计算一些相关性
        
    -- 使用DuckDB的CORR窗口函数
    CORR(close, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS corr_close_volume_10,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(open, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS corr_open_volume_10,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(high, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS corr_high_volume_5,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(vwap, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 5 PRECEDING AND CURRENT ROW
    )
 AS corr_vwap_volume_6,
        
        -- 预计算一些协方差
        
    COVAR_SAMP(close, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS cov_close_volume_5,
        
    COVAR_SAMP(high, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS cov_high_volume_5
        
    FROM base_ohlc
    WHERE timestamp >= CAST(CAST('2020-01-01' AS DATE) AS DATE) - INTERVAL '250 days'  -- 扩展时间范围以确保有足够的历史数据
),

-- 过滤回原始时间范围
final_data AS (
    SELECT *
    FROM enhanced_data
    WHERE timestamp >= CAST('2020-01-01' AS DATE)
      AND timestamp <= CAST('2024-12-31' AS DATE)
)

SELECT * FROM final_data
    );
  
  
[0m15:30:04.093347 [debug] [Thread-1 (]: SQL status: OK in 0.031 seconds
[0m15:30:04.097372 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_base_data"
[0m15:30:04.097643 [debug] [Thread-1 (]: On model.quant_features.alpha_base_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_base_data"} */
alter table "quant_features"."main"."alpha_base_data__dbt_tmp" rename to "alpha_base_data"
[0m15:30:04.098200 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:30:04.106619 [debug] [Thread-1 (]: On model.quant_features.alpha_base_data: COMMIT
[0m15:30:04.106913 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_base_data"
[0m15:30:04.107129 [debug] [Thread-1 (]: On model.quant_features.alpha_base_data: COMMIT
[0m15:30:04.110184 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m15:30:04.114005 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_base_data"
[0m15:30:04.114299 [debug] [Thread-1 (]: On model.quant_features.alpha_base_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_base_data"} */

      drop table if exists "quant_features"."main"."alpha_base_data__dbt_backup" cascade
    
[0m15:30:04.114739 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:30:04.116460 [debug] [Thread-1 (]: On model.quant_features.alpha_base_data: Close
[0m15:30:04.119312 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f583f5e6-b791-4144-83d2-01fc394916a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f838b646c50>]}
[0m15:30:04.119910 [info ] [Thread-1 (]: 1 of 1 OK created sql table model main.alpha_base_data ......................... [[32mOK[0m in 0.10s]
[0m15:30:04.120310 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_base_data
[0m15:30:04.122468 [debug] [MainThread]: Using duckdb connection "master"
[0m15:30:04.122719 [debug] [MainThread]: On master: BEGIN
[0m15:30:04.122886 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:30:04.123329 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:30:04.123520 [debug] [MainThread]: On master: COMMIT
[0m15:30:04.123679 [debug] [MainThread]: Using duckdb connection "master"
[0m15:30:04.123827 [debug] [MainThread]: On master: COMMIT
[0m15:30:04.124088 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:30:04.124285 [debug] [MainThread]: On master: Close
[0m15:30:04.124581 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:30:04.124741 [debug] [MainThread]: Connection 'model.quant_features.alpha_base_data' was properly closed.
[0m15:30:04.124920 [info ] [MainThread]: 
[0m15:30:04.125107 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.17 seconds (0.17s).
[0m15:30:04.125496 [debug] [MainThread]: Command end result
[0m15:30:04.149326 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:30:04.150623 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:30:04.154379 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:30:04.154608 [info ] [MainThread]: 
[0m15:30:04.154846 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:30:04.155017 [info ] [MainThread]: 
[0m15:30:04.155209 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m15:30:04.155562 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 4 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m15:30:04.156240 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 1.0142612, "process_in_blocks": "0", "process_kernel_time": 0.158677, "process_mem_max_rss": "184732", "process_out_blocks": "5048", "process_user_time": 1.793172}
[0m15:30:04.156573 [debug] [MainThread]: Command `dbt run` succeeded at 15:30:04.156511 after 1.01 seconds
[0m15:30:04.156806 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f838b5f2df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f838abdd910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f838b792bd0>]}
[0m15:30:04.157013 [debug] [MainThread]: Flushing usage events
[0m15:30:04.209971 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:30:11.172607 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a5592f770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a56f8da90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a547a7d90>]}


============================== 15:30:11.175104 | 1ab1ef80-2fd2-4638-9b1b-959a0187c48b ==============================
[0m15:30:11.175104 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:30:11.175463 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'printer_width': '80', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'write_json': 'True', 'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'profiles_dir': '/workspace/dbt_project', 'quiet': 'False', 'invocation_command': 'dbt run --select alpha_factors_001_020', 'log_path': '/workspace/dbt_project/logs', 'empty': 'False', 'use_colors': 'True', 'static_parser': 'True', 'warn_error': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'fail_fast': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None'}
[0m15:30:11.315415 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1ab1ef80-2fd2-4638-9b1b-959a0187c48b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a554be650>]}
[0m15:30:11.363020 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1ab1ef80-2fd2-4638-9b1b-959a0187c48b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a5489ebe0>]}
[0m15:30:11.364051 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:30:11.394736 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:30:11.482911 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:30:11.483144 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:30:11.519767 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1ab1ef80-2fd2-4638-9b1b-959a0187c48b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a510b9450>]}
[0m15:30:11.587986 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:30:11.589199 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:30:11.599932 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1ab1ef80-2fd2-4638-9b1b-959a0187c48b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a510e9220>]}
[0m15:30:11.600265 [info ] [MainThread]: Found 15 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:30:11.600487 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1ab1ef80-2fd2-4638-9b1b-959a0187c48b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a5112c050>]}
[0m15:30:11.601697 [info ] [MainThread]: 
[0m15:30:11.601950 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:30:11.602117 [info ] [MainThread]: 
[0m15:30:11.602416 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:30:11.603117 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features'
[0m15:30:11.621766 [debug] [ThreadPool]: Using duckdb connection "list_quant_features"
[0m15:30:11.622027 [debug] [ThreadPool]: On list_quant_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"quant_features"'
    
  
  
[0m15:30:11.622237 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:30:11.636589 [debug] [ThreadPool]: SQL status: OK in 0.014 seconds
[0m15:30:11.637555 [debug] [ThreadPool]: On list_quant_features: Close
[0m15:30:11.638278 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_quant_features, now create_quant_features_main)
[0m15:30:11.638587 [debug] [ThreadPool]: Creating schema "database: "quant_features"
schema: "main"
"
[0m15:30:11.642784 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:30:11.643048 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='quant_features'
        and type='sqlite'
    
  
[0m15:30:11.643238 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:30:11.644203 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:30:11.645082 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:30:11.645323 [debug] [ThreadPool]: On create_quant_features_main: BEGIN
[0m15:30:11.645684 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:30:11.645864 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:30:11.646012 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
    
        create schema if not exists "quant_features"."main"
    
[0m15:30:11.646347 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:30:11.646875 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:30:11.647056 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:30:11.647224 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:30:11.647511 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:30:11.647698 [debug] [ThreadPool]: On create_quant_features_main: Close
[0m15:30:11.652082 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_quant_features_main, now list_quant_features_main)
[0m15:30:11.688777 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:30:11.689039 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:30:11.689223 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:30:11.689638 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:30:11.689843 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:30:11.690006 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:30:11.696792 [debug] [ThreadPool]: SQL status: OK in 0.007 seconds
[0m15:30:11.697788 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:30:11.698411 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:30:11.698621 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:30:11.700312 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1ab1ef80-2fd2-4638-9b1b-959a0187c48b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a54863520>]}
[0m15:30:11.700643 [debug] [MainThread]: Using duckdb connection "master"
[0m15:30:11.700813 [debug] [MainThread]: On master: BEGIN
[0m15:30:11.700957 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:30:11.701342 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:30:11.701543 [debug] [MainThread]: On master: COMMIT
[0m15:30:11.701703 [debug] [MainThread]: Using duckdb connection "master"
[0m15:30:11.701852 [debug] [MainThread]: On master: COMMIT
[0m15:30:11.702085 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:30:11.702256 [debug] [MainThread]: On master: Close
[0m15:30:11.704410 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_factors_001_020
[0m15:30:11.704791 [info ] [Thread-1 (]: 1 of 1 START sql table model main.alpha_factors_001_020 ........................ [RUN]
[0m15:30:11.705073 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.alpha_factors_001_020)
[0m15:30:11.705290 [debug] [Thread-1 (]: Began compiling node model.quant_features.alpha_factors_001_020
[0m15:30:11.725609 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.alpha_factors_001_020"
[0m15:30:11.726092 [debug] [Thread-1 (]: Began executing node model.quant_features.alpha_factors_001_020
[0m15:30:11.746900 [debug] [Thread-1 (]: Writing runtime sql for node "model.quant_features.alpha_factors_001_020"
[0m15:30:11.747416 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_001_020"
[0m15:30:11.747650 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_001_020: BEGIN
[0m15:30:11.747848 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:30:11.748278 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:30:11.748514 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_001_020"
[0m15:30:11.748999 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_001_020: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_factors_001_020"} */

  
    
    

    create  table
      "quant_features"."main"."alpha_factors_001_020__dbt_tmp"
  
    as (
      

-- Alpha 101 因子计算 (001-020)
-- 基于预处理的基础数据计算前20个Alpha因子

WITH base_data AS (
    SELECT * FROM "quant_features"."main"."alpha_base_data"
),

-- 预计算一些复杂的中间变量
intermediate_calcs AS (
    SELECT 
        *,
        -- Alpha001 相关计算
        
    -- 使用ROW_NUMBER()来找到最大值的位置
    (5 - 1) - (
        ROW_NUMBER() OVER (
            PARTITION BY symbol, 
            (CASE WHEN returns < 0 THEN returns_std20 ELSE close END = 
    MAX(CASE WHEN returns < 0 THEN returns_std20 ELSE close END) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
)
            ORDER BY timestamp DESC
        ) - 1
    )
 AS alpha001_argmax,
        
        -- Alpha002 相关计算
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY 
    
    CASE 
        WHEN volume > 0 THEN LN(volume)
        ELSE NULL
    END
 - 
    LAG(
    CASE 
        WHEN volume > 0 THEN LN(volume)
        ELSE NULL
    END
, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )


    )
 AS alpha002_rank_delta_log_vol,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY 
    CASE 
        WHEN open = 0 OR open IS NULL THEN NULL
        WHEN ABS(open) < 1e-10 THEN NULL
        ELSE close - open / open
    END

    )
 AS alpha002_rank_ret,
        
        -- Alpha005 相关计算
        close_ma10 AS alpha005_mean_vwap,
        
        -- Alpha007 相关计算
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY 
    ABS(close_delta7)

        ROWS BETWEEN 59 PRECEDING AND CURRENT ROW
    )
 AS alpha007_ts_rank,
        
    CASE 
        WHEN close_delta7 > 0 THEN 1
        WHEN close_delta7 < 0 THEN -1
        ELSE 0
    END
 AS alpha007_sign,
        
        -- Alpha008 相关计算
        (open * 5 + returns_sum250 / 50) AS alpha008_sum_open_returns,  -- 简化计算
        
    LAG((open * 5 + returns_sum250 / 50), 10) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS alpha008_delay_sum,
        
        -- Alpha009-010 逻辑
        CASE 
            WHEN 
    MIN(close_delta1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 > 0 THEN close_delta1
            WHEN 
    MAX(close_delta1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 < 0 THEN close_delta1
            ELSE -1 * close_delta1
        END AS alpha009_logic,
        
        -- Alpha011 相关计算
        
    MAX(vwap - close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha011_max_vwap_close,
        
    MIN(vwap - close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha011_min_vwap_close,
        
        -- Alpha014 相关计算
        
    returns - 
    LAG(returns, 3) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS alpha014_delta_returns,
        
        -- Alpha015 相关计算
        
    -- 使用DuckDB的CORR窗口函数
    CORR(high_rank, volume_rank) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha015_corr_high_vol,
        
        -- Alpha017 相关计算
        
    close_delta1 - 
    LAG(close_delta1, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS alpha017_delta_delta_close,
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY 
    CASE 
        WHEN adv20 = 0 OR adv20 IS NULL THEN NULL
        WHEN ABS(adv20) < 1e-10 THEN NULL
        ELSE volume / adv20
    END

        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS alpha017_ts_rank_vol_adv,
        
        -- Alpha018 相关计算
        
    STDDEV(
    ABS(close - open)
) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS alpha018_stddev,
        close - open AS alpha018_close_open_diff,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(close, open) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS alpha018_corr_close_open,
        
        -- Alpha019 相关计算
        (close - close_lag7) + close_delta7 AS alpha019_close_diff_plus_delta,
        1 + returns_sum250 AS alpha019_sum_returns,
        
        -- Alpha020 相关计算
        open - high_lag1 AS alpha020_open_delay_high,
        open - close_lag1 AS alpha020_open_delay_close,
        open - low_lag1 AS alpha020_open_delay_low
        
    FROM base_data
),

-- 计算Alpha因子
alpha_factors AS (
    SELECT 
        symbol,
        timestamp,
        
        -- Alpha 001: RANK(Ts_ArgMax(SignedPower(((returns < 0) ? stddev(returns, 20) : close), 2.), 5)) - 0.5
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha001_argmax
    )
 - 0.5 AS alpha001,
        
        -- Alpha 002: (-1 * correlation(rank(delta(log(volume), 2)), rank(((close - open) / open)), 6))
        -1 * 
    -- 使用DuckDB的CORR窗口函数
    CORR(alpha002_rank_delta_log_vol, alpha002_rank_ret) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 5 PRECEDING AND CURRENT ROW
    )
 AS alpha002,
        
        -- Alpha 003: (-1 * correlation(rank(open), rank(volume), 10))
        -1 * corr_open_volume_10 AS alpha003,
        
        -- Alpha 004: (-1 * Ts_Rank(rank(low), 9))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY low_rank
        ROWS BETWEEN 8 PRECEDING AND CURRENT ROW
    )
 AS alpha004,
        
        -- Alpha 005: (rank((open - (sum(vwap, 10) / 10))) * (-1 * abs(rank((close - vwap)))))
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY open - alpha005_mean_vwap
    )
 * (-1 * 
    ABS(
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY close - vwap
    )
)
) AS alpha005,
        
        -- Alpha 006: (-1 * correlation(open, volume, 10))
        -1 * corr_open_volume_10 AS alpha006,
        
        -- Alpha 007: ((adv20 < volume) ? ((-1 * ts_rank(abs(delta(close, 7)), 60)) * sign(delta(close, 7))) : (-1))
        CASE 
            WHEN adv20 < volume THEN (-1 * alpha007_ts_rank) * alpha007_sign
            ELSE -1
        END AS alpha007,
        
        -- Alpha 008: (-1 * rank(((sum(open, 5) * sum(returns, 5)) - delay((sum(open, 5) * sum(returns, 5)), 10))))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha008_sum_open_returns - alpha008_delay_sum
    )
 AS alpha008,
        
        -- Alpha 009: ((0 < ts_min(delta(close, 1), 5)) ? delta(close, 1) : ((ts_max(delta(close, 1), 5) < 0) ? delta(close, 1) : (-1 * delta(close, 1))))
        alpha009_logic AS alpha009,
        
        -- Alpha 010: rank(((0 < ts_min(delta(close, 1), 4)) ? delta(close, 1) : ((ts_max(delta(close, 1), 4) < 0) ? delta(close, 1) : (-1 * delta(close, 1)))))
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha009_logic
    )
 AS alpha010,
        
        -- Alpha 011: ((rank(ts_max((vwap - close), 3)) + rank(ts_min((vwap - close), 3))) * rank(delta(volume, 3)))
        (
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha011_max_vwap_close
    )
 + 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha011_min_vwap_close
    )
) * 
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY volume_delta3
    )
 AS alpha011,
        
        -- Alpha 012: (sign(delta(volume, 1)) * (-1 * delta(close, 1)))
        
    CASE 
        WHEN volume_delta1 > 0 THEN 1
        WHEN volume_delta1 < 0 THEN -1
        ELSE 0
    END
 * (-1 * close_delta1) AS alpha012,
        
        -- Alpha 013: (-1 * rank(covariance(rank(close), rank(volume), 5)))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY cov_close_volume_5
    )
 AS alpha013,
        
        -- Alpha 014: ((-1 * rank(delta(returns, 3))) * correlation(open, volume, 10))
        (-1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha014_delta_returns
    )
) * corr_open_volume_10 AS alpha014,
        
        -- Alpha 015: (-1 * sum(rank(correlation(rank(high), rank(volume), 3)), 3))
        -1 * 
    SUM(
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha015_corr_high_vol
    )
) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha015,
        
        -- Alpha 016: (-1 * rank(covariance(rank(high), rank(volume), 5)))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY cov_high_volume_5
    )
 AS alpha016,
        
        -- Alpha 017: (((-1 * rank(ts_rank(close, 10))) * rank(delta(delta(close, 1), 1))) * rank(ts_rank((volume / adv20), 5)))
        ((-1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY close_ts_rank10
    )
) * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha017_delta_delta_close
    )
) * 
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha017_ts_rank_vol_adv
    )
 AS alpha017,
        
        -- Alpha 018: (-1 * rank(((stddev(abs((close - open)), 5) + (close - open)) + correlation(close, open, 10))))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha018_stddev + alpha018_close_open_diff + alpha018_corr_close_open
    )
 AS alpha018,
        
        -- Alpha 019: ((-1 * sign(((close - delay(close, 7)) + delta(close, 7)))) * (1 + rank((1 + sum(returns, 250)))))
        (-1 * 
    CASE 
        WHEN alpha019_close_diff_plus_delta > 0 THEN 1
        WHEN alpha019_close_diff_plus_delta < 0 THEN -1
        ELSE 0
    END
) * 
        (1 + 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha019_sum_returns
    )
) AS alpha019,
        
        -- Alpha 020: (((-1 * rank((open - delay(high, 1)))) * rank((open - delay(close, 1)))) * rank((open - delay(low, 1))))
        ((-1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha020_open_delay_high
    )
) * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha020_open_delay_close
    )
) * 
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha020_open_delay_low
    )
 AS alpha020
        
    FROM intermediate_calcs
)

SELECT * FROM alpha_factors
    );
  
  
[0m15:30:11.750503 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_factors_001_020"} */

  
    
    

    create  table
      "quant_features"."main"."alpha_factors_001_020__dbt_tmp"
  
    as (
      

-- Alpha 101 因子计算 (001-020)
-- 基于预处理的基础数据计算前20个Alpha因子

WITH base_data AS (
    SELECT * FROM "quant_features"."main"."alpha_base_data"
),

-- 预计算一些复杂的中间变量
intermediate_calcs AS (
    SELECT 
        *,
        -- Alpha001 相关计算
        
    -- 使用ROW_NUMBER()来找到最大值的位置
    (5 - 1) - (
        ROW_NUMBER() OVER (
            PARTITION BY symbol, 
            (CASE WHEN returns < 0 THEN returns_std20 ELSE close END = 
    MAX(CASE WHEN returns < 0 THEN returns_std20 ELSE close END) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
)
            ORDER BY timestamp DESC
        ) - 1
    )
 AS alpha001_argmax,
        
        -- Alpha002 相关计算
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY 
    
    CASE 
        WHEN volume > 0 THEN LN(volume)
        ELSE NULL
    END
 - 
    LAG(
    CASE 
        WHEN volume > 0 THEN LN(volume)
        ELSE NULL
    END
, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )


    )
 AS alpha002_rank_delta_log_vol,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY 
    CASE 
        WHEN open = 0 OR open IS NULL THEN NULL
        WHEN ABS(open) < 1e-10 THEN NULL
        ELSE close - open / open
    END

    )
 AS alpha002_rank_ret,
        
        -- Alpha005 相关计算
        close_ma10 AS alpha005_mean_vwap,
        
        -- Alpha007 相关计算
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY 
    ABS(close_delta7)

        ROWS BETWEEN 59 PRECEDING AND CURRENT ROW
    )
 AS alpha007_ts_rank,
        
    CASE 
        WHEN close_delta7 > 0 THEN 1
        WHEN close_delta7 < 0 THEN -1
        ELSE 0
    END
 AS alpha007_sign,
        
        -- Alpha008 相关计算
        (open * 5 + returns_sum250 / 50) AS alpha008_sum_open_returns,  -- 简化计算
        
    LAG((open * 5 + returns_sum250 / 50), 10) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS alpha008_delay_sum,
        
        -- Alpha009-010 逻辑
        CASE 
            WHEN 
    MIN(close_delta1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 > 0 THEN close_delta1
            WHEN 
    MAX(close_delta1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 < 0 THEN close_delta1
            ELSE -1 * close_delta1
        END AS alpha009_logic,
        
        -- Alpha011 相关计算
        
    MAX(vwap - close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha011_max_vwap_close,
        
    MIN(vwap - close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha011_min_vwap_close,
        
        -- Alpha014 相关计算
        
    returns - 
    LAG(returns, 3) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS alpha014_delta_returns,
        
        -- Alpha015 相关计算
        
    -- 使用DuckDB的CORR窗口函数
    CORR(high_rank, volume_rank) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha015_corr_high_vol,
        
        -- Alpha017 相关计算
        
    close_delta1 - 
    LAG(close_delta1, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS alpha017_delta_delta_close,
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY 
    CASE 
        WHEN adv20 = 0 OR adv20 IS NULL THEN NULL
        WHEN ABS(adv20) < 1e-10 THEN NULL
        ELSE volume / adv20
    END

        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS alpha017_ts_rank_vol_adv,
        
        -- Alpha018 相关计算
        
    STDDEV(
    ABS(close - open)
) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS alpha018_stddev,
        close - open AS alpha018_close_open_diff,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(close, open) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS alpha018_corr_close_open,
        
        -- Alpha019 相关计算
        (close - close_lag7) + close_delta7 AS alpha019_close_diff_plus_delta,
        1 + returns_sum250 AS alpha019_sum_returns,
        
        -- Alpha020 相关计算
        open - high_lag1 AS alpha020_open_delay_high,
        open - close_lag1 AS alpha020_open_delay_close,
        open - low_lag1 AS alpha020_open_delay_low
        
    FROM base_data
),

-- 计算Alpha因子
alpha_factors AS (
    SELECT 
        symbol,
        timestamp,
        
        -- Alpha 001: RANK(Ts_ArgMax(SignedPower(((returns < 0) ? stddev(returns, 20) : close), 2.), 5)) - 0.5
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha001_argmax
    )
 - 0.5 AS alpha001,
        
        -- Alpha 002: (-1 * correlation(rank(delta(log(volume), 2)), rank(((close - open) / open)), 6))
        -1 * 
    -- 使用DuckDB的CORR窗口函数
    CORR(alpha002_rank_delta_log_vol, alpha002_rank_ret) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 5 PRECEDING AND CURRENT ROW
    )
 AS alpha002,
        
        -- Alpha 003: (-1 * correlation(rank(open), rank(volume), 10))
        -1 * corr_open_volume_10 AS alpha003,
        
        -- Alpha 004: (-1 * Ts_Rank(rank(low), 9))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY low_rank
        ROWS BETWEEN 8 PRECEDING AND CURRENT ROW
    )
 AS alpha004,
        
        -- Alpha 005: (rank((open - (sum(vwap, 10) / 10))) * (-1 * abs(rank((close - vwap)))))
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY open - alpha005_mean_vwap
    )
 * (-1 * 
    ABS(
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY close - vwap
    )
)
) AS alpha005,
        
        -- Alpha 006: (-1 * correlation(open, volume, 10))
        -1 * corr_open_volume_10 AS alpha006,
        
        -- Alpha 007: ((adv20 < volume) ? ((-1 * ts_rank(abs(delta(close, 7)), 60)) * sign(delta(close, 7))) : (-1))
        CASE 
            WHEN adv20 < volume THEN (-1 * alpha007_ts_rank) * alpha007_sign
            ELSE -1
        END AS alpha007,
        
        -- Alpha 008: (-1 * rank(((sum(open, 5) * sum(returns, 5)) - delay((sum(open, 5) * sum(returns, 5)), 10))))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha008_sum_open_returns - alpha008_delay_sum
    )
 AS alpha008,
        
        -- Alpha 009: ((0 < ts_min(delta(close, 1), 5)) ? delta(close, 1) : ((ts_max(delta(close, 1), 5) < 0) ? delta(close, 1) : (-1 * delta(close, 1))))
        alpha009_logic AS alpha009,
        
        -- Alpha 010: rank(((0 < ts_min(delta(close, 1), 4)) ? delta(close, 1) : ((ts_max(delta(close, 1), 4) < 0) ? delta(close, 1) : (-1 * delta(close, 1)))))
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha009_logic
    )
 AS alpha010,
        
        -- Alpha 011: ((rank(ts_max((vwap - close), 3)) + rank(ts_min((vwap - close), 3))) * rank(delta(volume, 3)))
        (
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha011_max_vwap_close
    )
 + 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha011_min_vwap_close
    )
) * 
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY volume_delta3
    )
 AS alpha011,
        
        -- Alpha 012: (sign(delta(volume, 1)) * (-1 * delta(close, 1)))
        
    CASE 
        WHEN volume_delta1 > 0 THEN 1
        WHEN volume_delta1 < 0 THEN -1
        ELSE 0
    END
 * (-1 * close_delta1) AS alpha012,
        
        -- Alpha 013: (-1 * rank(covariance(rank(close), rank(volume), 5)))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY cov_close_volume_5
    )
 AS alpha013,
        
        -- Alpha 014: ((-1 * rank(delta(returns, 3))) * correlation(open, volume, 10))
        (-1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha014_delta_returns
    )
) * corr_open_volume_10 AS alpha014,
        
        -- Alpha 015: (-1 * sum(rank(correlation(rank(high), rank(volume), 3)), 3))
        -1 * 
    SUM(
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha015_corr_high_vol
    )
) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha015,
        
        -- Alpha 016: (-1 * rank(covariance(rank(high), rank(volume), 5)))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY cov_high_volume_5
    )
 AS alpha016,
        
        -- Alpha 017: (((-1 * rank(ts_rank(close, 10))) * rank(delta(delta(close, 1), 1))) * rank(ts_rank((volume / adv20), 5)))
        ((-1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY close_ts_rank10
    )
) * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha017_delta_delta_close
    )
) * 
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha017_ts_rank_vol_adv
    )
 AS alpha017,
        
        -- Alpha 018: (-1 * rank(((stddev(abs((close - open)), 5) + (close - open)) + correlation(close, open, 10))))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha018_stddev + alpha018_close_open_diff + alpha018_corr_close_open
    )
 AS alpha018,
        
        -- Alpha 019: ((-1 * sign(((close - delay(close, 7)) + delta(close, 7)))) * (1 + rank((1 + sum(returns, 250)))))
        (-1 * 
    CASE 
        WHEN alpha019_close_diff_plus_delta > 0 THEN 1
        WHEN alpha019_close_diff_plus_delta < 0 THEN -1
        ELSE 0
    END
) * 
        (1 + 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha019_sum_returns
    )
) AS alpha019,
        
        -- Alpha 020: (((-1 * rank((open - delay(high, 1)))) * rank((open - delay(close, 1)))) * rank((open - delay(low, 1))))
        ((-1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha020_open_delay_high
    )
) * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha020_open_delay_close
    )
) * 
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha020_open_delay_low
    )
 AS alpha020
        
    FROM intermediate_calcs
)

SELECT * FROM alpha_factors
    );
  
  
[0m15:30:11.751057 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m15:30:11.751335 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_001_020: ROLLBACK
[0m15:30:11.754636 [debug] [Thread-1 (]: Failed to rollback 'model.quant_features.alpha_factors_001_020'
[0m15:30:11.754906 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_001_020: Close
[0m15:30:11.757327 [debug] [Thread-1 (]: Runtime Error in model alpha_factors_001_020 (models/alpha101/alpha_factors_001_020.sql)
  Parser Error: window functions are not allowed in window definitions
[0m15:30:11.758492 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1ab1ef80-2fd2-4638-9b1b-959a0187c48b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a4f748f50>]}
[0m15:30:11.758960 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model main.alpha_factors_001_020 ............... [[31mERROR[0m in 0.05s]
[0m15:30:11.759318 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_factors_001_020
[0m15:30:11.760068 [debug] [Thread-7 (]: Marking all children of 'model.quant_features.alpha_factors_001_020' to be skipped because of status 'error'.  Reason: Runtime Error in model alpha_factors_001_020 (models/alpha101/alpha_factors_001_020.sql)
  Parser Error: window functions are not allowed in window definitions.
[0m15:30:11.762556 [debug] [MainThread]: Using duckdb connection "master"
[0m15:30:11.762801 [debug] [MainThread]: On master: BEGIN
[0m15:30:11.762964 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:30:11.763345 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:30:11.763530 [debug] [MainThread]: On master: COMMIT
[0m15:30:11.763691 [debug] [MainThread]: Using duckdb connection "master"
[0m15:30:11.763841 [debug] [MainThread]: On master: COMMIT
[0m15:30:11.764192 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:30:11.764401 [debug] [MainThread]: On master: Close
[0m15:30:11.764677 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:30:11.764834 [debug] [MainThread]: Connection 'model.quant_features.alpha_factors_001_020' was properly closed.
[0m15:30:11.764999 [info ] [MainThread]: 
[0m15:30:11.765179 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m15:30:11.765545 [debug] [MainThread]: Command end result
[0m15:30:11.787508 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:30:11.788702 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:30:11.792411 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:30:11.792637 [info ] [MainThread]: 
[0m15:30:11.792851 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:30:11.793030 [info ] [MainThread]: 
[0m15:30:11.793250 [error] [MainThread]: [31mFailure in model alpha_factors_001_020 (models/alpha101/alpha_factors_001_020.sql)[0m
[0m15:30:11.793453 [error] [MainThread]:   Runtime Error in model alpha_factors_001_020 (models/alpha101/alpha_factors_001_020.sql)
  Parser Error: window functions are not allowed in window definitions
[0m15:30:11.793606 [info ] [MainThread]: 
[0m15:30:11.793788 [info ] [MainThread]:   compiled code at target/compiled/quant_features/models/alpha101/alpha_factors_001_020.sql
[0m15:30:11.793931 [info ] [MainThread]: 
[0m15:30:11.794095 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m15:30:11.794633 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.65997976, "process_in_blocks": "0", "process_kernel_time": 0.159904, "process_mem_max_rss": "150912", "process_out_blocks": "3576", "process_user_time": 1.403141}
[0m15:30:11.794924 [debug] [MainThread]: Command `dbt run` failed at 15:30:11.794866 after 0.66 seconds
[0m15:30:11.795133 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a502475f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a54779810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a547799f0>]}
[0m15:30:11.795348 [debug] [MainThread]: Flushing usage events
[0m15:30:11.847239 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:31:28.548983 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43cf9e7770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43d102da90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43ce84fd90>]}


============================== 15:31:28.551268 | ab870e2e-37b6-4994-88c1-d5ee27c74a02 ==============================
[0m15:31:28.551268 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:31:28.551585 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'partial_parse': 'True', 'indirect_selection': 'eager', 'warn_error': 'None', 'use_experimental_parser': 'False', 'debug': 'False', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'quiet': 'False', 'introspect': 'True', 'use_colors': 'True', 'version_check': 'True', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'profiles_dir': '/workspace/dbt_project', 'static_parser': 'True', 'log_path': '/workspace/dbt_project/logs', 'log_format': 'default', 'invocation_command': 'dbt show --select alpha_base_data --limit 5', 'empty': 'None', 'no_print': 'None', 'log_cache_events': 'False'}
[0m15:31:28.691105 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ab870e2e-37b6-4994-88c1-d5ee27c74a02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43cf566650>]}
[0m15:31:28.732205 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ab870e2e-37b6-4994-88c1-d5ee27c74a02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43ce94abe0>]}
[0m15:31:28.740038 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:31:28.768921 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:31:28.855311 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m15:31:28.855702 [debug] [MainThread]: Partial parsing: added file: quant_features://models/alpha101/alpha_factors_simple.sql
[0m15:31:29.041758 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ab870e2e-37b6-4994-88c1-d5ee27c74a02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43ca4ca250>]}
[0m15:31:29.144932 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:31:29.146241 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:31:29.150148 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ab870e2e-37b6-4994-88c1-d5ee27c74a02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43ca7bd310>]}
[0m15:31:29.150493 [info ] [MainThread]: Found 16 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:31:29.150726 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ab870e2e-37b6-4994-88c1-d5ee27c74a02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43ca3d4f30>]}
[0m15:31:29.152100 [info ] [MainThread]: 
[0m15:31:29.152400 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:31:29.152582 [info ] [MainThread]: 
[0m15:31:29.152884 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:31:29.156587 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features_main'
[0m15:31:29.170523 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:31:29.170776 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:31:29.170958 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:31:29.183092 [debug] [ThreadPool]: SQL status: OK in 0.012 seconds
[0m15:31:29.183362 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:31:29.183547 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:31:29.190739 [debug] [ThreadPool]: SQL status: OK in 0.007 seconds
[0m15:31:29.191728 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:31:29.192393 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:31:29.192614 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:31:29.194405 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ab870e2e-37b6-4994-88c1-d5ee27c74a02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43ca2dc050>]}
[0m15:31:29.197371 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_base_data
[0m15:31:29.197773 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.alpha_base_data)
[0m15:31:29.198016 [debug] [Thread-1 (]: Began compiling node model.quant_features.alpha_base_data
[0m15:31:29.220181 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.alpha_base_data"
[0m15:31:29.220736 [debug] [Thread-1 (]: Began executing node model.quant_features.alpha_base_data
[0m15:31:29.225281 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_base_data"
[0m15:31:29.225826 [debug] [Thread-1 (]: On model.quant_features.alpha_base_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_base_data"} */

  
  

-- Alpha 101 基础数据准备
-- 为Alpha因子计算准备所有必要的基础数据

WITH base_ohlc AS (
    SELECT 
        symbol,
        timestamp,
        open,
        high,
        low,
        close,
        volume,
        -- 计算VWAP (简化版本，假设等权重)
        (high + low + close) / 3 AS vwap,
        -- 计算returns
        CASE 
            WHEN LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp) IS NOT NULL
            THEN (close - LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp)) / 
                 LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp)
            ELSE 0
        END AS returns
    FROM "quant_features"."main"."stg_ohlc_data"
    WHERE timestamp >= CAST('2020-01-01' AS DATE)
      AND timestamp <= CAST('2024-12-31' AS DATE)
),

enhanced_data AS (
    SELECT 
        *,
        -- 计算ADV (Average Daily Volume)
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )

 AS adv20,
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )

 AS adv10,
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )

 AS adv5,
        
        -- 预计算一些常用的时间序列指标
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_ma5,
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS close_ma10,
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_ma20,
        
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS volume_ma20,
        
        -- 预计算滚动标准差
        
    STDDEV(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_std20,
        
    STDDEV(returns) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS returns_std20,
        
        -- 预计算一些延迟项
        
    LAG(close, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag1,
        
    LAG(close, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag2,
        
    LAG(close, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag5,
        
    LAG(close, 10) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag10,
        
    LAG(close, 20) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag20,
        
    LAG(volume, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS volume_lag1,
        
    LAG(high, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS high_lag1,
        
    LAG(low, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS low_lag1,
        
    LAG(vwap, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS vwap_lag5,
        
        -- 预计算一些差值项
        
    close - 
    LAG(close, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta1,
        
    close - 
    LAG(close, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta2,
        
    close - 
    LAG(close, 3) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta3,
        
    close - 
    LAG(close, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta5,
        
    close - 
    LAG(close, 7) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta7,
        
    close - 
    LAG(close, 10) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta10,
        
    volume - 
    LAG(volume, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS volume_delta1,
        
    volume - 
    LAG(volume, 3) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS volume_delta3,
        
    high - 
    LAG(high, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS high_delta2,
        
        -- 预计算一些排序项
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY close
    )
 AS close_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY volume
    )
 AS volume_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY high
    )
 AS high_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY low
    )
 AS low_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY vwap
    )
 AS vwap_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY returns
    )
 AS returns_rank,
        
        -- 预计算时间序列排序
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY close
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS close_ts_rank10,
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY volume
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_ts_rank5,
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY high
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS high_ts_rank5,
        
        -- 预计算一些最值项
        
    MIN(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 99 PRECEDING AND CURRENT ROW
    )
 AS close_min100,
        
    MAX(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS close_max3,
        
    MIN(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_min5,
        
    MAX(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_max5,
        
    MIN(low) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS low_min5,
        
    MAX(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS high_max3,
        
        -- 预计算一些求和项
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_sum5,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 7 PRECEDING AND CURRENT ROW
    )
 AS close_sum8,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_sum20,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 99 PRECEDING AND CURRENT ROW
    )
 AS close_sum100,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 199 PRECEDING AND CURRENT ROW
    )
 AS close_sum200,
        
    SUM(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_sum5,
        
    SUM(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS volume_sum20,
        
    SUM(returns) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 249 PRECEDING AND CURRENT ROW
    )
 AS returns_sum250,
        
    SUM(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS high_sum5,
        
    SUM(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS high_sum20,
        
        -- 预计算一些相关性
        
    -- 使用DuckDB的CORR窗口函数
    CORR(close, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS corr_close_volume_10,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(open, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS corr_open_volume_10,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(high, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS corr_high_volume_5,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(vwap, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 5 PRECEDING AND CURRENT ROW
    )
 AS corr_vwap_volume_6,
        
        -- 预计算一些协方差
        
    COVAR_SAMP(close, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS cov_close_volume_5,
        
    COVAR_SAMP(high, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS cov_high_volume_5
        
    FROM base_ohlc
    WHERE timestamp >= CAST(CAST('2020-01-01' AS DATE) AS DATE) - INTERVAL '250 days'  -- 扩展时间范围以确保有足够的历史数据
),

-- 过滤回原始时间范围
final_data AS (
    SELECT *
    FROM enhanced_data
    WHERE timestamp >= CAST('2020-01-01' AS DATE)
      AND timestamp <= CAST('2024-12-31' AS DATE)
)

SELECT * FROM final_data
  
  limit 5

[0m15:31:29.226321 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:31:29.250169 [debug] [Thread-1 (]: SQL status: OK in 0.024 seconds
[0m15:31:29.254379 [debug] [Thread-1 (]: On model.quant_features.alpha_base_data: Close
[0m15:31:29.255662 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_base_data
[0m15:31:29.256634 [debug] [Thread-2 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78
[0m15:31:29.257149 [debug] [Thread-2 (]: Acquiring new duckdb connection 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78'
[0m15:31:29.257441 [debug] [Thread-1 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0
[0m15:31:29.257976 [debug] [Thread-3 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d
[0m15:31:29.258566 [debug] [Thread-2 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78
[0m15:31:29.258930 [debug] [Thread-4 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352
[0m15:31:29.259276 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.quant_features.alpha_base_data, now test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0)
[0m15:31:29.259776 [debug] [Thread-3 (]: Acquiring new duckdb connection 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d'
[0m15:31:29.264702 [debug] [Thread-4 (]: Acquiring new duckdb connection 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352'
[0m15:31:29.265183 [debug] [Thread-1 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0
[0m15:31:29.266581 [debug] [Thread-3 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d
[0m15:31:29.267276 [debug] [Thread-4 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352
[0m15:31:29.279020 [debug] [Thread-4 (]: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_volume__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:31:29.279437 [debug] [Thread-4 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352
[0m15:31:29.279820 [debug] [Thread-4 (]: Began running node test.quant_features.not_null_alpha_base_data_close.6db60d82aa
[0m15:31:29.280983 [debug] [Thread-3 (]: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_returns__10___1 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:31:29.282354 [debug] [Thread-1 (]: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_vwap__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:31:29.282798 [debug] [Thread-7 (]: Marking all children of 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352' to be skipped because of status 'error'.  Reason: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_volume__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m15:31:29.283330 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352, now test.quant_features.not_null_alpha_base_data_close.6db60d82aa)
[0m15:31:29.284595 [debug] [Thread-2 (]: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_close__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:31:29.285232 [debug] [Thread-3 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d
[0m15:31:29.285872 [debug] [Thread-1 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0
[0m15:31:29.286689 [debug] [Thread-4 (]: Began compiling node test.quant_features.not_null_alpha_base_data_close.6db60d82aa
[0m15:31:29.287148 [debug] [Thread-2 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78
[0m15:31:29.287495 [debug] [Thread-3 (]: Began running node test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd
[0m15:31:29.287829 [debug] [Thread-7 (]: Marking all children of 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d' to be skipped because of status 'error'.  Reason: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_returns__10___1 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m15:31:29.288228 [debug] [Thread-1 (]: Began running node test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974
[0m15:31:29.294244 [debug] [Thread-2 (]: Began running node test.quant_features.not_null_alpha_base_data_volume.21044032e6
[0m15:31:29.293848 [debug] [Thread-4 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_close.6db60d82aa"
[0m15:31:29.294920 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d, now test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd)
[0m15:31:29.295350 [debug] [Thread-7 (]: Marking all children of 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0' to be skipped because of status 'error'.  Reason: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_vwap__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m15:31:29.295666 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0, now test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974)
[0m15:31:29.296045 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78, now test.quant_features.not_null_alpha_base_data_volume.21044032e6)
[0m15:31:29.296435 [debug] [Thread-3 (]: Began compiling node test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd
[0m15:31:29.296974 [debug] [Thread-7 (]: Marking all children of 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78' to be skipped because of status 'error'.  Reason: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_close__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m15:31:29.297309 [debug] [Thread-4 (]: Began executing node test.quant_features.not_null_alpha_base_data_close.6db60d82aa
[0m15:31:29.297794 [debug] [Thread-1 (]: Began compiling node test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974
[0m15:31:29.298123 [debug] [Thread-2 (]: Began compiling node test.quant_features.not_null_alpha_base_data_volume.21044032e6
[0m15:31:29.300912 [debug] [Thread-3 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd"
[0m15:31:29.303383 [debug] [Thread-4 (]: Using duckdb connection "test.quant_features.not_null_alpha_base_data_close.6db60d82aa"
[0m15:31:29.306019 [debug] [Thread-1 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974"
[0m15:31:29.308671 [debug] [Thread-2 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_volume.21044032e6"
[0m15:31:29.309291 [debug] [Thread-3 (]: Began executing node test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd
[0m15:31:29.309813 [debug] [Thread-4 (]: On test.quant_features.not_null_alpha_base_data_close.6db60d82aa: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "test.quant_features.not_null_alpha_base_data_close.6db60d82aa"} */

  
  
    
    



select close
from "quant_features"."main"."alpha_base_data"
where close is null



  
  limit 5

[0m15:31:29.310358 [debug] [Thread-1 (]: Began executing node test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974
[0m15:31:29.312716 [debug] [Thread-3 (]: Using duckdb connection "test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd"
[0m15:31:29.313233 [debug] [Thread-2 (]: Began executing node test.quant_features.not_null_alpha_base_data_volume.21044032e6
[0m15:31:29.313745 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m15:31:29.315979 [debug] [Thread-1 (]: Using duckdb connection "test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974"
[0m15:31:29.316393 [debug] [Thread-3 (]: On test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd"} */

  
  
    
    



select symbol
from "quant_features"."main"."alpha_base_data"
where symbol is null



  
  limit 5

[0m15:31:29.318335 [debug] [Thread-2 (]: Using duckdb connection "test.quant_features.not_null_alpha_base_data_volume.21044032e6"
[0m15:31:29.319065 [debug] [Thread-1 (]: On test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974"} */

  
  
    
    



select timestamp
from "quant_features"."main"."alpha_base_data"
where timestamp is null



  
  limit 5

[0m15:31:29.319417 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:31:29.319720 [debug] [Thread-4 (]: SQL status: OK in 0.006 seconds
[0m15:31:29.320270 [debug] [Thread-2 (]: On test.quant_features.not_null_alpha_base_data_volume.21044032e6: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "test.quant_features.not_null_alpha_base_data_volume.21044032e6"} */

  
  
    
    



select volume
from "quant_features"."main"."alpha_base_data"
where volume is null



  
  limit 5

[0m15:31:29.320841 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:31:29.322086 [debug] [Thread-4 (]: On test.quant_features.not_null_alpha_base_data_close.6db60d82aa: Close
[0m15:31:29.322609 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:31:29.323844 [debug] [Thread-3 (]: SQL status: OK in 0.004 seconds
[0m15:31:29.324388 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m15:31:29.325220 [debug] [Thread-3 (]: On test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd: Close
[0m15:31:29.325770 [debug] [Thread-3 (]: Finished running node test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd
[0m15:31:29.326014 [debug] [Thread-3 (]: Began running node test.quant_features.not_null_alpha_base_data_vwap.380f9f922d
[0m15:31:29.326256 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd, now test.quant_features.not_null_alpha_base_data_vwap.380f9f922d)
[0m15:31:29.327064 [debug] [Thread-1 (]: On test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974: Close
[0m15:31:29.327696 [debug] [Thread-4 (]: Finished running node test.quant_features.not_null_alpha_base_data_close.6db60d82aa
[0m15:31:29.328239 [debug] [Thread-2 (]: SQL status: OK in 0.006 seconds
[0m15:31:29.329045 [debug] [Thread-3 (]: Began compiling node test.quant_features.not_null_alpha_base_data_vwap.380f9f922d
[0m15:31:29.329942 [debug] [Thread-1 (]: Finished running node test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974
[0m15:31:29.330926 [debug] [Thread-2 (]: On test.quant_features.not_null_alpha_base_data_volume.21044032e6: Close
[0m15:31:29.333416 [debug] [Thread-3 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_vwap.380f9f922d"
[0m15:31:29.334515 [debug] [Thread-2 (]: Finished running node test.quant_features.not_null_alpha_base_data_volume.21044032e6
[0m15:31:29.335047 [debug] [Thread-3 (]: Began executing node test.quant_features.not_null_alpha_base_data_vwap.380f9f922d
[0m15:31:29.337190 [debug] [Thread-3 (]: Using duckdb connection "test.quant_features.not_null_alpha_base_data_vwap.380f9f922d"
[0m15:31:29.337471 [debug] [Thread-3 (]: On test.quant_features.not_null_alpha_base_data_vwap.380f9f922d: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "test.quant_features.not_null_alpha_base_data_vwap.380f9f922d"} */

  
  
    
    



select vwap
from "quant_features"."main"."alpha_base_data"
where vwap is null



  
  limit 5

[0m15:31:29.337673 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:31:29.338266 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m15:31:29.338993 [debug] [Thread-3 (]: On test.quant_features.not_null_alpha_base_data_vwap.380f9f922d: Close
[0m15:31:29.339485 [debug] [Thread-3 (]: Finished running node test.quant_features.not_null_alpha_base_data_vwap.380f9f922d
[0m15:31:29.339873 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:31:29.340240 [debug] [MainThread]: Connection 'test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974' was properly closed.
[0m15:31:29.340424 [debug] [MainThread]: Connection 'test.quant_features.not_null_alpha_base_data_volume.21044032e6' was properly closed.
[0m15:31:29.340565 [debug] [MainThread]: Connection 'test.quant_features.not_null_alpha_base_data_vwap.380f9f922d' was properly closed.
[0m15:31:29.340699 [debug] [MainThread]: Connection 'test.quant_features.not_null_alpha_base_data_close.6db60d82aa' was properly closed.
[0m15:31:29.340954 [error] [MainThread]: Encountered an error:
Runtime Error
  Compilation Error in test dbt_utils_accepted_range_alpha_base_data_close__0 (models/alpha101/schema.yml)
    'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:31:29.341590 [debug] [MainThread]: Resource report: {"command_name": "show", "command_success": false, "command_wall_clock_time": 0.8283336, "process_in_blocks": "0", "process_kernel_time": 0.179873, "process_mem_max_rss": "182048", "process_out_blocks": "3464", "process_user_time": 1.558881}
[0m15:31:29.341876 [debug] [MainThread]: Command `dbt show` failed at 15:31:29.341818 after 0.83 seconds
[0m15:31:29.342086 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43cebe7110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43ca3ce780>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43ca3ced00>]}
[0m15:31:29.342306 [debug] [MainThread]: Flushing usage events
[0m15:31:29.397316 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:31:32.644304 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0dbcc6b770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0dbe2a5a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0dbbae7d90>]}


============================== 15:31:32.646549 | fa49536c-51a7-40fe-ac5a-0f7ec7d5d401 ==============================
[0m15:31:32.646549 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:31:32.646869 [debug] [MainThread]: running dbt with arguments {'cache_selected_only': 'False', 'profiles_dir': '/workspace/dbt_project', 'static_parser': 'True', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'quiet': 'False', 'printer_width': '80', 'log_path': '/workspace/dbt_project/logs', 'send_anonymous_usage_stats': 'True', 'version_check': 'True', 'target_path': 'None', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'partial_parse': 'True', 'warn_error': 'None', 'use_colors': 'True', 'log_format': 'default', 'indirect_selection': 'eager', 'no_print': 'None', 'invocation_command': 'dbt show --select alpha_base_data --limit 3'}
[0m15:31:32.780687 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fa49536c-51a7-40fe-ac5a-0f7ec7d5d401', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0dbc7fe650>]}
[0m15:31:32.821815 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fa49536c-51a7-40fe-ac5a-0f7ec7d5d401', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0dbbbdebe0>]}
[0m15:31:32.829796 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:31:32.859234 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:31:32.945552 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:31:32.945800 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:31:32.982384 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fa49536c-51a7-40fe-ac5a-0f7ec7d5d401', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0db79e1650>]}
[0m15:31:33.053911 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:31:33.055078 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:31:33.059146 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fa49536c-51a7-40fe-ac5a-0f7ec7d5d401', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0db79cd310>]}
[0m15:31:33.059485 [info ] [MainThread]: Found 16 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:31:33.059696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fa49536c-51a7-40fe-ac5a-0f7ec7d5d401', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0db7a58590>]}
[0m15:31:33.061053 [info ] [MainThread]: 
[0m15:31:33.061324 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:31:33.061495 [info ] [MainThread]: 
[0m15:31:33.061784 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:31:33.065511 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features_main'
[0m15:31:33.111711 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:31:33.111960 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:31:33.112128 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:31:33.123594 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m15:31:33.123846 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:31:33.124025 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:31:33.131279 [debug] [ThreadPool]: SQL status: OK in 0.007 seconds
[0m15:31:33.132244 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:31:33.132876 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:31:33.133087 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:31:33.134771 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fa49536c-51a7-40fe-ac5a-0f7ec7d5d401', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0db75fcbb0>]}
[0m15:31:33.137578 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_base_data
[0m15:31:33.137966 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.alpha_base_data)
[0m15:31:33.138194 [debug] [Thread-1 (]: Began compiling node model.quant_features.alpha_base_data
[0m15:31:33.160286 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.alpha_base_data"
[0m15:31:33.160761 [debug] [Thread-1 (]: Began executing node model.quant_features.alpha_base_data
[0m15:31:33.165176 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_base_data"
[0m15:31:33.165726 [debug] [Thread-1 (]: On model.quant_features.alpha_base_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_base_data"} */

  
  

-- Alpha 101 基础数据准备
-- 为Alpha因子计算准备所有必要的基础数据

WITH base_ohlc AS (
    SELECT 
        symbol,
        timestamp,
        open,
        high,
        low,
        close,
        volume,
        -- 计算VWAP (简化版本，假设等权重)
        (high + low + close) / 3 AS vwap,
        -- 计算returns
        CASE 
            WHEN LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp) IS NOT NULL
            THEN (close - LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp)) / 
                 LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp)
            ELSE 0
        END AS returns
    FROM "quant_features"."main"."stg_ohlc_data"
    WHERE timestamp >= CAST('2020-01-01' AS DATE)
      AND timestamp <= CAST('2024-12-31' AS DATE)
),

enhanced_data AS (
    SELECT 
        *,
        -- 计算ADV (Average Daily Volume)
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )

 AS adv20,
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )

 AS adv10,
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )

 AS adv5,
        
        -- 预计算一些常用的时间序列指标
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_ma5,
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS close_ma10,
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_ma20,
        
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS volume_ma20,
        
        -- 预计算滚动标准差
        
    STDDEV(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_std20,
        
    STDDEV(returns) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS returns_std20,
        
        -- 预计算一些延迟项
        
    LAG(close, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag1,
        
    LAG(close, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag2,
        
    LAG(close, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag5,
        
    LAG(close, 10) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag10,
        
    LAG(close, 20) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag20,
        
    LAG(volume, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS volume_lag1,
        
    LAG(high, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS high_lag1,
        
    LAG(low, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS low_lag1,
        
    LAG(vwap, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS vwap_lag5,
        
        -- 预计算一些差值项
        
    close - 
    LAG(close, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta1,
        
    close - 
    LAG(close, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta2,
        
    close - 
    LAG(close, 3) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta3,
        
    close - 
    LAG(close, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta5,
        
    close - 
    LAG(close, 7) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta7,
        
    close - 
    LAG(close, 10) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta10,
        
    volume - 
    LAG(volume, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS volume_delta1,
        
    volume - 
    LAG(volume, 3) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS volume_delta3,
        
    high - 
    LAG(high, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS high_delta2,
        
        -- 预计算一些排序项
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY close
    )
 AS close_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY volume
    )
 AS volume_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY high
    )
 AS high_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY low
    )
 AS low_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY vwap
    )
 AS vwap_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY returns
    )
 AS returns_rank,
        
        -- 预计算时间序列排序
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY close
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS close_ts_rank10,
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY volume
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_ts_rank5,
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY high
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS high_ts_rank5,
        
        -- 预计算一些最值项
        
    MIN(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 99 PRECEDING AND CURRENT ROW
    )
 AS close_min100,
        
    MAX(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS close_max3,
        
    MIN(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_min5,
        
    MAX(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_max5,
        
    MIN(low) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS low_min5,
        
    MAX(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS high_max3,
        
        -- 预计算一些求和项
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_sum5,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 7 PRECEDING AND CURRENT ROW
    )
 AS close_sum8,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_sum20,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 99 PRECEDING AND CURRENT ROW
    )
 AS close_sum100,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 199 PRECEDING AND CURRENT ROW
    )
 AS close_sum200,
        
    SUM(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_sum5,
        
    SUM(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS volume_sum20,
        
    SUM(returns) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 249 PRECEDING AND CURRENT ROW
    )
 AS returns_sum250,
        
    SUM(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS high_sum5,
        
    SUM(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS high_sum20,
        
        -- 预计算一些相关性
        
    -- 使用DuckDB的CORR窗口函数
    CORR(close, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS corr_close_volume_10,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(open, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS corr_open_volume_10,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(high, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS corr_high_volume_5,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(vwap, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 5 PRECEDING AND CURRENT ROW
    )
 AS corr_vwap_volume_6,
        
        -- 预计算一些协方差
        
    COVAR_SAMP(close, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS cov_close_volume_5,
        
    COVAR_SAMP(high, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS cov_high_volume_5
        
    FROM base_ohlc
    WHERE timestamp >= CAST(CAST('2020-01-01' AS DATE) AS DATE) - INTERVAL '250 days'  -- 扩展时间范围以确保有足够的历史数据
),

-- 过滤回原始时间范围
final_data AS (
    SELECT *
    FROM enhanced_data
    WHERE timestamp >= CAST('2020-01-01' AS DATE)
      AND timestamp <= CAST('2024-12-31' AS DATE)
)

SELECT * FROM final_data
  
  limit 3

[0m15:31:33.166221 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:31:33.188353 [debug] [Thread-1 (]: SQL status: OK in 0.022 seconds
[0m15:31:33.191771 [debug] [Thread-1 (]: On model.quant_features.alpha_base_data: Close
[0m15:31:33.193056 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_base_data
[0m15:31:33.194321 [debug] [Thread-3 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d
[0m15:31:33.194771 [debug] [Thread-4 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352
[0m15:31:33.195250 [debug] [Thread-2 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78
[0m15:31:33.195580 [debug] [Thread-1 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0
[0m15:31:33.196028 [debug] [Thread-3 (]: Acquiring new duckdb connection 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d'
[0m15:31:33.196704 [debug] [Thread-4 (]: Acquiring new duckdb connection 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352'
[0m15:31:33.197386 [debug] [Thread-2 (]: Acquiring new duckdb connection 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78'
[0m15:31:33.197924 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.quant_features.alpha_base_data, now test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0)
[0m15:31:33.198419 [debug] [Thread-3 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d
[0m15:31:33.198850 [debug] [Thread-4 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352
[0m15:31:33.199298 [debug] [Thread-2 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78
[0m15:31:33.199872 [debug] [Thread-1 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0
[0m15:31:33.213079 [debug] [Thread-3 (]: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_returns__10___1 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:31:33.217263 [debug] [Thread-1 (]: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_vwap__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:31:33.218539 [debug] [Thread-2 (]: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_close__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:31:33.219835 [debug] [Thread-4 (]: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_volume__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:31:33.220518 [debug] [Thread-3 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d
[0m15:31:33.221227 [debug] [Thread-1 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0
[0m15:31:33.221644 [debug] [Thread-2 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78
[0m15:31:33.222320 [debug] [Thread-4 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352
[0m15:31:33.222684 [debug] [Thread-3 (]: Began running node test.quant_features.not_null_alpha_base_data_close.6db60d82aa
[0m15:31:33.223106 [debug] [Thread-7 (]: Marking all children of 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d' to be skipped because of status 'error'.  Reason: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_returns__10___1 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m15:31:33.223510 [debug] [Thread-1 (]: Began running node test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd
[0m15:31:33.223869 [debug] [Thread-2 (]: Began running node test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974
[0m15:31:33.224195 [debug] [Thread-4 (]: Began running node test.quant_features.not_null_alpha_base_data_volume.21044032e6
[0m15:31:33.224518 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d, now test.quant_features.not_null_alpha_base_data_close.6db60d82aa)
[0m15:31:33.225424 [debug] [Thread-7 (]: Marking all children of 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0' to be skipped because of status 'error'.  Reason: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_vwap__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m15:31:33.225941 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0, now test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd)
[0m15:31:33.226393 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78, now test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974)
[0m15:31:33.226798 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352, now test.quant_features.not_null_alpha_base_data_volume.21044032e6)
[0m15:31:33.227178 [debug] [Thread-3 (]: Began compiling node test.quant_features.not_null_alpha_base_data_close.6db60d82aa
[0m15:31:33.227586 [debug] [Thread-7 (]: Marking all children of 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78' to be skipped because of status 'error'.  Reason: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_close__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m15:31:33.227877 [debug] [Thread-1 (]: Began compiling node test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd
[0m15:31:33.228188 [debug] [Thread-2 (]: Began compiling node test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974
[0m15:31:33.228553 [debug] [Thread-4 (]: Began compiling node test.quant_features.not_null_alpha_base_data_volume.21044032e6
[0m15:31:33.234155 [debug] [Thread-3 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_close.6db60d82aa"
[0m15:31:33.234763 [debug] [Thread-7 (]: Marking all children of 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352' to be skipped because of status 'error'.  Reason: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_volume__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m15:31:33.237503 [debug] [Thread-1 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd"
[0m15:31:33.240464 [debug] [Thread-2 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974"
[0m15:31:33.243340 [debug] [Thread-4 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_volume.21044032e6"
[0m15:31:33.244091 [debug] [Thread-3 (]: Began executing node test.quant_features.not_null_alpha_base_data_close.6db60d82aa
[0m15:31:33.244651 [debug] [Thread-1 (]: Began executing node test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd
[0m15:31:33.245348 [debug] [Thread-2 (]: Began executing node test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974
[0m15:31:33.246152 [debug] [Thread-4 (]: Began executing node test.quant_features.not_null_alpha_base_data_volume.21044032e6
[0m15:31:33.248416 [debug] [Thread-3 (]: Using duckdb connection "test.quant_features.not_null_alpha_base_data_close.6db60d82aa"
[0m15:31:33.250717 [debug] [Thread-1 (]: Using duckdb connection "test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd"
[0m15:31:33.252983 [debug] [Thread-2 (]: Using duckdb connection "test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974"
[0m15:31:33.255257 [debug] [Thread-4 (]: Using duckdb connection "test.quant_features.not_null_alpha_base_data_volume.21044032e6"
[0m15:31:33.255703 [debug] [Thread-3 (]: On test.quant_features.not_null_alpha_base_data_close.6db60d82aa: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "test.quant_features.not_null_alpha_base_data_close.6db60d82aa"} */

  
  
    
    



select close
from "quant_features"."main"."alpha_base_data"
where close is null



  
  limit 3

[0m15:31:33.256103 [debug] [Thread-1 (]: On test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd"} */

  
  
    
    



select symbol
from "quant_features"."main"."alpha_base_data"
where symbol is null



  
  limit 3

[0m15:31:33.256489 [debug] [Thread-2 (]: On test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974"} */

  
  
    
    



select timestamp
from "quant_features"."main"."alpha_base_data"
where timestamp is null



  
  limit 3

[0m15:31:33.257000 [debug] [Thread-4 (]: On test.quant_features.not_null_alpha_base_data_volume.21044032e6: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "test.quant_features.not_null_alpha_base_data_volume.21044032e6"} */

  
  
    
    



select volume
from "quant_features"."main"."alpha_base_data"
where volume is null



  
  limit 3

[0m15:31:33.257499 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:31:33.257860 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:31:33.258357 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:31:33.258695 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m15:31:33.259590 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m15:31:33.260097 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:31:33.260457 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m15:31:33.261310 [debug] [Thread-3 (]: On test.quant_features.not_null_alpha_base_data_close.6db60d82aa: Close
[0m15:31:33.262441 [debug] [Thread-1 (]: On test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd: Close
[0m15:31:33.263457 [debug] [Thread-2 (]: On test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974: Close
[0m15:31:33.263960 [debug] [Thread-4 (]: SQL status: OK in 0.005 seconds
[0m15:31:33.264713 [debug] [Thread-3 (]: Finished running node test.quant_features.not_null_alpha_base_data_close.6db60d82aa
[0m15:31:33.265455 [debug] [Thread-1 (]: Finished running node test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd
[0m15:31:33.266193 [debug] [Thread-2 (]: Finished running node test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974
[0m15:31:33.267208 [debug] [Thread-4 (]: On test.quant_features.not_null_alpha_base_data_volume.21044032e6: Close
[0m15:31:33.267582 [debug] [Thread-3 (]: Began running node test.quant_features.not_null_alpha_base_data_vwap.380f9f922d
[0m15:31:33.268097 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.quant_features.not_null_alpha_base_data_close.6db60d82aa, now test.quant_features.not_null_alpha_base_data_vwap.380f9f922d)
[0m15:31:33.268599 [debug] [Thread-4 (]: Finished running node test.quant_features.not_null_alpha_base_data_volume.21044032e6
[0m15:31:33.269031 [debug] [Thread-3 (]: Began compiling node test.quant_features.not_null_alpha_base_data_vwap.380f9f922d
[0m15:31:33.272333 [debug] [Thread-3 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_vwap.380f9f922d"
[0m15:31:33.272805 [debug] [Thread-3 (]: Began executing node test.quant_features.not_null_alpha_base_data_vwap.380f9f922d
[0m15:31:33.274905 [debug] [Thread-3 (]: Using duckdb connection "test.quant_features.not_null_alpha_base_data_vwap.380f9f922d"
[0m15:31:33.275177 [debug] [Thread-3 (]: On test.quant_features.not_null_alpha_base_data_vwap.380f9f922d: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "test.quant_features.not_null_alpha_base_data_vwap.380f9f922d"} */

  
  
    
    



select vwap
from "quant_features"."main"."alpha_base_data"
where vwap is null



  
  limit 3

[0m15:31:33.275395 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:31:33.275964 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m15:31:33.276730 [debug] [Thread-3 (]: On test.quant_features.not_null_alpha_base_data_vwap.380f9f922d: Close
[0m15:31:33.277234 [debug] [Thread-3 (]: Finished running node test.quant_features.not_null_alpha_base_data_vwap.380f9f922d
[0m15:31:33.277837 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:31:33.278154 [debug] [MainThread]: Connection 'test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd' was properly closed.
[0m15:31:33.278343 [debug] [MainThread]: Connection 'test.quant_features.not_null_alpha_base_data_vwap.380f9f922d' was properly closed.
[0m15:31:33.278510 [debug] [MainThread]: Connection 'test.quant_features.not_null_alpha_base_data_volume.21044032e6' was properly closed.
[0m15:31:33.278647 [debug] [MainThread]: Connection 'test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974' was properly closed.
[0m15:31:33.278899 [error] [MainThread]: Encountered an error:
Runtime Error
  Compilation Error in test dbt_utils_accepted_range_alpha_base_data_volume__0 (models/alpha101/schema.yml)
    'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:31:33.279580 [debug] [MainThread]: Resource report: {"command_name": "show", "command_success": false, "command_wall_clock_time": 0.67171174, "process_in_blocks": "0", "process_kernel_time": 0.128841, "process_mem_max_rss": "178868", "process_out_blocks": "2000", "process_user_time": 1.449423}
[0m15:31:33.279875 [debug] [MainThread]: Command `dbt show` failed at 15:31:33.279817 after 0.67 seconds
[0m15:31:33.280077 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0dbbc83110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0db773d7b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0db773dff0>]}
[0m15:31:33.280304 [debug] [MainThread]: Flushing usage events
[0m15:31:33.324302 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:31:40.651720 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d7c2d3770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d7d925a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d7b13fd90>]}


============================== 15:31:40.654086 | 18b0e479-fd97-4763-98ef-e9cea82926cc ==============================
[0m15:31:40.654086 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:31:40.654426 [debug] [MainThread]: running dbt with arguments {'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'quiet': 'False', 'profiles_dir': '/workspace/dbt_project', 'log_path': '/workspace/dbt_project/logs', 'use_colors': 'True', 'printer_width': '80', 'empty': 'None', 'warn_error': 'None', 'indirect_selection': 'eager', 'invocation_command': 'dbt show --select alpha_base_data --limit 3', 'no_print': 'None', 'version_check': 'True', 'cache_selected_only': 'False', 'log_cache_events': 'False', 'static_parser': 'True', 'log_format': 'default', 'send_anonymous_usage_stats': 'True', 'write_json': 'True', 'target_path': 'None', 'use_experimental_parser': 'False', 'debug': 'False', 'fail_fast': 'False', 'introspect': 'True'}
[0m15:31:40.795194 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '18b0e479-fd97-4763-98ef-e9cea82926cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d7be56650>]}
[0m15:31:40.836026 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '18b0e479-fd97-4763-98ef-e9cea82926cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d7b236be0>]}
[0m15:31:40.843888 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:31:40.873484 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:31:40.960017 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:31:40.960266 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:31:40.996900 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '18b0e479-fd97-4763-98ef-e9cea82926cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d770d1650>]}
[0m15:31:41.067745 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:31:41.068924 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:31:41.073139 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '18b0e479-fd97-4763-98ef-e9cea82926cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d770c1310>]}
[0m15:31:41.073473 [info ] [MainThread]: Found 16 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:31:41.073679 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '18b0e479-fd97-4763-98ef-e9cea82926cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d77148590>]}
[0m15:31:41.074979 [info ] [MainThread]: 
[0m15:31:41.075249 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:31:41.075426 [info ] [MainThread]: 
[0m15:31:41.075720 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:31:41.079452 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features_main'
[0m15:31:41.126884 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:31:41.127146 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:31:41.127333 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:31:41.138852 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m15:31:41.139105 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:31:41.139297 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:31:41.146224 [debug] [ThreadPool]: SQL status: OK in 0.007 seconds
[0m15:31:41.147188 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:31:41.147792 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:31:41.148001 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:31:41.149842 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '18b0e479-fd97-4763-98ef-e9cea82926cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d76cecbb0>]}
[0m15:31:41.151974 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_base_data
[0m15:31:41.152312 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.alpha_base_data)
[0m15:31:41.152541 [debug] [Thread-1 (]: Began compiling node model.quant_features.alpha_base_data
[0m15:31:41.174853 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.alpha_base_data"
[0m15:31:41.175299 [debug] [Thread-1 (]: Began executing node model.quant_features.alpha_base_data
[0m15:31:41.179730 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_base_data"
[0m15:31:41.180283 [debug] [Thread-1 (]: On model.quant_features.alpha_base_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_base_data"} */

  
  

-- Alpha 101 基础数据准备
-- 为Alpha因子计算准备所有必要的基础数据

WITH base_ohlc AS (
    SELECT 
        symbol,
        timestamp,
        open,
        high,
        low,
        close,
        volume,
        -- 计算VWAP (简化版本，假设等权重)
        (high + low + close) / 3 AS vwap,
        -- 计算returns
        CASE 
            WHEN LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp) IS NOT NULL
            THEN (close - LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp)) / 
                 LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp)
            ELSE 0
        END AS returns
    FROM "quant_features"."main"."stg_ohlc_data"
    WHERE timestamp >= CAST('2020-01-01' AS DATE)
      AND timestamp <= CAST('2024-12-31' AS DATE)
),

enhanced_data AS (
    SELECT 
        *,
        -- 计算ADV (Average Daily Volume)
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )

 AS adv20,
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )

 AS adv10,
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )

 AS adv5,
        
        -- 预计算一些常用的时间序列指标
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_ma5,
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS close_ma10,
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_ma20,
        
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS volume_ma20,
        
        -- 预计算滚动标准差
        
    STDDEV(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_std20,
        
    STDDEV(returns) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS returns_std20,
        
        -- 预计算一些延迟项
        
    LAG(close, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag1,
        
    LAG(close, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag2,
        
    LAG(close, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag5,
        
    LAG(close, 10) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag10,
        
    LAG(close, 20) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag20,
        
    LAG(volume, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS volume_lag1,
        
    LAG(high, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS high_lag1,
        
    LAG(low, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS low_lag1,
        
    LAG(vwap, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS vwap_lag5,
        
        -- 预计算一些差值项
        
    close - 
    LAG(close, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta1,
        
    close - 
    LAG(close, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta2,
        
    close - 
    LAG(close, 3) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta3,
        
    close - 
    LAG(close, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta5,
        
    close - 
    LAG(close, 7) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta7,
        
    close - 
    LAG(close, 10) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta10,
        
    volume - 
    LAG(volume, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS volume_delta1,
        
    volume - 
    LAG(volume, 3) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS volume_delta3,
        
    high - 
    LAG(high, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS high_delta2,
        
        -- 预计算一些排序项
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY close
    )
 AS close_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY volume
    )
 AS volume_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY high
    )
 AS high_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY low
    )
 AS low_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY vwap
    )
 AS vwap_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY returns
    )
 AS returns_rank,
        
        -- 预计算时间序列排序
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY close
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS close_ts_rank10,
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY volume
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_ts_rank5,
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY high
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS high_ts_rank5,
        
        -- 预计算一些最值项
        
    MIN(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 99 PRECEDING AND CURRENT ROW
    )
 AS close_min100,
        
    MAX(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS close_max3,
        
    MIN(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_min5,
        
    MAX(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_max5,
        
    MIN(low) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS low_min5,
        
    MAX(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS high_max3,
        
        -- 预计算一些求和项
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_sum5,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 7 PRECEDING AND CURRENT ROW
    )
 AS close_sum8,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_sum20,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 99 PRECEDING AND CURRENT ROW
    )
 AS close_sum100,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 199 PRECEDING AND CURRENT ROW
    )
 AS close_sum200,
        
    SUM(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_sum5,
        
    SUM(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS volume_sum20,
        
    SUM(returns) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 249 PRECEDING AND CURRENT ROW
    )
 AS returns_sum250,
        
    SUM(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS high_sum5,
        
    SUM(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS high_sum20,
        
        -- 预计算一些相关性
        
    -- 使用DuckDB的CORR窗口函数
    CORR(close, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS corr_close_volume_10,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(open, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS corr_open_volume_10,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(high, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS corr_high_volume_5,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(vwap, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 5 PRECEDING AND CURRENT ROW
    )
 AS corr_vwap_volume_6,
        
        -- 预计算一些协方差
        
    COVAR_SAMP(close, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS cov_close_volume_5,
        
    COVAR_SAMP(high, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS cov_high_volume_5
        
    FROM base_ohlc
    WHERE timestamp >= CAST(CAST('2020-01-01' AS DATE) AS DATE) - INTERVAL '250 days'  -- 扩展时间范围以确保有足够的历史数据
),

-- 过滤回原始时间范围
final_data AS (
    SELECT *
    FROM enhanced_data
    WHERE timestamp >= CAST('2020-01-01' AS DATE)
      AND timestamp <= CAST('2024-12-31' AS DATE)
)

SELECT * FROM final_data
  
  limit 3

[0m15:31:41.180802 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:31:41.203177 [debug] [Thread-1 (]: SQL status: OK in 0.022 seconds
[0m15:31:41.207043 [debug] [Thread-1 (]: On model.quant_features.alpha_base_data: Close
[0m15:31:41.208395 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_base_data
[0m15:31:41.209365 [debug] [Thread-3 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d
[0m15:31:41.209799 [debug] [Thread-2 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78
[0m15:31:41.210330 [debug] [Thread-4 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352
[0m15:31:41.210785 [debug] [Thread-3 (]: Acquiring new duckdb connection 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d'
[0m15:31:41.212472 [debug] [Thread-3 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d
[0m15:31:41.211140 [debug] [Thread-1 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0
[0m15:31:41.212172 [debug] [Thread-4 (]: Acquiring new duckdb connection 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352'
[0m15:31:41.211694 [debug] [Thread-2 (]: Acquiring new duckdb connection 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78'
[0m15:31:41.217243 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.quant_features.alpha_base_data, now test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0)
[0m15:31:41.217732 [debug] [Thread-4 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352
[0m15:31:41.218982 [debug] [Thread-2 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78
[0m15:31:41.219426 [debug] [Thread-1 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0
[0m15:31:41.226811 [debug] [Thread-2 (]: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_close__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:31:41.228043 [debug] [Thread-4 (]: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_volume__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:31:41.229391 [debug] [Thread-3 (]: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_returns__10___1 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:31:41.232679 [debug] [Thread-2 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78
[0m15:31:41.233796 [debug] [Thread-1 (]: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_vwap__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:31:41.234359 [debug] [Thread-4 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352
[0m15:31:41.235009 [debug] [Thread-3 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d
[0m15:31:41.235384 [debug] [Thread-2 (]: Began running node test.quant_features.not_null_alpha_base_data_close.6db60d82aa
[0m15:31:41.235809 [debug] [Thread-1 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0
[0m15:31:41.236114 [debug] [Thread-4 (]: Began running node test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd
[0m15:31:41.236457 [debug] [Thread-3 (]: Began running node test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974
[0m15:31:41.236946 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78, now test.quant_features.not_null_alpha_base_data_close.6db60d82aa)
[0m15:31:41.237637 [debug] [Thread-7 (]: Marking all children of 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78' to be skipped because of status 'error'.  Reason: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_close__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m15:31:41.238220 [debug] [Thread-1 (]: Began running node test.quant_features.not_null_alpha_base_data_volume.21044032e6
[0m15:31:41.238634 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352, now test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd)
[0m15:31:41.239028 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d, now test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974)
[0m15:31:41.239348 [debug] [Thread-2 (]: Began compiling node test.quant_features.not_null_alpha_base_data_close.6db60d82aa
[0m15:31:41.240338 [debug] [Thread-7 (]: Marking all children of 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352' to be skipped because of status 'error'.  Reason: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_volume__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m15:31:41.240843 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0, now test.quant_features.not_null_alpha_base_data_volume.21044032e6)
[0m15:31:41.241227 [debug] [Thread-4 (]: Began compiling node test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd
[0m15:31:41.241733 [debug] [Thread-3 (]: Began compiling node test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974
[0m15:31:41.247155 [debug] [Thread-2 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_close.6db60d82aa"
[0m15:31:41.247787 [debug] [Thread-7 (]: Marking all children of 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d' to be skipped because of status 'error'.  Reason: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_returns__10___1 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m15:31:41.248096 [debug] [Thread-1 (]: Began compiling node test.quant_features.not_null_alpha_base_data_volume.21044032e6
[0m15:31:41.250897 [debug] [Thread-4 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd"
[0m15:31:41.253657 [debug] [Thread-3 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974"
[0m15:31:41.254276 [debug] [Thread-7 (]: Marking all children of 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0' to be skipped because of status 'error'.  Reason: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_vwap__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m15:31:41.254632 [debug] [Thread-2 (]: Began executing node test.quant_features.not_null_alpha_base_data_close.6db60d82aa
[0m15:31:41.257433 [debug] [Thread-1 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_volume.21044032e6"
[0m15:31:41.258074 [debug] [Thread-4 (]: Began executing node test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd
[0m15:31:41.258614 [debug] [Thread-3 (]: Began executing node test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974
[0m15:31:41.260756 [debug] [Thread-2 (]: Using duckdb connection "test.quant_features.not_null_alpha_base_data_close.6db60d82aa"
[0m15:31:41.263109 [debug] [Thread-4 (]: Using duckdb connection "test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd"
[0m15:31:41.263530 [debug] [Thread-1 (]: Began executing node test.quant_features.not_null_alpha_base_data_volume.21044032e6
[0m15:31:41.265640 [debug] [Thread-3 (]: Using duckdb connection "test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974"
[0m15:31:41.266124 [debug] [Thread-2 (]: On test.quant_features.not_null_alpha_base_data_close.6db60d82aa: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "test.quant_features.not_null_alpha_base_data_close.6db60d82aa"} */

  
  
    
    



select close
from "quant_features"."main"."alpha_base_data"
where close is null



  
  limit 3

[0m15:31:41.266688 [debug] [Thread-4 (]: On test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd"} */

  
  
    
    



select symbol
from "quant_features"."main"."alpha_base_data"
where symbol is null



  
  limit 3

[0m15:31:41.268819 [debug] [Thread-1 (]: Using duckdb connection "test.quant_features.not_null_alpha_base_data_volume.21044032e6"
[0m15:31:41.269344 [debug] [Thread-3 (]: On test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974"} */

  
  
    
    



select timestamp
from "quant_features"."main"."alpha_base_data"
where timestamp is null



  
  limit 3

[0m15:31:41.269838 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:31:41.270388 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m15:31:41.270784 [debug] [Thread-1 (]: On test.quant_features.not_null_alpha_base_data_volume.21044032e6: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "test.quant_features.not_null_alpha_base_data_volume.21044032e6"} */

  
  
    
    



select volume
from "quant_features"."main"."alpha_base_data"
where volume is null



  
  limit 3

[0m15:31:41.271162 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:31:41.271927 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:31:41.272419 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m15:31:41.273019 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:31:41.273840 [debug] [Thread-2 (]: On test.quant_features.not_null_alpha_base_data_close.6db60d82aa: Close
[0m15:31:41.274183 [debug] [Thread-4 (]: SQL status: OK in 0.004 seconds
[0m15:31:41.274491 [debug] [Thread-3 (]: SQL status: OK in 0.003 seconds
[0m15:31:41.275342 [debug] [Thread-1 (]: On test.quant_features.not_null_alpha_base_data_volume.21044032e6: Close
[0m15:31:41.276339 [debug] [Thread-4 (]: On test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd: Close
[0m15:31:41.277182 [debug] [Thread-3 (]: On test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974: Close
[0m15:31:41.277973 [debug] [Thread-2 (]: Finished running node test.quant_features.not_null_alpha_base_data_close.6db60d82aa
[0m15:31:41.278747 [debug] [Thread-4 (]: Finished running node test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd
[0m15:31:41.279315 [debug] [Thread-3 (]: Finished running node test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974
[0m15:31:41.279900 [debug] [Thread-1 (]: Finished running node test.quant_features.not_null_alpha_base_data_volume.21044032e6
[0m15:31:41.280227 [debug] [Thread-2 (]: Began running node test.quant_features.not_null_alpha_base_data_vwap.380f9f922d
[0m15:31:41.281429 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.quant_features.not_null_alpha_base_data_close.6db60d82aa, now test.quant_features.not_null_alpha_base_data_vwap.380f9f922d)
[0m15:31:41.281714 [debug] [Thread-2 (]: Began compiling node test.quant_features.not_null_alpha_base_data_vwap.380f9f922d
[0m15:31:41.284093 [debug] [Thread-2 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_vwap.380f9f922d"
[0m15:31:41.284536 [debug] [Thread-2 (]: Began executing node test.quant_features.not_null_alpha_base_data_vwap.380f9f922d
[0m15:31:41.286615 [debug] [Thread-2 (]: Using duckdb connection "test.quant_features.not_null_alpha_base_data_vwap.380f9f922d"
[0m15:31:41.286883 [debug] [Thread-2 (]: On test.quant_features.not_null_alpha_base_data_vwap.380f9f922d: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "test.quant_features.not_null_alpha_base_data_vwap.380f9f922d"} */

  
  
    
    



select vwap
from "quant_features"."main"."alpha_base_data"
where vwap is null



  
  limit 3

[0m15:31:41.287083 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:31:41.287695 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:31:41.288485 [debug] [Thread-2 (]: On test.quant_features.not_null_alpha_base_data_vwap.380f9f922d: Close
[0m15:31:41.288979 [debug] [Thread-2 (]: Finished running node test.quant_features.not_null_alpha_base_data_vwap.380f9f922d
[0m15:31:41.289583 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:31:41.289979 [debug] [MainThread]: Connection 'test.quant_features.not_null_alpha_base_data_volume.21044032e6' was properly closed.
[0m15:31:41.290190 [debug] [MainThread]: Connection 'test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974' was properly closed.
[0m15:31:41.290348 [debug] [MainThread]: Connection 'test.quant_features.not_null_alpha_base_data_vwap.380f9f922d' was properly closed.
[0m15:31:41.290482 [debug] [MainThread]: Connection 'test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd' was properly closed.
[0m15:31:41.290735 [error] [MainThread]: Encountered an error:
Runtime Error
  Compilation Error in test dbt_utils_accepted_range_alpha_base_data_vwap__0 (models/alpha101/schema.yml)
    'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:31:41.291300 [debug] [MainThread]: Resource report: {"command_name": "show", "command_success": false, "command_wall_clock_time": 0.6760595, "process_in_blocks": "0", "process_kernel_time": 0.164336, "process_mem_max_rss": "179924", "process_out_blocks": "2000", "process_user_time": 1.418995}
[0m15:31:41.291591 [debug] [MainThread]: Command `dbt show` failed at 15:31:41.291529 after 0.68 seconds
[0m15:31:41.291810 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d7b4d7110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d76e29a70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d76e29ff0>]}
[0m15:31:41.292033 [debug] [MainThread]: Flushing usage events
[0m15:31:41.342101 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:31:51.518486 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f81db027770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f81dc65da90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f81d9e5fd90>]}


============================== 15:31:51.520953 | 3b4ebca7-5a17-4210-bf3f-4ef58d6c49bb ==============================
[0m15:31:51.520953 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:31:51.521282 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'empty': 'None', 'profiles_dir': '/workspace/dbt_project', 'invocation_command': 'dbt deps', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/workspace/dbt_project/logs', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_cache_events': 'False', 'warn_error': 'None', 'version_check': 'True', 'log_format': 'default', 'fail_fast': 'False', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'no_print': 'None', 'debug': 'False', 'printer_width': '80', 'write_json': 'True', 'use_colors': 'True'}
[0m15:31:51.601081 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3b4ebca7-5a17-4210-bf3f-4ef58d6c49bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f81dabae650>]}
[0m15:31:51.609103 [debug] [MainThread]: Set downloads directory='/tmp/dbt-downloads-62g_k9c0'
[0m15:31:51.609462 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m15:31:51.660318 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m15:31:51.661312 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m15:31:51.695738 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m15:31:51.703183 [info ] [MainThread]: Updating lock file in file path: /workspace/dbt_project/package-lock.yml
[0m15:31:51.704653 [debug] [MainThread]: Set downloads directory='/tmp/dbt-downloads-i2i6nkzo'
[0m15:31:51.706394 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m15:31:51.753742 [info ] [MainThread]: Installed from version 1.1.1
[0m15:31:51.753993 [info ] [MainThread]: Updated version available: 1.3.0
[0m15:31:51.754202 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '3b4ebca7-5a17-4210-bf3f-4ef58d6c49bb', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f81d9f62250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f81d9f61bf0>]}
[0m15:31:51.754412 [info ] [MainThread]: 
[0m15:31:51.754578 [info ] [MainThread]: Updates available for packages: ['dbt-labs/dbt_utils']                 
Update your versions in packages.yml, then run dbt deps
[0m15:31:51.755267 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 0.2728091, "process_in_blocks": "0", "process_kernel_time": 0.111867, "process_mem_max_rss": "104552", "process_out_blocks": "2224", "process_user_time": 0.950863}
[0m15:31:51.755575 [debug] [MainThread]: Command `dbt deps` succeeded at 15:31:51.755514 after 0.27 seconds
[0m15:31:51.755778 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f81d9c4d250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f81da68f980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f81d9ddc230>]}
[0m15:31:51.755991 [debug] [MainThread]: Flushing usage events
[0m15:31:51.796724 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:32:09.993871 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3aa0953770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3aa1f85a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a9f783d90>]}


============================== 15:32:09.996378 | dbdad144-d48d-41ca-a851-3e11f6a68c1e ==============================
[0m15:32:09.996378 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:32:09.996751 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'warn_error': 'None', 'no_print': 'None', 'profiles_dir': '/workspace/dbt_project', 'introspect': 'True', 'version_check': 'True', 'fail_fast': 'False', 'indirect_selection': 'eager', 'use_colors': 'True', 'log_path': '/workspace/dbt_project/logs', 'debug': 'False', 'log_format': 'default', 'quiet': 'False', 'static_parser': 'True', 'invocation_command': 'dbt run --select feast_features', 'log_cache_events': 'False', 'cache_selected_only': 'False', 'target_path': 'None', 'empty': 'False', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'write_json': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80'}
[0m15:32:10.143920 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'dbdad144-d48d-41ca-a851-3e11f6a68c1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3aa063f360>]}
[0m15:32:10.186853 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'dbdad144-d48d-41ca-a851-3e11f6a68c1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a9f882be0>]}
[0m15:32:10.187899 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:32:10.219853 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:32:10.300075 [info ] [MainThread]: Unable to do partial parsing because a project dependency has been added
[0m15:32:10.300428 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'dbdad144-d48d-41ca-a851-3e11f6a68c1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a9f84f850>]}
[0m15:32:11.396575 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `accepted_values`. Arguments to generic tests
should be nested under the `arguments` property.`
[0m15:32:11.396890 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'dbdad144-d48d-41ca-a851-3e11f6a68c1e', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a9b4d34d0>]}
[0m15:32:11.627087 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'dbdad144-d48d-41ca-a851-3e11f6a68c1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a9a80c210>]}
[0m15:32:11.697554 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:32:11.698745 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:32:11.709339 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'dbdad144-d48d-41ca-a851-3e11f6a68c1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a9a6553d0>]}
[0m15:32:11.709720 [info ] [MainThread]: Found 17 models, 2 seeds, 37 data tests, 1 source, 679 macros
[0m15:32:11.709967 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dbdad144-d48d-41ca-a851-3e11f6a68c1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a9a7622d0>]}
[0m15:32:11.711217 [info ] [MainThread]: 
[0m15:32:11.711479 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:32:11.711658 [info ] [MainThread]: 
[0m15:32:11.711949 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:32:11.712743 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features'
[0m15:32:11.728483 [debug] [ThreadPool]: Using duckdb connection "list_quant_features"
[0m15:32:11.728778 [debug] [ThreadPool]: On list_quant_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"quant_features"'
    
  
  
[0m15:32:11.728980 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:32:11.742378 [debug] [ThreadPool]: SQL status: OK in 0.013 seconds
[0m15:32:11.743268 [debug] [ThreadPool]: On list_quant_features: Close
[0m15:32:11.743830 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_quant_features, now create_quant_features_main)
[0m15:32:11.744141 [debug] [ThreadPool]: Creating schema "database: "quant_features"
schema: "main"
"
[0m15:32:11.748414 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:32:11.748671 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='quant_features'
        and type='sqlite'
    
  
[0m15:32:11.748844 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:32:11.749549 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:32:11.750463 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:32:11.750673 [debug] [ThreadPool]: On create_quant_features_main: BEGIN
[0m15:32:11.750981 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:32:11.751148 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:32:11.751313 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
    
        create schema if not exists "quant_features"."main"
    
[0m15:32:11.751629 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:32:11.752163 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:32:11.752382 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:32:11.752550 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:32:11.752827 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:32:11.753000 [debug] [ThreadPool]: On create_quant_features_main: Close
[0m15:32:11.757659 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_quant_features_main, now list_quant_features_main)
[0m15:32:11.761724 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:32:11.762036 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:32:11.762220 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:32:11.762603 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:32:11.762785 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:32:11.762947 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:32:11.769404 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m15:32:11.770348 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:32:11.770957 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:32:11.771163 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:32:11.772804 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dbdad144-d48d-41ca-a851-3e11f6a68c1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a9a6d0050>]}
[0m15:32:11.773105 [debug] [MainThread]: Using duckdb connection "master"
[0m15:32:11.773282 [debug] [MainThread]: On master: BEGIN
[0m15:32:11.773437 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:32:11.773810 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:32:11.773994 [debug] [MainThread]: On master: COMMIT
[0m15:32:11.774143 [debug] [MainThread]: Using duckdb connection "master"
[0m15:32:11.774295 [debug] [MainThread]: On master: COMMIT
[0m15:32:11.774540 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:32:11.774703 [debug] [MainThread]: On master: Close
[0m15:32:11.776505 [debug] [Thread-1 (]: Began running node model.quant_features.feast_features
[0m15:32:11.776883 [info ] [Thread-1 (]: 1 of 1 START sql table model main.feast_features ............................... [RUN]
[0m15:32:11.777168 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.feast_features)
[0m15:32:11.777386 [debug] [Thread-1 (]: Began compiling node model.quant_features.feast_features
[0m15:32:11.782166 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.feast_features"
[0m15:32:11.782606 [debug] [Thread-1 (]: Began executing node model.quant_features.feast_features
[0m15:32:11.803102 [debug] [Thread-1 (]: Writing runtime sql for node "model.quant_features.feast_features"
[0m15:32:11.803589 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.feast_features"
[0m15:32:11.803807 [debug] [Thread-1 (]: On model.quant_features.feast_features: BEGIN
[0m15:32:11.803987 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:32:11.804532 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:32:11.804786 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.feast_features"
[0m15:32:11.805030 [debug] [Thread-1 (]: On model.quant_features.feast_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.feast_features"} */

  
    
    

    create  table
      "quant_features"."main"."feast_features__dbt_tmp"
  
    as (
      

-- Feast 特征表
-- 整合所有特征用于 Feast 消费

WITH base_features AS (
    SELECT 
        symbol,
        timestamp as event_timestamp,
        close,
        volume,
        returns,
        close_ma5,
        close_ma10,
        close_ma20,
        volume_ma20,
        returns_std20,
        adv20,
        adv10,
        adv5,
        close_lag1,
        close_lag5,
        volume_lag1,
        close_delta1,
        close_delta5,
        volume_delta1
    FROM "quant_features"."main"."alpha_base_data"
    WHERE close_ma20 IS NOT NULL
      AND returns_std20 IS NOT NULL
      AND adv20 IS NOT NULL
),

technical_features AS (
    SELECT 
        symbol,
        event_timestamp,
        close,
        volume,
        
        -- 价格特征
        close_ma5 / NULLIF(close_ma20, 0) - 1 as price_momentum_5_20,
        close_ma10 / NULLIF(close_ma20, 0) - 1 as price_momentum_10_20,
        (close - close_lag5) / NULLIF(close_lag5, 0) as price_return_5d,
        close_delta1 / NULLIF(close_lag1, 0) as price_return_1d,
        
        -- 成交量特征
        volume / NULLIF(volume_ma20, 0) - 1 as volume_ratio_20d,
        volume / NULLIF(adv20, 0) - 1 as volume_ratio_adv20,
        (volume - volume_lag1) / NULLIF(volume_lag1, 0) as volume_change_1d,
        
        -- 波动率特征
        returns_std20 as volatility_20d,
        returns / NULLIF(returns_std20, 0) as risk_adjusted_return,
        
        -- 排序特征
        PERCENT_RANK() OVER (PARTITION BY event_timestamp ORDER BY close) as price_rank,
        PERCENT_RANK() OVER (PARTITION BY event_timestamp ORDER BY volume) as volume_rank,
        PERCENT_RANK() OVER (PARTITION BY event_timestamp ORDER BY returns) as return_rank,
        
        -- 原始数据
        returns,
        close_ma5,
        close_ma10,
        close_ma20,
        volume_ma20,
        returns_std20,
        adv20
        
    FROM base_features
)

SELECT * FROM technical_features
    );
  
  
[0m15:32:11.813115 [debug] [Thread-1 (]: SQL status: OK in 0.008 seconds
[0m15:32:11.818763 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.feast_features"
[0m15:32:11.819041 [debug] [Thread-1 (]: On model.quant_features.feast_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.feast_features"} */
alter table "quant_features"."main"."feast_features__dbt_tmp" rename to "feast_features"
[0m15:32:11.819624 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:32:11.827916 [debug] [Thread-1 (]: On model.quant_features.feast_features: COMMIT
[0m15:32:11.828239 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.feast_features"
[0m15:32:11.828461 [debug] [Thread-1 (]: On model.quant_features.feast_features: COMMIT
[0m15:32:11.831047 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:32:11.834845 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.feast_features"
[0m15:32:11.835122 [debug] [Thread-1 (]: On model.quant_features.feast_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.feast_features"} */

      drop table if exists "quant_features"."main"."feast_features__dbt_backup" cascade
    
[0m15:32:11.835777 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:32:11.837516 [debug] [Thread-1 (]: On model.quant_features.feast_features: Close
[0m15:32:11.839041 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'dbdad144-d48d-41ca-a851-3e11f6a68c1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3aa0490b90>]}
[0m15:32:11.839505 [info ] [Thread-1 (]: 1 of 1 OK created sql table model main.feast_features .......................... [[32mOK[0m in 0.06s]
[0m15:32:11.839874 [debug] [Thread-1 (]: Finished running node model.quant_features.feast_features
[0m15:32:11.841876 [debug] [MainThread]: Using duckdb connection "master"
[0m15:32:11.842131 [debug] [MainThread]: On master: BEGIN
[0m15:32:11.842308 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:32:11.842673 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:32:11.842855 [debug] [MainThread]: On master: COMMIT
[0m15:32:11.843012 [debug] [MainThread]: Using duckdb connection "master"
[0m15:32:11.843155 [debug] [MainThread]: On master: COMMIT
[0m15:32:11.843466 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:32:11.843651 [debug] [MainThread]: On master: Close
[0m15:32:11.843919 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:32:11.844077 [debug] [MainThread]: Connection 'model.quant_features.feast_features' was properly closed.
[0m15:32:11.844259 [info ] [MainThread]: 
[0m15:32:11.844450 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.13 seconds (0.13s).
[0m15:32:11.844815 [debug] [MainThread]: Command end result
[0m15:32:11.870059 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:32:11.871266 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:32:11.874820 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:32:11.875056 [info ] [MainThread]: 
[0m15:32:11.875293 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:32:11.875464 [info ] [MainThread]: 
[0m15:32:11.875639 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m15:32:11.875936 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 12 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m15:32:11.876582 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 1.9192353, "process_in_blocks": "0", "process_kernel_time": 0.168249, "process_mem_max_rss": "163936", "process_out_blocks": "5704", "process_user_time": 2.631843}
[0m15:32:11.876892 [debug] [MainThread]: Command `dbt run` succeeded at 15:32:11.876831 after 1.92 seconds
[0m15:32:11.877108 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a9a9bc1d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3aa04960d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a9f7a9e10>]}
[0m15:32:11.877334 [debug] [MainThread]: Flushing usage events
[0m15:32:11.927545 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:32:18.802206 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f08871b7770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0888815a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f088602fd90>]}


============================== 15:32:18.804563 | be6ddc75-4d24-46f5-b53b-2ccb3d947e75 ==============================
[0m15:32:18.804563 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:32:18.804885 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'static_parser': 'True', 'target_path': 'None', 'write_json': 'True', 'introspect': 'True', 'send_anonymous_usage_stats': 'True', 'printer_width': '80', 'empty': 'None', 'fail_fast': 'False', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'profiles_dir': '/workspace/dbt_project', 'log_format': 'default', 'invocation_command': 'dbt show --select feast_features --limit 5', 'cache_selected_only': 'False', 'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'log_path': '/workspace/dbt_project/logs', 'version_check': 'True', 'no_print': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False'}
[0m15:32:18.949877 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'be6ddc75-4d24-46f5-b53b-2ccb3d947e75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0886eb3360>]}
[0m15:32:18.992098 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'be6ddc75-4d24-46f5-b53b-2ccb3d947e75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f088612abe0>]}
[0m15:32:19.000222 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:32:19.033145 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:32:19.123264 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:32:19.123509 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:32:19.160503 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'be6ddc75-4d24-46f5-b53b-2ccb3d947e75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0881fcd150>]}
[0m15:32:19.235339 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:32:19.236535 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:32:19.242095 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'be6ddc75-4d24-46f5-b53b-2ccb3d947e75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0881fc19a0>]}
[0m15:32:19.242447 [info ] [MainThread]: Found 17 models, 2 seeds, 37 data tests, 1 source, 679 macros
[0m15:32:19.242685 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'be6ddc75-4d24-46f5-b53b-2ccb3d947e75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0882044050>]}
[0m15:32:19.243892 [info ] [MainThread]: 
[0m15:32:19.244143 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:32:19.244324 [info ] [MainThread]: 
[0m15:32:19.244633 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:32:19.248474 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features_main'
[0m15:32:19.294930 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:32:19.295181 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:32:19.295365 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:32:19.307278 [debug] [ThreadPool]: SQL status: OK in 0.012 seconds
[0m15:32:19.307542 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:32:19.307729 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:32:19.314748 [debug] [ThreadPool]: SQL status: OK in 0.007 seconds
[0m15:32:19.315776 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:32:19.316405 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:32:19.316626 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:32:19.318325 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'be6ddc75-4d24-46f5-b53b-2ccb3d947e75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0881bc35f0>]}
[0m15:32:19.320452 [debug] [Thread-1 (]: Began running node model.quant_features.feast_features
[0m15:32:19.320797 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.feast_features)
[0m15:32:19.321018 [debug] [Thread-1 (]: Began compiling node model.quant_features.feast_features
[0m15:32:19.326147 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.feast_features"
[0m15:32:19.326590 [debug] [Thread-1 (]: Began executing node model.quant_features.feast_features
[0m15:32:19.331029 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.feast_features"
[0m15:32:19.331416 [debug] [Thread-1 (]: On model.quant_features.feast_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.feast_features"} */

  
  

-- Feast 特征表
-- 整合所有特征用于 Feast 消费

WITH base_features AS (
    SELECT 
        symbol,
        timestamp as event_timestamp,
        close,
        volume,
        returns,
        close_ma5,
        close_ma10,
        close_ma20,
        volume_ma20,
        returns_std20,
        adv20,
        adv10,
        adv5,
        close_lag1,
        close_lag5,
        volume_lag1,
        close_delta1,
        close_delta5,
        volume_delta1
    FROM "quant_features"."main"."alpha_base_data"
    WHERE close_ma20 IS NOT NULL
      AND returns_std20 IS NOT NULL
      AND adv20 IS NOT NULL
),

technical_features AS (
    SELECT 
        symbol,
        event_timestamp,
        close,
        volume,
        
        -- 价格特征
        close_ma5 / NULLIF(close_ma20, 0) - 1 as price_momentum_5_20,
        close_ma10 / NULLIF(close_ma20, 0) - 1 as price_momentum_10_20,
        (close - close_lag5) / NULLIF(close_lag5, 0) as price_return_5d,
        close_delta1 / NULLIF(close_lag1, 0) as price_return_1d,
        
        -- 成交量特征
        volume / NULLIF(volume_ma20, 0) - 1 as volume_ratio_20d,
        volume / NULLIF(adv20, 0) - 1 as volume_ratio_adv20,
        (volume - volume_lag1) / NULLIF(volume_lag1, 0) as volume_change_1d,
        
        -- 波动率特征
        returns_std20 as volatility_20d,
        returns / NULLIF(returns_std20, 0) as risk_adjusted_return,
        
        -- 排序特征
        PERCENT_RANK() OVER (PARTITION BY event_timestamp ORDER BY close) as price_rank,
        PERCENT_RANK() OVER (PARTITION BY event_timestamp ORDER BY volume) as volume_rank,
        PERCENT_RANK() OVER (PARTITION BY event_timestamp ORDER BY returns) as return_rank,
        
        -- 原始数据
        returns,
        close_ma5,
        close_ma10,
        close_ma20,
        volume_ma20,
        returns_std20,
        adv20
        
    FROM base_features
)

SELECT * FROM technical_features
  
  limit 5

[0m15:32:19.331678 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:32:19.336637 [debug] [Thread-1 (]: SQL status: OK in 0.005 seconds
[0m15:32:19.338760 [debug] [Thread-1 (]: On model.quant_features.feast_features: Close
[0m15:32:19.339488 [debug] [Thread-1 (]: Finished running node model.quant_features.feast_features
[0m15:32:19.340510 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:32:19.340752 [debug] [MainThread]: Connection 'model.quant_features.feast_features' was properly closed.
[0m15:32:19.341143 [debug] [MainThread]: Command end result
[0m15:32:19.367627 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:32:19.368819 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:32:19.372959 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:32:19.373764 [info ] [MainThread]: Previewing node 'feast_features':
| symbol | event_timestamp |   close |  volume | price_momentum_5_20 | price_momentum_10_20 | ... |
| ------ | --------------- | ------- | ------- | ------------------- | -------------------- | --- |
| MSFT   |      2024-01-10 |   396.2 |  975000 |              0.008… |                    0 | ... |
| AAPL   |      2024-01-10 |   158.2 | 1450000 |              0.010… |                    0 | ... |
| GOOGL  |      2024-01-10 | 2,935.0 | 1150000 |              0.008… |                    0 | ... |
| TSLA   |      2024-01-10 |   269.9 | 2700000 |              0.014… |                    0 | ... |
| MSFT   |      2024-01-08 |   392.6 |  925000 |              0.003… |                    0 | ... |

[0m15:32:19.374400 [debug] [MainThread]: Resource report: {"command_name": "show", "command_success": true, "command_wall_clock_time": 0.60872763, "process_in_blocks": "0", "process_kernel_time": 0.127668, "process_mem_max_rss": "152776", "process_out_blocks": "3984", "process_user_time": 1.356463}
[0m15:32:19.374703 [debug] [MainThread]: Command `dbt show` succeeded at 15:32:19.374642 after 0.61 seconds
[0m15:32:19.374922 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f08863c7110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0881bc5f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0881bc5ff0>]}
[0m15:32:19.375137 [debug] [MainThread]: Flushing usage events
[0m15:32:19.437792 [debug] [MainThread]: An error was encountered while trying to flush usage events
