[0m15:26:19.841838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97fc323770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97fd985a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97fb193d90>]}


============================== 15:26:19.844214 | 2f9e1454-b404-41fc-b1e0-45b93a99f7c8 ==============================
[0m15:26:19.844214 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:26:19.844552 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'static_parser': 'True', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'quiet': 'False', 'log_format': 'default', 'log_cache_events': 'False', 'profiles_dir': '/workspace/dbt_project', 'cache_selected_only': 'False', 'use_colors': 'True', 'target_path': 'None', 'introspect': 'True', 'write_json': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'warn_error': 'None', 'partial_parse': 'True', 'log_path': '/workspace/dbt_project/logs', 'invocation_command': 'dbt seed', 'indirect_selection': 'eager', 'empty': 'None', 'version_check': 'True', 'fail_fast': 'False', 'no_print': 'None'}
[0m15:26:19.978648 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2f9e1454-b404-41fc-b1e0-45b93a99f7c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97fbeaa650>]}
[0m15:26:20.020083 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2f9e1454-b404-41fc-b1e0-45b93a99f7c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97fb28ebe0>]}
[0m15:26:20.021116 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:26:20.049913 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:26:20.050328 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m15:26:20.050536 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '2f9e1454-b404-41fc-b1e0-45b93a99f7c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97f7aa4e50>]}
[0m15:26:21.075943 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `accepted_values`. Arguments to generic tests
should be nested under the `arguments` property.`
[0m15:26:21.076257 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '2f9e1454-b404-41fc-b1e0-45b93a99f7c8', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97f6e45220>]}
[0m15:26:21.279041 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2f9e1454-b404-41fc-b1e0-45b93a99f7c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97f617dd30>]}
[0m15:26:21.344765 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:26:21.345874 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:26:21.356022 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2f9e1454-b404-41fc-b1e0-45b93a99f7c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97f62f8ae0>]}
[0m15:26:21.356352 [info ] [MainThread]: Found 15 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:26:21.356587 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2f9e1454-b404-41fc-b1e0-45b93a99f7c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97f6dd3710>]}
[0m15:26:21.358081 [info ] [MainThread]: 
[0m15:26:21.358347 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:26:21.358517 [info ] [MainThread]: 
[0m15:26:21.358797 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:26:21.362031 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features'
[0m15:26:21.376764 [debug] [ThreadPool]: Using duckdb connection "list_quant_features"
[0m15:26:21.377026 [debug] [ThreadPool]: On list_quant_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"quant_features"'
    
  
  
[0m15:26:21.377203 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:26:21.393160 [debug] [ThreadPool]: SQL status: OK in 0.016 seconds
[0m15:26:21.394033 [debug] [ThreadPool]: On list_quant_features: Close
[0m15:26:21.394722 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_quant_features, now create_quant_features_main)
[0m15:26:21.395039 [debug] [ThreadPool]: Creating schema "database: "quant_features"
schema: "main"
"
[0m15:26:21.399138 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:26:21.399391 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='quant_features'
        and type='sqlite'
    
  
[0m15:26:21.399568 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:26:21.400251 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:26:21.401105 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:26:21.401310 [debug] [ThreadPool]: On create_quant_features_main: BEGIN
[0m15:26:21.401604 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:26:21.401769 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:26:21.401926 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
    
        create schema if not exists "quant_features"."main"
    
[0m15:26:21.402211 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:26:21.402722 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:26:21.402913 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:26:21.403093 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:26:21.403425 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:26:21.403604 [debug] [ThreadPool]: On create_quant_features_main: Close
[0m15:26:21.405348 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_quant_features_main, now list_quant_features_main)
[0m15:26:21.409211 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:26:21.409448 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:26:21.409603 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:26:21.410052 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:26:21.410245 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:26:21.410402 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:26:21.416486 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m15:26:21.417267 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:26:21.417847 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:26:21.418070 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:26:21.418894 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2f9e1454-b404-41fc-b1e0-45b93a99f7c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97f6067c20>]}
[0m15:26:21.419398 [debug] [MainThread]: Using duckdb connection "master"
[0m15:26:21.419593 [debug] [MainThread]: On master: BEGIN
[0m15:26:21.419747 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:26:21.420175 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:26:21.420379 [debug] [MainThread]: On master: COMMIT
[0m15:26:21.420533 [debug] [MainThread]: Using duckdb connection "master"
[0m15:26:21.420677 [debug] [MainThread]: On master: COMMIT
[0m15:26:21.420938 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:26:21.421125 [debug] [MainThread]: On master: Close
[0m15:26:21.423527 [debug] [Thread-1 (]: Began running node seed.quant_features.market_data
[0m15:26:21.423932 [debug] [Thread-2 (]: Began running node seed.quant_features.raw_stock_prices
[0m15:26:21.424437 [info ] [Thread-1 (]: 1 of 2 START seed file main.market_data ........................................ [RUN]
[0m15:26:21.424950 [info ] [Thread-2 (]: 2 of 2 START seed file main.raw_stock_prices ................................... [RUN]
[0m15:26:21.425467 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now seed.quant_features.market_data)
[0m15:26:21.426034 [debug] [Thread-2 (]: Acquiring new duckdb connection 'seed.quant_features.raw_stock_prices'
[0m15:26:21.426564 [debug] [Thread-1 (]: Began compiling node seed.quant_features.market_data
[0m15:26:21.426921 [debug] [Thread-2 (]: Began compiling node seed.quant_features.raw_stock_prices
[0m15:26:21.427275 [debug] [Thread-1 (]: Began executing node seed.quant_features.market_data
[0m15:26:21.427579 [debug] [Thread-2 (]: Began executing node seed.quant_features.raw_stock_prices
[0m15:26:21.447246 [debug] [Thread-1 (]: Using duckdb connection "seed.quant_features.market_data"
[0m15:26:21.447517 [debug] [Thread-1 (]: On seed.quant_features.market_data: BEGIN
[0m15:26:21.447701 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:26:21.449650 [debug] [Thread-2 (]: Using duckdb connection "seed.quant_features.raw_stock_prices"
[0m15:26:21.450009 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:26:21.450385 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: BEGIN
[0m15:26:21.450734 [debug] [Thread-1 (]: Using duckdb connection "seed.quant_features.market_data"
[0m15:26:21.451037 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:26:21.451302 [debug] [Thread-1 (]: On seed.quant_features.market_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "seed.quant_features.market_data"} */

    create table "quant_features"."main"."market_data" ("date" date,"symbol" text,"market_cap" integer,"pe_ratio" float8,"dividend_yield" float8,"sector" text)
  
[0m15:26:21.451917 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:26:21.452172 [debug] [Thread-2 (]: Using duckdb connection "seed.quant_features.raw_stock_prices"
[0m15:26:21.452421 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:26:21.452676 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "seed.quant_features.raw_stock_prices"} */

    create table "quant_features"."main"."raw_stock_prices" ("date" date,"symbol" text,"open" float8,"high" float8,"low" float8,"close" float8,"volume" integer)
  
[0m15:26:21.459497 [debug] [Thread-1 (]: Using duckdb connection "seed.quant_features.market_data"
[0m15:26:21.459940 [debug] [Thread-1 (]: On seed.quant_features.market_data: 
          COPY "quant_features"."main"."market_data" FROM '/workspace/dbt_project/seeds/market_data.csv' (FORMAT CSV, HEADER TRUE, DELIMITER ',')
        ...
[0m15:26:21.460315 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:26:21.461158 [debug] [Thread-2 (]: Using duckdb connection "seed.quant_features.raw_stock_prices"
[0m15:26:21.461381 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: 
          COPY "quant_features"."main"."raw_stock_prices" FROM '/workspace/dbt_project/seeds/raw_stock_prices.csv' (FORMAT CSV, HEADER TRUE, DELIMITER ',')
        ...
[0m15:26:21.464417 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m15:26:21.464849 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: 
          COPY "quant_features"."main"."market_data" FROM '/workspace/dbt_project/seeds/market_data.csv' (FORMAT CSV, HEADER TRUE, DELIMITER ',')
        
[0m15:26:21.468535 [debug] [Thread-2 (]: Writing runtime SQL for node "seed.quant_features.raw_stock_prices"
[0m15:26:21.468882 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m15:26:21.469762 [debug] [Thread-1 (]: On seed.quant_features.market_data: ROLLBACK
[0m15:26:21.479561 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: COMMIT
[0m15:26:21.479845 [debug] [Thread-2 (]: Using duckdb connection "seed.quant_features.raw_stock_prices"
[0m15:26:21.480057 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: COMMIT
[0m15:26:21.482852 [debug] [Thread-1 (]: Failed to rollback 'seed.quant_features.market_data'
[0m15:26:21.483137 [debug] [Thread-1 (]: On seed.quant_features.market_data: Close
[0m15:26:21.485823 [debug] [Thread-1 (]: Runtime Error in seed market_data (seeds/market_data.csv)
  Conversion Error: CSV Error on Line: 2
  Original Line: 2024-01-01,AAPL,3000000000000,28.5,0.44,Technology
  Error when converting column "market_cap". Could not convert string "3000000000000" to 'INTEGER'
  
  Column market_cap is being converted as type INTEGER
  This type was either manually set or derived from an existing table. Select a different type to correctly parse this column.
  * Check whether the null string value is set correctly (e.g., nullstr = 'N/A')
  
    file = /workspace/dbt_project/seeds/market_data.csv
    delimiter = , (Set By User)
    quote = (empty) (Auto-Detected)
    escape = (empty) (Auto-Detected)
    new_line = \n (Auto-Detected)
    header = true (Set By User)
    skip_rows = 0 (Auto-Detected)
    comment = (empty) (Auto-Detected)
    strict_mode = true (Auto-Detected)
    date_format =  (Auto-Detected)
    timestamp_format =  (Auto-Detected)
    null_padding = 0
    sample_size = 20480
    ignore_errors = false
    all_varchar = 0
  The Column types set by the user do not match the ones found by the sniffer. 
  Column at position: 2 Set type: INTEGER Sniffed type: BIGINT
  
  
[0m15:26:21.486875 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2f9e1454-b404-41fc-b1e0-45b93a99f7c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97fbe6cb90>]}
[0m15:26:21.487234 [debug] [Thread-2 (]: SQL status: OK in 0.007 seconds
[0m15:26:21.487697 [error] [Thread-1 (]: 1 of 2 ERROR loading seed file main.market_data ................................ [[31mERROR[0m in 0.06s]
[0m15:26:21.489035 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: Close
[0m15:26:21.489724 [debug] [Thread-1 (]: Finished running node seed.quant_features.market_data
[0m15:26:21.490259 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2f9e1454-b404-41fc-b1e0-45b93a99f7c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97f54e1550>]}
[0m15:26:21.490868 [debug] [Thread-7 (]: Marking all children of 'seed.quant_features.market_data' to be skipped because of status 'error'.  Reason: Runtime Error in seed market_data (seeds/market_data.csv)
  Conversion Error: CSV Error on Line: 2
  Original Line: 2024-01-01,AAPL,3000000000000,28.5,0.44,Technology
  Error when converting column "market_cap". Could not convert string "3000000000000" to 'INTEGER'
  
  Column market_cap is being converted as type INTEGER
  This type was either manually set or derived from an existing table. Select a different type to correctly parse this column.
  * Check whether the null string value is set correctly (e.g., nullstr = 'N/A')
  
    file = /workspace/dbt_project/seeds/market_data.csv
    delimiter = , (Set By User)
    quote = (empty) (Auto-Detected)
    escape = (empty) (Auto-Detected)
    new_line = \n (Auto-Detected)
    header = true (Set By User)
    skip_rows = 0 (Auto-Detected)
    comment = (empty) (Auto-Detected)
    strict_mode = true (Auto-Detected)
    date_format =  (Auto-Detected)
    timestamp_format =  (Auto-Detected)
    null_padding = 0
    sample_size = 20480
    ignore_errors = false
    all_varchar = 0
  The Column types set by the user do not match the ones found by the sniffer. 
  Column at position: 2 Set type: INTEGER Sniffed type: BIGINT
  
  .
[0m15:26:21.491301 [info ] [Thread-2 (]: 2 of 2 OK loaded seed file main.raw_stock_prices ............................... [[32mINSERT 32[0m in 0.06s]
[0m15:26:21.492173 [debug] [Thread-2 (]: Finished running node seed.quant_features.raw_stock_prices
[0m15:26:21.493687 [debug] [MainThread]: Using duckdb connection "master"
[0m15:26:21.494762 [debug] [MainThread]: On master: BEGIN
[0m15:26:21.494952 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:26:21.495342 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:26:21.495533 [debug] [MainThread]: On master: COMMIT
[0m15:26:21.495696 [debug] [MainThread]: Using duckdb connection "master"
[0m15:26:21.495846 [debug] [MainThread]: On master: COMMIT
[0m15:26:21.496106 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:26:21.496270 [debug] [MainThread]: On master: Close
[0m15:26:21.496549 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:26:21.496717 [debug] [MainThread]: Connection 'seed.quant_features.market_data' was properly closed.
[0m15:26:21.496859 [debug] [MainThread]: Connection 'seed.quant_features.raw_stock_prices' was properly closed.
[0m15:26:21.497042 [info ] [MainThread]: 
[0m15:26:21.497219 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 0.14 seconds (0.14s).
[0m15:26:21.497683 [debug] [MainThread]: Command end result
[0m15:26:21.519309 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:26:21.520404 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:26:21.523885 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:26:21.524122 [info ] [MainThread]: 
[0m15:26:21.524336 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:26:21.524536 [info ] [MainThread]: 
[0m15:26:21.524748 [error] [MainThread]: [31mFailure in seed market_data (seeds/market_data.csv)[0m
[0m15:26:21.524985 [error] [MainThread]:   Runtime Error in seed market_data (seeds/market_data.csv)
  Conversion Error: CSV Error on Line: 2
  Original Line: 2024-01-01,AAPL,3000000000000,28.5,0.44,Technology
  Error when converting column "market_cap". Could not convert string "3000000000000" to 'INTEGER'
  
  Column market_cap is being converted as type INTEGER
  This type was either manually set or derived from an existing table. Select a different type to correctly parse this column.
  * Check whether the null string value is set correctly (e.g., nullstr = 'N/A')
  
    file = /workspace/dbt_project/seeds/market_data.csv
    delimiter = , (Set By User)
    quote = (empty) (Auto-Detected)
    escape = (empty) (Auto-Detected)
    new_line = \n (Auto-Detected)
    header = true (Set By User)
    skip_rows = 0 (Auto-Detected)
    comment = (empty) (Auto-Detected)
    strict_mode = true (Auto-Detected)
    date_format =  (Auto-Detected)
    timestamp_format =  (Auto-Detected)
    null_padding = 0
    sample_size = 20480
    ignore_errors = false
    all_varchar = 0
  The Column types set by the user do not match the ones found by the sniffer. 
  Column at position: 2 Set type: INTEGER Sniffed type: BIGINT
  
  
[0m15:26:21.525193 [info ] [MainThread]: 
[0m15:26:21.525368 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=2
[0m15:26:21.525653 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 12 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m15:26:21.526202 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": false, "command_wall_clock_time": 1.7209142, "process_in_blocks": "0", "process_kernel_time": 0.163583, "process_mem_max_rss": "163496", "process_out_blocks": "4920", "process_user_time": 2.429802}
[0m15:26:21.526485 [debug] [MainThread]: Command `dbt seed` failed at 15:26:21.526427 after 1.72 seconds
[0m15:26:21.526698 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97fb1bdda0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97ff3a5400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97f54cd250>]}
[0m15:26:21.526905 [debug] [MainThread]: Flushing usage events
[0m15:26:21.578850 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:26:34.299965 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f8338f770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f849e5a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f821cbd90>]}


============================== 15:26:34.302378 | 5807ed42-b7c8-43f4-9682-a91126bcf7fc ==============================
[0m15:26:34.302378 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:26:34.302711 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'version_check': 'True', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'profiles_dir': '/workspace/dbt_project', 'cache_selected_only': 'False', 'debug': 'False', 'static_parser': 'True', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'use_colors': 'True', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'log_path': '/workspace/dbt_project/logs', 'introspect': 'True', 'empty': 'None', 'log_format': 'default', 'printer_width': '80', 'write_json': 'True', 'warn_error': 'None', 'invocation_command': 'dbt seed --full-refresh'}
[0m15:26:34.437089 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5807ed42-b7c8-43f4-9682-a91126bcf7fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f82f16650>]}
[0m15:26:34.478235 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5807ed42-b7c8-43f4-9682-a91126bcf7fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f822cabe0>]}
[0m15:26:34.479248 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:26:34.508354 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:26:34.595504 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m15:26:34.595967 [debug] [MainThread]: Partial parsing: added file: quant_features://seeds/schema.yml
[0m15:26:34.693065 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5807ed42-b7c8-43f4-9682-a91126bcf7fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f7defe350>]}
[0m15:26:34.788652 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:26:34.789801 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:26:34.799721 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5807ed42-b7c8-43f4-9682-a91126bcf7fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f7de654f0>]}
[0m15:26:34.800056 [info ] [MainThread]: Found 15 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:26:34.800266 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5807ed42-b7c8-43f4-9682-a91126bcf7fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f821ad2b0>]}
[0m15:26:34.801738 [info ] [MainThread]: 
[0m15:26:34.802042 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:26:34.802234 [info ] [MainThread]: 
[0m15:26:34.802544 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:26:34.805709 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features'
[0m15:26:34.820392 [debug] [ThreadPool]: Using duckdb connection "list_quant_features"
[0m15:26:34.820649 [debug] [ThreadPool]: On list_quant_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"quant_features"'
    
  
  
[0m15:26:34.820825 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:26:34.830716 [debug] [ThreadPool]: SQL status: OK in 0.010 seconds
[0m15:26:34.831607 [debug] [ThreadPool]: On list_quant_features: Close
[0m15:26:34.832166 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_quant_features, now create_quant_features_main)
[0m15:26:34.832481 [debug] [ThreadPool]: Creating schema "database: "quant_features"
schema: "main"
"
[0m15:26:34.836591 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:26:34.836847 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='quant_features'
        and type='sqlite'
    
  
[0m15:26:34.837027 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:26:34.837677 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:26:34.838519 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:26:34.838735 [debug] [ThreadPool]: On create_quant_features_main: BEGIN
[0m15:26:34.839056 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:26:34.839230 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:26:34.839386 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
    
        create schema if not exists "quant_features"."main"
    
[0m15:26:34.839678 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:26:34.840207 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:26:34.840421 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:26:34.840581 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:26:34.840864 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:26:34.841064 [debug] [ThreadPool]: On create_quant_features_main: Close
[0m15:26:34.842916 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_quant_features_main, now list_quant_features_main)
[0m15:26:34.846840 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:26:34.847096 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:26:34.847259 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:26:34.847684 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:26:34.847888 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:26:34.848055 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:26:34.854194 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m15:26:34.855054 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:26:34.855654 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:26:34.855866 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:26:34.857044 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5807ed42-b7c8-43f4-9682-a91126bcf7fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f7d158bb0>]}
[0m15:26:34.857439 [debug] [MainThread]: Using duckdb connection "master"
[0m15:26:34.857634 [debug] [MainThread]: On master: BEGIN
[0m15:26:34.857792 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:26:34.858210 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:26:34.858402 [debug] [MainThread]: On master: COMMIT
[0m15:26:34.858555 [debug] [MainThread]: Using duckdb connection "master"
[0m15:26:34.858702 [debug] [MainThread]: On master: COMMIT
[0m15:26:34.859132 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:26:34.859326 [debug] [MainThread]: On master: Close
[0m15:26:34.861375 [debug] [Thread-2 (]: Began running node seed.quant_features.raw_stock_prices
[0m15:26:34.861767 [debug] [Thread-1 (]: Began running node seed.quant_features.market_data
[0m15:26:34.862275 [info ] [Thread-2 (]: 2 of 2 START seed file main.raw_stock_prices ................................... [RUN]
[0m15:26:34.862787 [info ] [Thread-1 (]: 1 of 2 START seed file main.market_data ........................................ [RUN]
[0m15:26:34.863224 [debug] [Thread-2 (]: Acquiring new duckdb connection 'seed.quant_features.raw_stock_prices'
[0m15:26:34.863583 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now seed.quant_features.market_data)
[0m15:26:34.863867 [debug] [Thread-2 (]: Began compiling node seed.quant_features.raw_stock_prices
[0m15:26:34.864141 [debug] [Thread-1 (]: Began compiling node seed.quant_features.market_data
[0m15:26:34.864429 [debug] [Thread-2 (]: Began executing node seed.quant_features.raw_stock_prices
[0m15:26:34.864725 [debug] [Thread-1 (]: Began executing node seed.quant_features.market_data
[0m15:26:34.884816 [debug] [Thread-1 (]: Using duckdb connection "seed.quant_features.market_data"
[0m15:26:34.885102 [debug] [Thread-1 (]: On seed.quant_features.market_data: BEGIN
[0m15:26:34.885297 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:26:34.885797 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:26:34.886046 [debug] [Thread-1 (]: Using duckdb connection "seed.quant_features.market_data"
[0m15:26:34.886238 [debug] [Thread-1 (]: On seed.quant_features.market_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "seed.quant_features.market_data"} */

    create table "quant_features"."main"."market_data" ("date" date,"symbol" text,"market_cap" integer,"pe_ratio" float8,"dividend_yield" float8,"sector" text)
  
[0m15:26:34.893615 [debug] [Thread-2 (]: Using duckdb connection "seed.quant_features.raw_stock_prices"
[0m15:26:34.893915 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "seed.quant_features.raw_stock_prices"} */

      drop table if exists "quant_features"."main"."raw_stock_prices" cascade
    
[0m15:26:34.894121 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:26:34.894585 [debug] [Thread-1 (]: SQL status: OK in 0.008 seconds
[0m15:26:34.901658 [debug] [Thread-1 (]: Using duckdb connection "seed.quant_features.market_data"
[0m15:26:34.902000 [debug] [Thread-2 (]: SQL status: OK in 0.008 seconds
[0m15:26:34.902436 [debug] [Thread-1 (]: On seed.quant_features.market_data: 
          COPY "quant_features"."main"."market_data" FROM '/workspace/dbt_project/seeds/market_data.csv' (FORMAT CSV, HEADER TRUE, DELIMITER ',')
        ...
[0m15:26:34.903634 [debug] [Thread-2 (]: Using duckdb connection "seed.quant_features.raw_stock_prices"
[0m15:26:34.904045 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: BEGIN
[0m15:26:34.904459 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:26:34.904672 [debug] [Thread-2 (]: Using duckdb connection "seed.quant_features.raw_stock_prices"
[0m15:26:34.904854 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "seed.quant_features.raw_stock_prices"} */

    create table "quant_features"."main"."raw_stock_prices" ("date" date,"symbol" text,"open" float8,"high" float8,"low" float8,"close" float8,"volume" integer)
  
[0m15:26:34.905337 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:26:34.906069 [debug] [Thread-2 (]: Using duckdb connection "seed.quant_features.raw_stock_prices"
[0m15:26:34.906296 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: 
          COPY "quant_features"."main"."raw_stock_prices" FROM '/workspace/dbt_project/seeds/raw_stock_prices.csv' (FORMAT CSV, HEADER TRUE, DELIMITER ',')
        ...
[0m15:26:34.907924 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: 
          COPY "quant_features"."main"."market_data" FROM '/workspace/dbt_project/seeds/market_data.csv' (FORMAT CSV, HEADER TRUE, DELIMITER ',')
        
[0m15:26:34.908194 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m15:26:34.908470 [debug] [Thread-1 (]: On seed.quant_features.market_data: ROLLBACK
[0m15:26:34.909585 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m15:26:34.913142 [debug] [Thread-2 (]: Writing runtime SQL for node "seed.quant_features.raw_stock_prices"
[0m15:26:34.923403 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: COMMIT
[0m15:26:34.923679 [debug] [Thread-2 (]: Using duckdb connection "seed.quant_features.raw_stock_prices"
[0m15:26:34.923888 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: COMMIT
[0m15:26:34.926895 [debug] [Thread-1 (]: Failed to rollback 'seed.quant_features.market_data'
[0m15:26:34.927201 [debug] [Thread-1 (]: On seed.quant_features.market_data: Close
[0m15:26:34.927537 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m15:26:34.928996 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: Close
[0m15:26:34.930896 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5807ed42-b7c8-43f4-9682-a91126bcf7fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f7d2456d0>]}
[0m15:26:34.933030 [debug] [Thread-1 (]: Runtime Error in seed market_data (seeds/market_data.csv)
  Conversion Error: CSV Error on Line: 2
  Original Line: 2024-01-01,AAPL,3000000000000,28.5,0.44,Technology
  Error when converting column "market_cap". Could not convert string "3000000000000" to 'INTEGER'
  
  Column market_cap is being converted as type INTEGER
  This type was either manually set or derived from an existing table. Select a different type to correctly parse this column.
  * Check whether the null string value is set correctly (e.g., nullstr = 'N/A')
  
    file = /workspace/dbt_project/seeds/market_data.csv
    delimiter = , (Set By User)
    quote = (empty) (Auto-Detected)
    escape = (empty) (Auto-Detected)
    new_line = \n (Auto-Detected)
    header = true (Set By User)
    skip_rows = 0 (Auto-Detected)
    comment = (empty) (Auto-Detected)
    strict_mode = true (Auto-Detected)
    date_format =  (Auto-Detected)
    timestamp_format =  (Auto-Detected)
    null_padding = 0
    sample_size = 20480
    ignore_errors = false
    all_varchar = 0
  The Column types set by the user do not match the ones found by the sniffer. 
  Column at position: 2 Set type: INTEGER Sniffed type: BIGINT
  
  
[0m15:26:34.933618 [info ] [Thread-2 (]: 2 of 2 OK loaded seed file main.raw_stock_prices ............................... [[32mCREATE 32[0m in 0.07s]
[0m15:26:34.934143 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5807ed42-b7c8-43f4-9682-a91126bcf7fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f7dc826d0>]}
[0m15:26:34.934693 [debug] [Thread-2 (]: Finished running node seed.quant_features.raw_stock_prices
[0m15:26:34.935166 [error] [Thread-1 (]: 1 of 2 ERROR loading seed file main.market_data ................................ [[31mERROR[0m in 0.07s]
[0m15:26:34.935870 [debug] [Thread-1 (]: Finished running node seed.quant_features.market_data
[0m15:26:34.936325 [debug] [Thread-7 (]: Marking all children of 'seed.quant_features.market_data' to be skipped because of status 'error'.  Reason: Runtime Error in seed market_data (seeds/market_data.csv)
  Conversion Error: CSV Error on Line: 2
  Original Line: 2024-01-01,AAPL,3000000000000,28.5,0.44,Technology
  Error when converting column "market_cap". Could not convert string "3000000000000" to 'INTEGER'
  
  Column market_cap is being converted as type INTEGER
  This type was either manually set or derived from an existing table. Select a different type to correctly parse this column.
  * Check whether the null string value is set correctly (e.g., nullstr = 'N/A')
  
    file = /workspace/dbt_project/seeds/market_data.csv
    delimiter = , (Set By User)
    quote = (empty) (Auto-Detected)
    escape = (empty) (Auto-Detected)
    new_line = \n (Auto-Detected)
    header = true (Set By User)
    skip_rows = 0 (Auto-Detected)
    comment = (empty) (Auto-Detected)
    strict_mode = true (Auto-Detected)
    date_format =  (Auto-Detected)
    timestamp_format =  (Auto-Detected)
    null_padding = 0
    sample_size = 20480
    ignore_errors = false
    all_varchar = 0
  The Column types set by the user do not match the ones found by the sniffer. 
  Column at position: 2 Set type: INTEGER Sniffed type: BIGINT
  
  .
[0m15:26:34.938611 [debug] [MainThread]: Using duckdb connection "master"
[0m15:26:34.938860 [debug] [MainThread]: On master: BEGIN
[0m15:26:34.939043 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:26:34.939396 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:26:34.939573 [debug] [MainThread]: On master: COMMIT
[0m15:26:34.939728 [debug] [MainThread]: Using duckdb connection "master"
[0m15:26:34.939872 [debug] [MainThread]: On master: COMMIT
[0m15:26:34.940188 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:26:34.940387 [debug] [MainThread]: On master: Close
[0m15:26:34.940661 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:26:34.940823 [debug] [MainThread]: Connection 'seed.quant_features.market_data' was properly closed.
[0m15:26:34.940971 [debug] [MainThread]: Connection 'seed.quant_features.raw_stock_prices' was properly closed.
[0m15:26:34.941140 [info ] [MainThread]: 
[0m15:26:34.941310 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 0.14 seconds (0.14s).
[0m15:26:34.941776 [debug] [MainThread]: Command end result
[0m15:26:34.963683 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:26:34.964831 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:26:34.968372 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:26:34.968600 [info ] [MainThread]: 
[0m15:26:34.968811 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:26:34.968994 [info ] [MainThread]: 
[0m15:26:34.969204 [error] [MainThread]: [31mFailure in seed market_data (seeds/market_data.csv)[0m
[0m15:26:34.969418 [error] [MainThread]:   Runtime Error in seed market_data (seeds/market_data.csv)
  Conversion Error: CSV Error on Line: 2
  Original Line: 2024-01-01,AAPL,3000000000000,28.5,0.44,Technology
  Error when converting column "market_cap". Could not convert string "3000000000000" to 'INTEGER'
  
  Column market_cap is being converted as type INTEGER
  This type was either manually set or derived from an existing table. Select a different type to correctly parse this column.
  * Check whether the null string value is set correctly (e.g., nullstr = 'N/A')
  
    file = /workspace/dbt_project/seeds/market_data.csv
    delimiter = , (Set By User)
    quote = (empty) (Auto-Detected)
    escape = (empty) (Auto-Detected)
    new_line = \n (Auto-Detected)
    header = true (Set By User)
    skip_rows = 0 (Auto-Detected)
    comment = (empty) (Auto-Detected)
    strict_mode = true (Auto-Detected)
    date_format =  (Auto-Detected)
    timestamp_format =  (Auto-Detected)
    null_padding = 0
    sample_size = 20480
    ignore_errors = false
    all_varchar = 0
  The Column types set by the user do not match the ones found by the sniffer. 
  Column at position: 2 Set type: INTEGER Sniffed type: BIGINT
  
  
[0m15:26:34.969605 [info ] [MainThread]: 
[0m15:26:34.969771 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=2
[0m15:26:34.970294 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": false, "command_wall_clock_time": 0.70694566, "process_in_blocks": "0", "process_kernel_time": 0.163698, "process_mem_max_rss": "155708", "process_out_blocks": "4920", "process_user_time": 1.441296}
[0m15:26:34.970569 [debug] [MainThread]: Command `dbt seed` failed at 15:26:34.970513 after 0.71 seconds
[0m15:26:34.970771 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f7d2e7a70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f7ddcc9e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f7eb368d0>]}
[0m15:26:34.970979 [debug] [MainThread]: Flushing usage events
[0m15:26:34.998342 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:26:48.899990 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facd9667770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facdacbda90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facd849fd90>]}


============================== 15:26:48.902308 | eb21f7a4-8bd1-4ba5-9b26-576fcbe93fb2 ==============================
[0m15:26:48.902308 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:26:48.902635 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/workspace/dbt_project', 'target_path': 'None', 'log_path': '/workspace/dbt_project/logs', 'warn_error': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_cache_events': 'False', 'introspect': 'True', 'debug': 'False', 'use_experimental_parser': 'False', 'invocation_command': 'dbt seed --full-refresh', 'use_colors': 'True', 'empty': 'None', 'write_json': 'True', 'fail_fast': 'False', 'version_check': 'True', 'log_format': 'default', 'partial_parse': 'True', 'printer_width': '80', 'no_print': 'None', 'indirect_selection': 'eager'}
[0m15:26:49.042437 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'eb21f7a4-8bd1-4ba5-9b26-576fcbe93fb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facd91ee650>]}
[0m15:26:49.083719 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'eb21f7a4-8bd1-4ba5-9b26-576fcbe93fb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facd859ebe0>]}
[0m15:26:49.084724 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:26:49.113876 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:26:49.192220 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m15:26:49.192527 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'eb21f7a4-8bd1-4ba5-9b26-576fcbe93fb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facd856f550>]}
[0m15:26:50.181177 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `accepted_values`. Arguments to generic tests
should be nested under the `arguments` property.`
[0m15:26:50.181490 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'eb21f7a4-8bd1-4ba5-9b26-576fcbe93fb2', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facd373f980>]}
[0m15:26:50.390067 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'eb21f7a4-8bd1-4ba5-9b26-576fcbe93fb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facd34fd8d0>]}
[0m15:26:50.453451 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:26:50.454641 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:26:50.465155 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'eb21f7a4-8bd1-4ba5-9b26-576fcbe93fb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facd33d7520>]}
[0m15:26:50.465490 [info ] [MainThread]: Found 15 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:26:50.465696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'eb21f7a4-8bd1-4ba5-9b26-576fcbe93fb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facd42ecb90>]}
[0m15:26:50.467245 [info ] [MainThread]: 
[0m15:26:50.467523 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:26:50.467695 [info ] [MainThread]: 
[0m15:26:50.467998 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:26:50.471441 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features'
[0m15:26:50.485948 [debug] [ThreadPool]: Using duckdb connection "list_quant_features"
[0m15:26:50.486203 [debug] [ThreadPool]: On list_quant_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"quant_features"'
    
  
  
[0m15:26:50.486385 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:26:50.496227 [debug] [ThreadPool]: SQL status: OK in 0.010 seconds
[0m15:26:50.497162 [debug] [ThreadPool]: On list_quant_features: Close
[0m15:26:50.497881 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_quant_features, now create_quant_features_main)
[0m15:26:50.498174 [debug] [ThreadPool]: Creating schema "database: "quant_features"
schema: "main"
"
[0m15:26:50.502318 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:26:50.502575 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='quant_features'
        and type='sqlite'
    
  
[0m15:26:50.502745 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:26:50.503411 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:26:50.504262 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:26:50.504487 [debug] [ThreadPool]: On create_quant_features_main: BEGIN
[0m15:26:50.504788 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:26:50.504968 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:26:50.505121 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
    
        create schema if not exists "quant_features"."main"
    
[0m15:26:50.505497 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:26:50.506055 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:26:50.506244 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:26:50.506402 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:26:50.506697 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:26:50.506899 [debug] [ThreadPool]: On create_quant_features_main: Close
[0m15:26:50.508677 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_quant_features_main, now list_quant_features_main)
[0m15:26:50.512443 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:26:50.512691 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:26:50.512869 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:26:50.513267 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:26:50.513454 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:26:50.513612 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:26:50.519737 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m15:26:50.520610 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:26:50.521230 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:26:50.521440 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:26:50.522430 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'eb21f7a4-8bd1-4ba5-9b26-576fcbe93fb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facd33936a0>]}
[0m15:26:50.522789 [debug] [MainThread]: Using duckdb connection "master"
[0m15:26:50.522988 [debug] [MainThread]: On master: BEGIN
[0m15:26:50.523139 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:26:50.523520 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:26:50.523705 [debug] [MainThread]: On master: COMMIT
[0m15:26:50.523872 [debug] [MainThread]: Using duckdb connection "master"
[0m15:26:50.524017 [debug] [MainThread]: On master: COMMIT
[0m15:26:50.524276 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:26:50.524486 [debug] [MainThread]: On master: Close
[0m15:26:50.527101 [debug] [Thread-1 (]: Began running node seed.quant_features.market_data
[0m15:26:50.527518 [debug] [Thread-2 (]: Began running node seed.quant_features.raw_stock_prices
[0m15:26:50.528052 [info ] [Thread-1 (]: 1 of 2 START seed file main.market_data ........................................ [RUN]
[0m15:26:50.528529 [info ] [Thread-2 (]: 2 of 2 START seed file main.raw_stock_prices ................................... [RUN]
[0m15:26:50.529111 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now seed.quant_features.market_data)
[0m15:26:50.529620 [debug] [Thread-2 (]: Acquiring new duckdb connection 'seed.quant_features.raw_stock_prices'
[0m15:26:50.529974 [debug] [Thread-1 (]: Began compiling node seed.quant_features.market_data
[0m15:26:50.530283 [debug] [Thread-2 (]: Began compiling node seed.quant_features.raw_stock_prices
[0m15:26:50.530585 [debug] [Thread-1 (]: Began executing node seed.quant_features.market_data
[0m15:26:50.530870 [debug] [Thread-2 (]: Began executing node seed.quant_features.raw_stock_prices
[0m15:26:50.563685 [debug] [Thread-1 (]: Using duckdb connection "seed.quant_features.market_data"
[0m15:26:50.566174 [debug] [Thread-2 (]: Using duckdb connection "seed.quant_features.raw_stock_prices"
[0m15:26:50.566654 [debug] [Thread-1 (]: On seed.quant_features.market_data: BEGIN
[0m15:26:50.567026 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "seed.quant_features.raw_stock_prices"} */

      drop table if exists "quant_features"."main"."raw_stock_prices" cascade
    
[0m15:26:50.567699 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:26:50.567382 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:26:50.568613 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:26:50.569074 [debug] [Thread-1 (]: Using duckdb connection "seed.quant_features.market_data"
[0m15:26:50.569420 [debug] [Thread-1 (]: On seed.quant_features.market_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "seed.quant_features.market_data"} */

    create table "quant_features"."main"."market_data" ("date" date,"symbol" text,"market_cap" BIGINT,"pe_ratio" DECIMAL(10,2),"dividend_yield" DECIMAL(5,2),"sector" text)
  
[0m15:26:50.570004 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m15:26:50.570992 [debug] [Thread-2 (]: Using duckdb connection "seed.quant_features.raw_stock_prices"
[0m15:26:50.571235 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: BEGIN
[0m15:26:50.571617 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:26:50.571822 [debug] [Thread-2 (]: Using duckdb connection "seed.quant_features.raw_stock_prices"
[0m15:26:50.572017 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "seed.quant_features.raw_stock_prices"} */

    create table "quant_features"."main"."raw_stock_prices" ("date" date,"symbol" text,"open" DECIMAL(10,2),"high" DECIMAL(10,2),"low" DECIMAL(10,2),"close" DECIMAL(10,2),"volume" BIGINT)
  
[0m15:26:50.572519 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m15:26:50.579426 [debug] [Thread-1 (]: Using duckdb connection "seed.quant_features.market_data"
[0m15:26:50.579720 [debug] [Thread-1 (]: On seed.quant_features.market_data: 
          COPY "quant_features"."main"."market_data" FROM '/workspace/dbt_project/seeds/market_data.csv' (FORMAT CSV, HEADER TRUE, DELIMITER ',')
        ...
[0m15:26:50.580176 [debug] [Thread-2 (]: SQL status: OK in 0.008 seconds
[0m15:26:50.581003 [debug] [Thread-2 (]: Using duckdb connection "seed.quant_features.raw_stock_prices"
[0m15:26:50.581240 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: 
          COPY "quant_features"."main"."raw_stock_prices" FROM '/workspace/dbt_project/seeds/raw_stock_prices.csv' (FORMAT CSV, HEADER TRUE, DELIMITER ',')
        ...
[0m15:26:50.584001 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m15:26:50.587542 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.quant_features.market_data"
[0m15:26:50.588037 [debug] [Thread-2 (]: SQL status: OK in 0.007 seconds
[0m15:26:50.593712 [debug] [Thread-2 (]: Writing runtime SQL for node "seed.quant_features.raw_stock_prices"
[0m15:26:50.597242 [debug] [Thread-1 (]: On seed.quant_features.market_data: COMMIT
[0m15:26:50.597764 [debug] [Thread-1 (]: Using duckdb connection "seed.quant_features.market_data"
[0m15:26:50.598026 [debug] [Thread-1 (]: On seed.quant_features.market_data: COMMIT
[0m15:26:50.600516 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: COMMIT
[0m15:26:50.600800 [debug] [Thread-2 (]: Using duckdb connection "seed.quant_features.raw_stock_prices"
[0m15:26:50.601006 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: COMMIT
[0m15:26:50.601341 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m15:26:50.602754 [debug] [Thread-1 (]: On seed.quant_features.market_data: Close
[0m15:26:50.603241 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m15:26:50.603800 [debug] [Thread-2 (]: On seed.quant_features.raw_stock_prices: Close
[0m15:26:50.605049 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'eb21f7a4-8bd1-4ba5-9b26-576fcbe93fb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facd91b0b90>]}
[0m15:26:50.605427 [info ] [Thread-1 (]: 1 of 2 OK loaded seed file main.market_data .................................... [[32mCREATE 32[0m in 0.08s]
[0m15:26:50.605890 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'eb21f7a4-8bd1-4ba5-9b26-576fcbe93fb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facd28c0850>]}
[0m15:26:50.606320 [debug] [Thread-1 (]: Finished running node seed.quant_features.market_data
[0m15:26:50.606716 [info ] [Thread-2 (]: 2 of 2 OK loaded seed file main.raw_stock_prices ............................... [[32mCREATE 32[0m in 0.08s]
[0m15:26:50.607575 [debug] [Thread-2 (]: Finished running node seed.quant_features.raw_stock_prices
[0m15:26:50.608973 [debug] [MainThread]: Using duckdb connection "master"
[0m15:26:50.609230 [debug] [MainThread]: On master: BEGIN
[0m15:26:50.609399 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:26:50.609801 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:26:50.610009 [debug] [MainThread]: On master: COMMIT
[0m15:26:50.610168 [debug] [MainThread]: Using duckdb connection "master"
[0m15:26:50.610315 [debug] [MainThread]: On master: COMMIT
[0m15:26:50.610568 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:26:50.610740 [debug] [MainThread]: On master: Close
[0m15:26:50.611016 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:26:50.611173 [debug] [MainThread]: Connection 'seed.quant_features.market_data' was properly closed.
[0m15:26:50.611302 [debug] [MainThread]: Connection 'seed.quant_features.raw_stock_prices' was properly closed.
[0m15:26:50.611468 [info ] [MainThread]: 
[0m15:26:50.611654 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 0.14 seconds (0.14s).
[0m15:26:50.612102 [debug] [MainThread]: Command end result
[0m15:26:50.632560 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:26:50.633698 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:26:50.637276 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:26:50.637513 [info ] [MainThread]: 
[0m15:26:50.637732 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:26:50.637910 [info ] [MainThread]: 
[0m15:26:50.638086 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m15:26:50.638372 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 12 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m15:26:50.638961 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 1.7758723, "process_in_blocks": "0", "process_kernel_time": 0.151495, "process_mem_max_rss": "163276", "process_out_blocks": "4896", "process_user_time": 2.503625}
[0m15:26:50.639278 [debug] [MainThread]: Command `dbt seed` succeeded at 15:26:50.639211 after 1.78 seconds
[0m15:26:50.639477 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facd84cdda0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facdc6e5400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facd348a330>]}
[0m15:26:50.639688 [debug] [MainThread]: Flushing usage events
[0m15:26:50.686796 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:26:56.448294 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05b31f3770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05b482da90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05b206fd90>]}


============================== 15:26:56.450578 | b9de5e66-0c9a-407b-8ac2-3599d5e61949 ==============================
[0m15:26:56.450578 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:26:56.451338 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'no_print': 'None', 'log_cache_events': 'False', 'invocation_command': 'dbt run', 'debug': 'False', 'indirect_selection': 'eager', 'profiles_dir': '/workspace/dbt_project', 'empty': 'False', 'printer_width': '80', 'target_path': 'None', 'quiet': 'False', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'log_path': '/workspace/dbt_project/logs', 'log_format': 'default', 'cache_selected_only': 'False', 'write_json': 'True', 'version_check': 'True', 'introspect': 'True', 'warn_error': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'use_colors': 'True'}
[0m15:26:56.594082 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b9de5e66-0c9a-407b-8ac2-3599d5e61949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05b2d86650>]}
[0m15:26:56.635228 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b9de5e66-0c9a-407b-8ac2-3599d5e61949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05b216abe0>]}
[0m15:26:56.636306 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:26:56.665055 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:26:56.751531 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:26:56.751775 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:26:56.788097 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b9de5e66-0c9a-407b-8ac2-3599d5e61949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05ae139450>]}
[0m15:26:56.857650 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:26:56.858834 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:26:56.869282 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b9de5e66-0c9a-407b-8ac2-3599d5e61949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05ae16d220>]}
[0m15:26:56.869630 [info ] [MainThread]: Found 15 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:26:56.869856 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b9de5e66-0c9a-407b-8ac2-3599d5e61949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05ae1ac130>]}
[0m15:26:56.871454 [info ] [MainThread]: 
[0m15:26:56.871714 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:26:56.871904 [info ] [MainThread]: 
[0m15:26:56.872186 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:26:56.875782 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features'
[0m15:26:56.893799 [debug] [ThreadPool]: Using duckdb connection "list_quant_features"
[0m15:26:56.894069 [debug] [ThreadPool]: On list_quant_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"quant_features"'
    
  
  
[0m15:26:56.894251 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:26:56.905772 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m15:26:56.906684 [debug] [ThreadPool]: On list_quant_features: Close
[0m15:26:56.907398 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_quant_features, now create_quant_features_main)
[0m15:26:56.907708 [debug] [ThreadPool]: Creating schema "database: "quant_features"
schema: "main"
"
[0m15:26:56.912007 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:26:56.912265 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='quant_features'
        and type='sqlite'
    
  
[0m15:26:56.912466 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:26:56.913153 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:26:56.913996 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:26:56.914210 [debug] [ThreadPool]: On create_quant_features_main: BEGIN
[0m15:26:56.914572 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:26:56.914752 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:26:56.914921 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
    
        create schema if not exists "quant_features"."main"
    
[0m15:26:56.915251 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:26:56.915772 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:26:56.915989 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:26:56.916153 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:26:56.916467 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:26:56.916651 [debug] [ThreadPool]: On create_quant_features_main: Close
[0m15:26:56.918736 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_quant_features_main, now list_quant_features_main)
[0m15:26:56.953161 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:26:56.953411 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:26:56.953581 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:26:56.954003 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:26:56.954211 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:26:56.954378 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:26:56.960875 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m15:26:56.961726 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:26:56.962325 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:26:56.962528 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:26:56.963759 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b9de5e66-0c9a-407b-8ac2-3599d5e61949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05b212b520>]}
[0m15:26:56.964220 [debug] [MainThread]: Using duckdb connection "master"
[0m15:26:56.964428 [debug] [MainThread]: On master: BEGIN
[0m15:26:56.964582 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:26:56.964979 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:26:56.965169 [debug] [MainThread]: On master: COMMIT
[0m15:26:56.965315 [debug] [MainThread]: Using duckdb connection "master"
[0m15:26:56.965452 [debug] [MainThread]: On master: COMMIT
[0m15:26:56.965708 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:26:56.965900 [debug] [MainThread]: On master: Close
[0m15:26:56.968114 [debug] [Thread-1 (]: Began running node model.quant_features.stg_market_data
[0m15:26:56.968488 [debug] [Thread-2 (]: Began running node model.quant_features.stg_ohlc_data
[0m15:26:56.968960 [debug] [Thread-3 (]: Began running node model.quant_features.stg_stock_prices
[0m15:26:56.969504 [info ] [Thread-1 (]: 1 of 15 START sql view model main.stg_market_data .............................. [RUN]
[0m15:26:56.970031 [info ] [Thread-2 (]: 2 of 15 START sql view model main.stg_ohlc_data ................................ [RUN]
[0m15:26:56.970559 [info ] [Thread-3 (]: 3 of 15 START sql view model main.stg_stock_prices ............................. [RUN]
[0m15:26:56.971119 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.stg_market_data)
[0m15:26:56.971718 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.quant_features.stg_ohlc_data'
[0m15:26:56.972189 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.quant_features.stg_stock_prices'
[0m15:26:56.972485 [debug] [Thread-1 (]: Began compiling node model.quant_features.stg_market_data
[0m15:26:56.972766 [debug] [Thread-2 (]: Began compiling node model.quant_features.stg_ohlc_data
[0m15:26:56.973255 [debug] [Thread-3 (]: Began compiling node model.quant_features.stg_stock_prices
[0m15:26:56.978076 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.stg_market_data"
[0m15:26:56.980913 [debug] [Thread-2 (]: Writing injected SQL for node "model.quant_features.stg_ohlc_data"
[0m15:26:56.983137 [debug] [Thread-3 (]: Writing injected SQL for node "model.quant_features.stg_stock_prices"
[0m15:26:56.984175 [debug] [Thread-2 (]: Began executing node model.quant_features.stg_ohlc_data
[0m15:26:56.996465 [debug] [Thread-1 (]: Began executing node model.quant_features.stg_market_data
[0m15:26:57.010009 [debug] [Thread-1 (]: Writing runtime sql for node "model.quant_features.stg_market_data"
[0m15:26:57.011624 [debug] [Thread-2 (]: Writing runtime sql for node "model.quant_features.stg_ohlc_data"
[0m15:26:57.012118 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:26:57.012407 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: BEGIN
[0m15:26:57.012621 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:26:57.013239 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stg_ohlc_data"
[0m15:26:57.013517 [debug] [Thread-2 (]: On model.quant_features.stg_ohlc_data: BEGIN
[0m15:26:57.013704 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:26:57.014131 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:26:57.014464 [debug] [Thread-3 (]: Began executing node model.quant_features.stg_stock_prices
[0m15:26:57.014802 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stg_ohlc_data"
[0m15:26:57.015156 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:26:57.016994 [debug] [Thread-3 (]: Writing runtime sql for node "model.quant_features.stg_stock_prices"
[0m15:26:57.017521 [debug] [Thread-2 (]: On model.quant_features.stg_ohlc_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_ohlc_data"} */

  
  create view "quant_features"."main"."stg_ohlc_data__dbt_tmp" as (
    

with raw_ohlc as (
    select 
        symbol,
        timestamp,
        open,
        high,
        low,
        close,
        volume,
        -- 数据清洗和验证
        case 
            when open <= 0 or high <= 0 or low <= 0 or close <= 0 then null
            when high < greatest(open, close, low) then null
            when low > least(open, close, high) then null
            else timestamp
        end as valid_timestamp
    from "quant_features"."raw"."ohlc_data"
),

cleaned_ohlc as (
    select 
        symbol,
        timestamp,
        open,
        high,
        low,
        close,
        volume,
        -- 计算基础指标
        (high + low + close) / 3 as typical_price,
        (high - low) as daily_range,
        case when open != 0 then (close - open) / open else 0 end as daily_return,
        case when close != 0 then volume / close else 0 end as volume_price_ratio
    from raw_ohlc
    where valid_timestamp is not null
      and timestamp >= '2020-01-01'
      and timestamp <= '2024-12-31'
)

select * from cleaned_ohlc
  );

[0m15:26:57.017953 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:26:57.018902 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_market_data"} */

  
  create view "quant_features"."main"."stg_market_data__dbt_tmp" as (
    

with source_data as (
    select
        date,
        symbol,
        market_cap,
        pe_ratio,
        dividend_yield,
        sector,
        -- 添加计算字段
        case 
            when pe_ratio > 0 then market_cap / pe_ratio 
            else null 
        end as estimated_earnings,
        case 
            when dividend_yield > 0 then market_cap * dividend_yield / 100 
            else 0 
        end as estimated_dividend_payout
    from "quant_features"."main"."market_data"
    where date is not null
      and symbol is not null
      and market_cap > 0
)

select * from source_data
  );

[0m15:26:57.019335 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:26:57.019753 [debug] [Thread-3 (]: On model.quant_features.stg_stock_prices: BEGIN
[0m15:26:57.020125 [debug] [Thread-2 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_ohlc_data"} */

  
  create view "quant_features"."main"."stg_ohlc_data__dbt_tmp" as (
    

with raw_ohlc as (
    select 
        symbol,
        timestamp,
        open,
        high,
        low,
        close,
        volume,
        -- 数据清洗和验证
        case 
            when open <= 0 or high <= 0 or low <= 0 or close <= 0 then null
            when high < greatest(open, close, low) then null
            when low > least(open, close, high) then null
            else timestamp
        end as valid_timestamp
    from "quant_features"."raw"."ohlc_data"
),

cleaned_ohlc as (
    select 
        symbol,
        timestamp,
        open,
        high,
        low,
        close,
        volume,
        -- 计算基础指标
        (high + low + close) / 3 as typical_price,
        (high - low) as daily_range,
        case when open != 0 then (close - open) / open else 0 end as daily_return,
        case when close != 0 then volume / close else 0 end as volume_price_ratio
    from raw_ohlc
    where valid_timestamp is not null
      and timestamp >= '2020-01-01'
      and timestamp <= '2024-12-31'
)

select * from cleaned_ohlc
  );

[0m15:26:57.020452 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:26:57.020764 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:26:57.021280 [debug] [Thread-2 (]: DuckDB adapter: Rolling back transaction.
[0m15:26:57.025646 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:26:57.026148 [debug] [Thread-2 (]: On model.quant_features.stg_ohlc_data: ROLLBACK
[0m15:26:57.026473 [debug] [Thread-3 (]: SQL status: OK in 0.006 seconds
[0m15:26:57.026857 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_market_data"} */
alter view "quant_features"."main"."stg_market_data__dbt_tmp" rename to "stg_market_data"
[0m15:26:57.027925 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:26:57.028621 [debug] [Thread-3 (]: On model.quant_features.stg_stock_prices: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_stock_prices"} */

  
  create view "quant_features"."main"."stg_stock_prices__dbt_tmp" as (
    

with source_data as (
    select
        date,
        symbol,
        open,
        high,
        low,
        close,
        volume,
        -- 计算基础技术指标
        (high - low) as daily_range,
        (close - open) as daily_change,
        (close - open) / open as daily_return,
        -- 添加数据质量检查
        case 
            when high >= low and high >= open and high >= close 
                 and low <= open and low <= close 
            then true 
            else false 
        end as is_valid_ohlc
    from "quant_features"."main"."raw_stock_prices"
    where date is not null
      and symbol is not null
      and open > 0
      and high > 0
      and low > 0
      and close > 0
      and volume >= 0
)

select * from source_data
where is_valid_ohlc = true
  );

[0m15:26:57.029208 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:26:57.034733 [debug] [Thread-3 (]: SQL status: OK in 0.006 seconds
[0m15:26:57.036589 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:26:57.037124 [debug] [Thread-3 (]: On model.quant_features.stg_stock_prices: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_stock_prices"} */
alter view "quant_features"."main"."stg_stock_prices__dbt_tmp" rename to "stg_stock_prices"
[0m15:26:57.038697 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: COMMIT
[0m15:26:57.039577 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:26:57.040047 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: COMMIT
[0m15:26:57.040537 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m15:26:57.041441 [debug] [Thread-3 (]: On model.quant_features.stg_stock_prices: COMMIT
[0m15:26:57.041678 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:26:57.041941 [debug] [Thread-3 (]: On model.quant_features.stg_stock_prices: COMMIT
[0m15:26:57.042359 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:26:57.045881 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:26:57.046159 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_market_data"} */

      drop view if exists "quant_features"."main"."stg_market_data__dbt_backup" cascade
    
[0m15:26:57.046529 [debug] [Thread-3 (]: SQL status: OK in 0.004 seconds
[0m15:26:57.047988 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:26:57.048276 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:26:57.048631 [debug] [Thread-3 (]: On model.quant_features.stg_stock_prices: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_stock_prices"} */

      drop view if exists "quant_features"."main"."stg_stock_prices__dbt_backup" cascade
    
[0m15:26:57.050931 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: Close
[0m15:26:57.051812 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:26:57.052913 [debug] [Thread-3 (]: On model.quant_features.stg_stock_prices: Close
[0m15:26:57.054499 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b9de5e66-0c9a-407b-8ac2-3599d5e61949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05ac4947d0>]}
[0m15:26:57.054825 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b9de5e66-0c9a-407b-8ac2-3599d5e61949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05adb552e0>]}
[0m15:26:57.056884 [debug] [Thread-2 (]: Failed to rollback 'model.quant_features.stg_ohlc_data'
[0m15:26:57.057550 [info ] [Thread-3 (]: 3 of 15 OK created sql view model main.stg_stock_prices ........................ [[32mOK[0m in 0.08s]
[0m15:26:57.058125 [info ] [Thread-1 (]: 1 of 15 OK created sql view model main.stg_market_data ......................... [[32mOK[0m in 0.08s]
[0m15:26:57.058548 [debug] [Thread-2 (]: On model.quant_features.stg_ohlc_data: Close
[0m15:26:57.059023 [debug] [Thread-3 (]: Finished running node model.quant_features.stg_stock_prices
[0m15:26:57.059524 [debug] [Thread-1 (]: Finished running node model.quant_features.stg_market_data
[0m15:26:57.062310 [debug] [Thread-2 (]: Runtime Error in model stg_ohlc_data (models/staging/stg_ohlc_data.sql)
  Catalog Error: Table with name ohlc_data does not exist!
  Did you mean "pg_catalog.pg_database"?
  
  LINE 23:     from "quant_features"."raw"."ohlc_data"
                    ^
[0m15:26:57.062636 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b9de5e66-0c9a-407b-8ac2-3599d5e61949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05ad0e9450>]}
[0m15:26:57.063084 [error] [Thread-2 (]: 2 of 15 ERROR creating sql view model main.stg_ohlc_data ....................... [[31mERROR[0m in 0.09s]
[0m15:26:57.063430 [debug] [Thread-2 (]: Finished running node model.quant_features.stg_ohlc_data
[0m15:26:57.063848 [debug] [Thread-7 (]: Marking all children of 'model.quant_features.stg_ohlc_data' to be skipped because of status 'error'.  Reason: Runtime Error in model stg_ohlc_data (models/staging/stg_ohlc_data.sql)
  Catalog Error: Table with name ohlc_data does not exist!
  Did you mean "pg_catalog.pg_database"?
  
  LINE 23:     from "quant_features"."raw"."ohlc_data"
                    ^.
[0m15:26:57.064720 [debug] [Thread-4 (]: Began running node model.quant_features.daily_stock_summary
[0m15:26:57.065116 [info ] [Thread-4 (]: 4 of 15 START sql table model main.daily_stock_summary ......................... [RUN]
[0m15:26:57.065652 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.quant_features.daily_stock_summary'
[0m15:26:57.065987 [debug] [Thread-4 (]: Began compiling node model.quant_features.daily_stock_summary
[0m15:26:57.066238 [debug] [Thread-3 (]: Began running node model.quant_features.alpha_base_data
[0m15:26:57.068309 [debug] [Thread-4 (]: Writing injected SQL for node "model.quant_features.daily_stock_summary"
[0m15:26:57.068679 [debug] [Thread-1 (]: Began running node model.quant_features.mart_technical_indicators
[0m15:26:57.069079 [info ] [Thread-3 (]: 5 of 15 SKIP relation main.alpha_base_data ..................................... [[33mSKIP[0m]
[0m15:26:57.069607 [info ] [Thread-1 (]: 6 of 15 SKIP relation main.mart_technical_indicators ........................... [[33mSKIP[0m]
[0m15:26:57.070036 [debug] [Thread-3 (]: Finished running node model.quant_features.alpha_base_data
[0m15:26:57.070453 [debug] [Thread-1 (]: Finished running node model.quant_features.mart_technical_indicators
[0m15:26:57.070994 [debug] [Thread-3 (]: Began running node model.quant_features.alpha_factors_021_050
[0m15:26:57.071240 [info ] [Thread-3 (]: 8 of 15 SKIP relation main.alpha_factors_021_050 ............................... [[33mSKIP[0m]
[0m15:26:57.071535 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_factors_051_075
[0m15:26:57.071894 [debug] [Thread-2 (]: Began running node model.quant_features.alpha_factors_001_020
[0m15:26:57.072194 [debug] [Thread-3 (]: Finished running node model.quant_features.alpha_factors_021_050
[0m15:26:57.072532 [debug] [Thread-4 (]: Began executing node model.quant_features.daily_stock_summary
[0m15:26:57.072936 [info ] [Thread-1 (]: 9 of 15 SKIP relation main.alpha_factors_051_075 ............................... [[33mSKIP[0m]
[0m15:26:57.073355 [info ] [Thread-2 (]: 7 of 15 SKIP relation main.alpha_factors_001_020 ............................... [[33mSKIP[0m]
[0m15:26:57.073985 [debug] [Thread-3 (]: Began running node model.quant_features.alpha_factors_076_101
[0m15:26:57.084594 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_factors_051_075
[0m15:26:57.087120 [debug] [Thread-4 (]: Writing runtime sql for node "model.quant_features.daily_stock_summary"
[0m15:26:57.087669 [debug] [Thread-2 (]: Finished running node model.quant_features.alpha_factors_001_020
[0m15:26:57.088056 [info ] [Thread-3 (]: 10 of 15 SKIP relation main.alpha_factors_076_101 .............................. [[33mSKIP[0m]
[0m15:26:57.088423 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_factors_advanced
[0m15:26:57.088903 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:26:57.089413 [debug] [Thread-2 (]: Began running node model.quant_features.features_ohlc_technical
[0m15:26:57.089753 [debug] [Thread-3 (]: Finished running node model.quant_features.alpha_factors_076_101
[0m15:26:57.090340 [info ] [Thread-1 (]: 11 of 15 SKIP relation main.alpha_factors_advanced ............................. [[33mSKIP[0m]
[0m15:26:57.090710 [debug] [Thread-4 (]: On model.quant_features.daily_stock_summary: BEGIN
[0m15:26:57.091061 [info ] [Thread-2 (]: 12 of 15 SKIP relation main.features_ohlc_technical ............................ [[33mSKIP[0m]
[0m15:26:57.091521 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_factors_advanced
[0m15:26:57.091839 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m15:26:57.092123 [debug] [Thread-2 (]: Finished running node model.quant_features.features_ohlc_technical
[0m15:26:57.092561 [debug] [Thread-3 (]: Began running node model.quant_features.alpha101_complete
[0m15:26:57.092827 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_factors_final
[0m15:26:57.093574 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m15:26:57.093915 [info ] [Thread-3 (]: 13 of 15 SKIP relation main.alpha101_complete .................................. [[33mSKIP[0m]
[0m15:26:57.094250 [info ] [Thread-1 (]: 14 of 15 SKIP relation main.alpha_factors_final ................................ [[33mSKIP[0m]
[0m15:26:57.094688 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:26:57.095068 [debug] [Thread-3 (]: Finished running node model.quant_features.alpha101_complete
[0m15:26:57.095451 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_factors_final
[0m15:26:57.095897 [debug] [Thread-4 (]: On model.quant_features.daily_stock_summary: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.daily_stock_summary"} */

  
    
    

    create  table
      "quant_features"."main"."daily_stock_summary__dbt_tmp"
  
    as (
      

with price_data as (
    select * from "quant_features"."main"."stg_stock_prices"
),

market_data as (
    select * from "quant_features"."main"."stg_market_data"
),

combined_data as (
    select
        p.date,
        p.symbol,
        p.open,
        p.high,
        p.low,
        p.close,
        p.volume,
        p.daily_range,
        p.daily_change,
        p.daily_return,
        m.market_cap,
        m.pe_ratio,
        m.dividend_yield,
        m.sector,
        m.estimated_earnings,
        m.estimated_dividend_payout,
        -- 计算额外的技术指标
        (p.high + p.low + p.close) / 3 as typical_price,
        p.volume * p.close as dollar_volume,
        case 
            when p.daily_return > 0.05 then 'Strong Up'
            when p.daily_return > 0.02 then 'Up'
            when p.daily_return > -0.02 then 'Flat'
            when p.daily_return > -0.05 then 'Down'
            else 'Strong Down'
        end as price_movement_category
    from price_data p
    left join market_data m
        on p.date = m.date
        and p.symbol = m.symbol
)

select * from combined_data
    );
  
  
[0m15:26:57.100755 [debug] [Thread-4 (]: SQL status: OK in 0.004 seconds
[0m15:26:57.102478 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:26:57.102747 [debug] [Thread-4 (]: On model.quant_features.daily_stock_summary: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.daily_stock_summary"} */
alter table "quant_features"."main"."daily_stock_summary__dbt_tmp" rename to "daily_stock_summary"
[0m15:26:57.103219 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m15:26:57.106267 [debug] [Thread-4 (]: On model.quant_features.daily_stock_summary: COMMIT
[0m15:26:57.106542 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:26:57.106733 [debug] [Thread-4 (]: On model.quant_features.daily_stock_summary: COMMIT
[0m15:26:57.108696 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m15:26:57.110402 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:26:57.110668 [debug] [Thread-4 (]: On model.quant_features.daily_stock_summary: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.daily_stock_summary"} */

      drop table if exists "quant_features"."main"."daily_stock_summary__dbt_backup" cascade
    
[0m15:26:57.111086 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m15:26:57.111915 [debug] [Thread-4 (]: On model.quant_features.daily_stock_summary: Close
[0m15:26:57.112392 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b9de5e66-0c9a-407b-8ac2-3599d5e61949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05b206b550>]}
[0m15:26:57.112774 [info ] [Thread-4 (]: 4 of 15 OK created sql table model main.daily_stock_summary .................... [[32mOK[0m in 0.05s]
[0m15:26:57.113141 [debug] [Thread-4 (]: Finished running node model.quant_features.daily_stock_summary
[0m15:26:57.113624 [debug] [Thread-2 (]: Began running node model.quant_features.stock_features
[0m15:26:57.113924 [info ] [Thread-2 (]: 15 of 15 START sql table model main.stock_features ............................. [RUN]
[0m15:26:57.114253 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.quant_features.stg_ohlc_data, now model.quant_features.stock_features)
[0m15:26:57.114462 [debug] [Thread-2 (]: Began compiling node model.quant_features.stock_features
[0m15:26:57.116280 [debug] [Thread-2 (]: Writing injected SQL for node "model.quant_features.stock_features"
[0m15:26:57.116757 [debug] [Thread-2 (]: Began executing node model.quant_features.stock_features
[0m15:26:57.118763 [debug] [Thread-2 (]: Writing runtime sql for node "model.quant_features.stock_features"
[0m15:26:57.119225 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:26:57.119445 [debug] [Thread-2 (]: On model.quant_features.stock_features: BEGIN
[0m15:26:57.119626 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:26:57.120020 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:26:57.120226 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:26:57.120469 [debug] [Thread-2 (]: On model.quant_features.stock_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stock_features"} */

  
    
    

    create  table
      "quant_features"."main"."stock_features__dbt_tmp"
  
    as (
      

with daily_data as (
    select * from "quant_features"."main"."daily_stock_summary"
),

windowed_features as (
    select
        *,
        -- 移动平均
        avg(close) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        ) as ma_5d,
        
        avg(close) over (
            partition by symbol 
            order by date 
            rows between 9 preceding and current row
        ) as ma_10d,
        
        -- 波动率（标准差）
        stddev(daily_return) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        ) as volatility_5d,
        
        -- 价格相对位置
        (close - min(low) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        )) / (max(high) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        ) - min(low) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        )) as price_position_5d,
        
        -- 成交量相对强度
        volume / avg(volume) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        ) as volume_ratio_5d
        
    from daily_data
),

final_features as (
    select
        *,
        -- 技术信号
        case 
            when close > ma_5d and ma_5d > ma_10d then 'Bullish'
            when close < ma_5d and ma_5d < ma_10d then 'Bearish'
            else 'Neutral'
        end as trend_signal,
        
        case 
            when volatility_5d > 0.03 then 'High'
            when volatility_5d > 0.01 then 'Medium'
            else 'Low'
        end as volatility_category
        
    from windowed_features
)

select * from final_features
    );
  
  
[0m15:26:57.125899 [debug] [Thread-2 (]: SQL status: OK in 0.005 seconds
[0m15:26:57.127592 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:26:57.127864 [debug] [Thread-2 (]: On model.quant_features.stock_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stock_features"} */
alter table "quant_features"."main"."stock_features__dbt_tmp" rename to "stock_features"
[0m15:26:57.128348 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:26:57.129195 [debug] [Thread-2 (]: On model.quant_features.stock_features: COMMIT
[0m15:26:57.129440 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:26:57.129635 [debug] [Thread-2 (]: On model.quant_features.stock_features: COMMIT
[0m15:26:57.131312 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:26:57.132683 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:26:57.132943 [debug] [Thread-2 (]: On model.quant_features.stock_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stock_features"} */

      drop table if exists "quant_features"."main"."stock_features__dbt_backup" cascade
    
[0m15:26:57.133348 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:26:57.134170 [debug] [Thread-2 (]: On model.quant_features.stock_features: Close
[0m15:26:57.134813 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b9de5e66-0c9a-407b-8ac2-3599d5e61949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05ac56f9a0>]}
[0m15:26:57.135208 [info ] [Thread-2 (]: 15 of 15 OK created sql table model main.stock_features ........................ [[32mOK[0m in 0.02s]
[0m15:26:57.135513 [debug] [Thread-2 (]: Finished running node model.quant_features.stock_features
[0m15:26:57.136653 [debug] [MainThread]: Using duckdb connection "master"
[0m15:26:57.136926 [debug] [MainThread]: On master: BEGIN
[0m15:26:57.137089 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:26:57.137439 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:26:57.137616 [debug] [MainThread]: On master: COMMIT
[0m15:26:57.137772 [debug] [MainThread]: Using duckdb connection "master"
[0m15:26:57.137934 [debug] [MainThread]: On master: COMMIT
[0m15:26:57.138188 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:26:57.138357 [debug] [MainThread]: On master: Close
[0m15:26:57.138631 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:26:57.138790 [debug] [MainThread]: Connection 'model.quant_features.stg_market_data' was properly closed.
[0m15:26:57.138941 [debug] [MainThread]: Connection 'model.quant_features.stock_features' was properly closed.
[0m15:26:57.139072 [debug] [MainThread]: Connection 'model.quant_features.stg_stock_prices' was properly closed.
[0m15:26:57.139204 [debug] [MainThread]: Connection 'model.quant_features.daily_stock_summary' was properly closed.
[0m15:26:57.139419 [info ] [MainThread]: 
[0m15:26:57.139618 [info ] [MainThread]: Finished running 12 table models, 3 view models in 0 hours 0 minutes and 0.27 seconds (0.27s).
[0m15:26:57.140395 [debug] [MainThread]: Command end result
[0m15:26:57.163073 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:26:57.164191 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:26:57.168089 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:26:57.168322 [info ] [MainThread]: 
[0m15:26:57.168544 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:26:57.168715 [info ] [MainThread]: 
[0m15:26:57.168931 [error] [MainThread]: [31mFailure in model stg_ohlc_data (models/staging/stg_ohlc_data.sql)[0m
[0m15:26:57.169129 [error] [MainThread]:   Runtime Error in model stg_ohlc_data (models/staging/stg_ohlc_data.sql)
  Catalog Error: Table with name ohlc_data does not exist!
  Did you mean "pg_catalog.pg_database"?
  
  LINE 23:     from "quant_features"."raw"."ohlc_data"
                    ^
[0m15:26:57.169279 [info ] [MainThread]: 
[0m15:26:57.169469 [info ] [MainThread]:   compiled code at target/compiled/quant_features/models/staging/stg_ohlc_data.sql
[0m15:26:57.169618 [info ] [MainThread]: 
[0m15:26:57.169780 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=1 SKIP=10 NO-OP=0 TOTAL=15
[0m15:26:57.170320 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.757745, "process_in_blocks": "0", "process_kernel_time": 0.148485, "process_mem_max_rss": "160840", "process_out_blocks": "3680", "process_user_time": 1.504961}
[0m15:26:57.170611 [debug] [MainThread]: Command `dbt run` failed at 15:26:57.170551 after 0.76 seconds
[0m15:26:57.170830 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05adb3c0b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05b2166570>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05ae03fdd0>]}
[0m15:26:57.171039 [debug] [MainThread]: Flushing usage events
[0m15:26:57.212220 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:27:19.361235 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90cfad3770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90d1105a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90ce947d90>]}


============================== 15:27:19.363613 | 546a7400-bea5-4d62-9693-c27be28811e4 ==============================
[0m15:27:19.363613 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:27:19.363934 [debug] [MainThread]: running dbt with arguments {'log_path': '/workspace/dbt_project/logs', 'static_parser': 'True', 'fail_fast': 'False', 'no_print': 'None', 'version_check': 'True', 'use_colors': 'True', 'introspect': 'True', 'target_path': 'None', 'use_experimental_parser': 'False', 'invocation_command': 'dbt run', 'cache_selected_only': 'False', 'debug': 'False', 'indirect_selection': 'eager', 'warn_error': 'None', 'profiles_dir': '/workspace/dbt_project', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'log_cache_events': 'False', 'empty': 'False', 'printer_width': '80', 'write_json': 'True', 'log_format': 'default', 'quiet': 'False', 'partial_parse': 'True'}
[0m15:27:19.499701 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '546a7400-bea5-4d62-9693-c27be28811e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90cf65e650>]}
[0m15:27:19.541214 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '546a7400-bea5-4d62-9693-c27be28811e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90cea42be0>]}
[0m15:27:19.542209 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:27:19.571543 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:27:19.657453 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:27:19.657918 [debug] [MainThread]: Partial parsing: updated file: quant_features://models/staging/stg_ohlc_data.sql
[0m15:27:19.977364 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '546a7400-bea5-4d62-9693-c27be28811e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90ca51e250>]}
[0m15:27:20.046399 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:27:20.047614 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:27:20.058917 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '546a7400-bea5-4d62-9693-c27be28811e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90cb28d220>]}
[0m15:27:20.059292 [info ] [MainThread]: Found 15 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:27:20.059507 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '546a7400-bea5-4d62-9693-c27be28811e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90ca99ab30>]}
[0m15:27:20.061329 [info ] [MainThread]: 
[0m15:27:20.061604 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:27:20.061773 [info ] [MainThread]: 
[0m15:27:20.062049 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:27:20.066050 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features'
[0m15:27:20.081967 [debug] [ThreadPool]: Using duckdb connection "list_quant_features"
[0m15:27:20.082237 [debug] [ThreadPool]: On list_quant_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"quant_features"'
    
  
  
[0m15:27:20.082417 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:27:20.092613 [debug] [ThreadPool]: SQL status: OK in 0.010 seconds
[0m15:27:20.093480 [debug] [ThreadPool]: On list_quant_features: Close
[0m15:27:20.094266 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_quant_features, now create_quant_features_main)
[0m15:27:20.094651 [debug] [ThreadPool]: Creating schema "database: "quant_features"
schema: "main"
"
[0m15:27:20.098933 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:27:20.099206 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='quant_features'
        and type='sqlite'
    
  
[0m15:27:20.099388 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:27:20.100314 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:27:20.101150 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:27:20.101370 [debug] [ThreadPool]: On create_quant_features_main: BEGIN
[0m15:27:20.101689 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:27:20.101851 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:27:20.101997 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
    
        create schema if not exists "quant_features"."main"
    
[0m15:27:20.102327 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:27:20.102866 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:27:20.103090 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:27:20.103312 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:27:20.103715 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:27:20.103905 [debug] [ThreadPool]: On create_quant_features_main: Close
[0m15:27:20.105485 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_quant_features_main, now list_quant_features_main)
[0m15:27:20.109256 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:27:20.109509 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:27:20.109684 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:27:20.110095 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:27:20.110300 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:27:20.110472 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:27:20.117055 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m15:27:20.117986 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:27:20.118624 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:27:20.118838 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:27:20.120386 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '546a7400-bea5-4d62-9693-c27be28811e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90c9137790>]}
[0m15:27:20.120813 [debug] [MainThread]: Using duckdb connection "master"
[0m15:27:20.121011 [debug] [MainThread]: On master: BEGIN
[0m15:27:20.121166 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:27:20.121589 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:27:20.121783 [debug] [MainThread]: On master: COMMIT
[0m15:27:20.121940 [debug] [MainThread]: Using duckdb connection "master"
[0m15:27:20.122090 [debug] [MainThread]: On master: COMMIT
[0m15:27:20.122404 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:27:20.122604 [debug] [MainThread]: On master: Close
[0m15:27:20.124560 [debug] [Thread-3 (]: Began running node model.quant_features.stg_stock_prices
[0m15:27:20.124925 [info ] [Thread-3 (]: 3 of 15 START sql view model main.stg_stock_prices ............................. [RUN]
[0m15:27:20.125253 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.quant_features.stg_stock_prices'
[0m15:27:20.125459 [debug] [Thread-3 (]: Began compiling node model.quant_features.stg_stock_prices
[0m15:27:20.125826 [debug] [Thread-2 (]: Began running node model.quant_features.stg_ohlc_data
[0m15:27:20.130600 [debug] [Thread-3 (]: Writing injected SQL for node "model.quant_features.stg_stock_prices"
[0m15:27:20.131020 [debug] [Thread-1 (]: Began running node model.quant_features.stg_market_data
[0m15:27:20.131594 [info ] [Thread-2 (]: 2 of 15 START sql view model main.stg_ohlc_data ................................ [RUN]
[0m15:27:20.132248 [info ] [Thread-1 (]: 1 of 15 START sql view model main.stg_market_data .............................. [RUN]
[0m15:27:20.132824 [debug] [Thread-3 (]: Began executing node model.quant_features.stg_stock_prices
[0m15:27:20.133424 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.quant_features.stg_ohlc_data'
[0m15:27:20.133934 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.stg_market_data)
[0m15:27:20.144896 [debug] [Thread-2 (]: Began compiling node model.quant_features.stg_ohlc_data
[0m15:27:20.154445 [debug] [Thread-3 (]: Writing runtime sql for node "model.quant_features.stg_stock_prices"
[0m15:27:20.154963 [debug] [Thread-1 (]: Began compiling node model.quant_features.stg_market_data
[0m15:27:20.157774 [debug] [Thread-2 (]: Writing injected SQL for node "model.quant_features.stg_ohlc_data"
[0m15:27:20.158265 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:27:20.160312 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.stg_market_data"
[0m15:27:20.160967 [debug] [Thread-2 (]: Began executing node model.quant_features.stg_ohlc_data
[0m15:27:20.161395 [debug] [Thread-3 (]: On model.quant_features.stg_stock_prices: BEGIN
[0m15:27:20.162053 [debug] [Thread-1 (]: Began executing node model.quant_features.stg_market_data
[0m15:27:20.164518 [debug] [Thread-2 (]: Writing runtime sql for node "model.quant_features.stg_ohlc_data"
[0m15:27:20.165045 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:27:20.167085 [debug] [Thread-1 (]: Writing runtime sql for node "model.quant_features.stg_market_data"
[0m15:27:20.167716 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stg_ohlc_data"
[0m15:27:20.168256 [debug] [Thread-2 (]: On model.quant_features.stg_ohlc_data: BEGIN
[0m15:27:20.168480 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:27:20.168930 [debug] [Thread-3 (]: SQL status: OK in 0.004 seconds
[0m15:27:20.169229 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:27:20.169462 [debug] [Thread-3 (]: On model.quant_features.stg_stock_prices: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_stock_prices"} */

  
  create view "quant_features"."main"."stg_stock_prices__dbt_tmp" as (
    

with source_data as (
    select
        date,
        symbol,
        open,
        high,
        low,
        close,
        volume,
        -- 计算基础技术指标
        (high - low) as daily_range,
        (close - open) as daily_change,
        (close - open) / open as daily_return,
        -- 添加数据质量检查
        case 
            when high >= low and high >= open and high >= close 
                 and low <= open and low <= close 
            then true 
            else false 
        end as is_valid_ohlc
    from "quant_features"."main"."raw_stock_prices"
    where date is not null
      and symbol is not null
      and open > 0
      and high > 0
      and low > 0
      and close > 0
      and volume >= 0
)

select * from source_data
where is_valid_ohlc = true
  );

[0m15:27:20.169953 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:27:20.170325 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m15:27:20.170661 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: BEGIN
[0m15:27:20.171022 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m15:27:20.171342 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stg_ohlc_data"
[0m15:27:20.171613 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:27:20.175825 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:27:20.176390 [debug] [Thread-2 (]: On model.quant_features.stg_ohlc_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_ohlc_data"} */

  
  create view "quant_features"."main"."stg_ohlc_data__dbt_tmp" as (
    

with raw_ohlc as (
    select 
        symbol,
        date as timestamp,
        open,
        high,
        low,
        close,
        volume,
        -- 数据清洗和验证
        case 
            when open <= 0 or high <= 0 or low <= 0 or close <= 0 then null
            when high < greatest(open, close, low) then null
            when low > least(open, close, high) then null
            else date
        end as valid_timestamp
    from "quant_features"."main"."raw_stock_prices"
),

cleaned_ohlc as (
    select 
        symbol,
        timestamp,
        open,
        high,
        low,
        close,
        volume,
        -- 计算基础指标
        (high + low + close) / 3 as typical_price,
        (high - low) as daily_range,
        case when open != 0 then (close - open) / open else 0 end as daily_return,
        case when close != 0 then volume / close else 0 end as volume_price_ratio
    from raw_ohlc
    where valid_timestamp is not null
      and timestamp >= '2020-01-01'
      and timestamp <= '2024-12-31'
)

select * from cleaned_ohlc
  );

[0m15:27:20.177210 [debug] [Thread-3 (]: On model.quant_features.stg_stock_prices: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_stock_prices"} */
alter view "quant_features"."main"."stg_stock_prices" rename to "stg_stock_prices__dbt_backup"
[0m15:27:20.177747 [debug] [Thread-1 (]: SQL status: OK in 0.006 seconds
[0m15:27:20.178489 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:27:20.178818 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_market_data"} */

  
  create view "quant_features"."main"."stg_market_data__dbt_tmp" as (
    

with source_data as (
    select
        date,
        symbol,
        market_cap,
        pe_ratio,
        dividend_yield,
        sector,
        -- 添加计算字段
        case 
            when pe_ratio > 0 then market_cap / pe_ratio 
            else null 
        end as estimated_earnings,
        case 
            when dividend_yield > 0 then market_cap * dividend_yield / 100 
            else 0 
        end as estimated_dividend_payout
    from "quant_features"."main"."market_data"
    where date is not null
      and symbol is not null
      and market_cap > 0
)

select * from source_data
  );

[0m15:27:20.179124 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m15:27:20.179464 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:27:20.181248 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:27:20.183879 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stg_ohlc_data"
[0m15:27:20.184214 [debug] [Thread-3 (]: On model.quant_features.stg_stock_prices: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_stock_prices"} */
alter view "quant_features"."main"."stg_stock_prices__dbt_tmp" rename to "stg_stock_prices"
[0m15:27:20.184510 [debug] [Thread-2 (]: On model.quant_features.stg_ohlc_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_ohlc_data"} */
alter view "quant_features"."main"."stg_ohlc_data__dbt_tmp" rename to "stg_ohlc_data"
[0m15:27:20.185035 [debug] [Thread-1 (]: SQL status: OK in 0.006 seconds
[0m15:27:20.186012 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:27:20.188116 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:27:20.188495 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m15:27:20.196317 [debug] [Thread-3 (]: On model.quant_features.stg_stock_prices: COMMIT
[0m15:27:20.196830 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_market_data"} */
alter view "quant_features"."main"."stg_market_data" rename to "stg_market_data__dbt_backup"
[0m15:27:20.197910 [debug] [Thread-2 (]: On model.quant_features.stg_ohlc_data: COMMIT
[0m15:27:20.198382 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:27:20.199157 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stg_ohlc_data"
[0m15:27:20.199665 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:27:20.200144 [debug] [Thread-3 (]: On model.quant_features.stg_stock_prices: COMMIT
[0m15:27:20.200656 [debug] [Thread-2 (]: On model.quant_features.stg_ohlc_data: COMMIT
[0m15:27:20.202875 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:27:20.203709 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_market_data"} */
alter view "quant_features"."main"."stg_market_data__dbt_tmp" rename to "stg_market_data"
[0m15:27:20.204289 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:27:20.205121 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: COMMIT
[0m15:27:20.205463 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m15:27:20.205842 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:27:20.206256 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m15:27:20.209777 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:27:20.210325 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: COMMIT
[0m15:27:20.212346 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stg_ohlc_data"
[0m15:27:20.212913 [debug] [Thread-3 (]: On model.quant_features.stg_stock_prices: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_stock_prices"} */

      drop view if exists "quant_features"."main"."stg_stock_prices__dbt_backup" cascade
    
[0m15:27:20.213583 [debug] [Thread-2 (]: On model.quant_features.stg_ohlc_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_ohlc_data"} */

      drop view if exists "quant_features"."main"."stg_ohlc_data__dbt_backup" cascade
    
[0m15:27:20.214765 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:27:20.216161 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:27:20.216507 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m15:27:20.216818 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_market_data"} */

      drop view if exists "quant_features"."main"."stg_market_data__dbt_backup" cascade
    
[0m15:27:20.218644 [debug] [Thread-3 (]: On model.quant_features.stg_stock_prices: Close
[0m15:27:20.219117 [debug] [Thread-2 (]: SQL status: OK in 0.005 seconds
[0m15:27:20.220217 [debug] [Thread-2 (]: On model.quant_features.stg_ohlc_data: Close
[0m15:27:20.221364 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '546a7400-bea5-4d62-9693-c27be28811e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90c90b2210>]}
[0m15:27:20.221786 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m15:27:20.222337 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '546a7400-bea5-4d62-9693-c27be28811e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90ca4dad00>]}
[0m15:27:20.222823 [info ] [Thread-3 (]: 3 of 15 OK created sql view model main.stg_stock_prices ........................ [[32mOK[0m in 0.10s]
[0m15:27:20.223817 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: Close
[0m15:27:20.224236 [info ] [Thread-2 (]: 2 of 15 OK created sql view model main.stg_ohlc_data ........................... [[32mOK[0m in 0.09s]
[0m15:27:20.224671 [debug] [Thread-3 (]: Finished running node model.quant_features.stg_stock_prices
[0m15:27:20.225073 [debug] [Thread-2 (]: Finished running node model.quant_features.stg_ohlc_data
[0m15:27:20.225422 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '546a7400-bea5-4d62-9693-c27be28811e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90cb2dfc50>]}
[0m15:27:20.226378 [debug] [Thread-4 (]: Began running node model.quant_features.alpha_base_data
[0m15:27:20.226769 [info ] [Thread-1 (]: 1 of 15 OK created sql view model main.stg_market_data ......................... [[32mOK[0m in 0.09s]
[0m15:27:20.227295 [debug] [Thread-3 (]: Began running node model.quant_features.mart_technical_indicators
[0m15:27:20.227700 [info ] [Thread-4 (]: 4 of 15 START sql table model main.alpha_base_data ............................. [RUN]
[0m15:27:20.228072 [debug] [Thread-1 (]: Finished running node model.quant_features.stg_market_data
[0m15:27:20.228836 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.quant_features.alpha_base_data'
[0m15:27:20.229220 [debug] [Thread-4 (]: Began compiling node model.quant_features.alpha_base_data
[0m15:27:20.228493 [info ] [Thread-3 (]: 5 of 15 START sql table model main.mart_technical_indicators ................... [RUN]
[0m15:27:20.240134 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.quant_features.stg_stock_prices, now model.quant_features.mart_technical_indicators)
[0m15:27:20.240474 [debug] [Thread-3 (]: Began compiling node model.quant_features.mart_technical_indicators
[0m15:27:20.242500 [debug] [Thread-3 (]: Writing injected SQL for node "model.quant_features.mart_technical_indicators"
[0m15:27:20.252238 [debug] [Thread-4 (]: Writing injected SQL for node "model.quant_features.alpha_base_data"
[0m15:27:20.252607 [debug] [Thread-3 (]: Began executing node model.quant_features.mart_technical_indicators
[0m15:27:20.258414 [debug] [Thread-2 (]: Began running node model.quant_features.daily_stock_summary
[0m15:27:20.258819 [info ] [Thread-2 (]: 6 of 15 START sql table model main.daily_stock_summary ......................... [RUN]
[0m15:27:20.266248 [debug] [Thread-3 (]: Writing runtime sql for node "model.quant_features.mart_technical_indicators"
[0m15:27:20.266795 [debug] [Thread-4 (]: Began executing node model.quant_features.alpha_base_data
[0m15:27:20.267315 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.quant_features.stg_ohlc_data, now model.quant_features.daily_stock_summary)
[0m15:27:20.269501 [debug] [Thread-4 (]: Writing runtime sql for node "model.quant_features.alpha_base_data"
[0m15:27:20.270020 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.mart_technical_indicators"
[0m15:27:20.270592 [debug] [Thread-2 (]: Began compiling node model.quant_features.daily_stock_summary
[0m15:27:20.271076 [debug] [Thread-3 (]: On model.quant_features.mart_technical_indicators: BEGIN
[0m15:27:20.275681 [debug] [Thread-2 (]: Writing injected SQL for node "model.quant_features.daily_stock_summary"
[0m15:27:20.276155 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:27:20.276618 [debug] [Thread-2 (]: Began executing node model.quant_features.daily_stock_summary
[0m15:27:20.278814 [debug] [Thread-2 (]: Writing runtime sql for node "model.quant_features.daily_stock_summary"
[0m15:27:20.279331 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.alpha_base_data"
[0m15:27:20.279736 [debug] [Thread-3 (]: SQL status: OK in 0.004 seconds
[0m15:27:20.280092 [debug] [Thread-4 (]: On model.quant_features.alpha_base_data: BEGIN
[0m15:27:20.280437 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:27:20.280859 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.mart_technical_indicators"
[0m15:27:20.281227 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m15:27:20.281725 [debug] [Thread-2 (]: On model.quant_features.daily_stock_summary: BEGIN
[0m15:27:20.282233 [debug] [Thread-3 (]: On model.quant_features.mart_technical_indicators: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.mart_technical_indicators"} */

  
    
    

    create  table
      "quant_features"."main"."mart_technical_indicators__dbt_tmp"
  
    as (
      

with base_data as (
    select * from "quant_features"."main"."stg_ohlc_data"
),

technical_indicators as (
    select 
        symbol,
        timestamp,
        open,
        high,
        low,
        close,
        volume,
        typical_price,
        daily_range,
        daily_return,
        volume_price_ratio,
        
        -- 移动平均线
        avg(close) over (
            partition by symbol 
            order by timestamp 
            rows between 4 preceding and current row
        ) as ma_5,
        
        avg(close) over (
            partition by symbol 
            order by timestamp 
            rows between 9 preceding and current row
        ) as ma_10,
        
        avg(close) over (
            partition by symbol 
            order by timestamp 
            rows between 19 preceding and current row
        ) as ma_20,
        
        -- 波动率 (标准差)
        stddev(daily_return) over (
            partition by symbol 
            order by timestamp 
            rows between 19 preceding and current row
        ) as volatility_20d,
        
        -- RSI相关计算
        case when daily_return > 0 then daily_return else 0 end as gain,
        case when daily_return < 0 then abs(daily_return) else 0 end as loss,
        
        -- 价格位置指标
        (close - min(low) over (
            partition by symbol 
            order by timestamp 
            rows between 13 preceding and current row
        )) / nullif((max(high) over (
            partition by symbol 
            order by timestamp 
            rows between 13 preceding and current row
        ) - min(low) over (
            partition by symbol 
            order by timestamp 
            rows between 13 preceding and current row
        )), 0) as stoch_k_14,
        
        -- 成交量指标
        avg(volume) over (
            partition by symbol 
            order by timestamp 
            rows between 19 preceding and current row
        ) as avg_volume_20d
        
    from base_data
),

rsi_calculation as (
    select *,
        -- RSI计算
        avg(gain) over (
            partition by symbol 
            order by timestamp 
            rows between 13 preceding and current row
        ) as avg_gain_14,
        
        avg(loss) over (
            partition by symbol 
            order by timestamp 
            rows between 13 preceding and current row
        ) as avg_loss_14
    from technical_indicators
),

final_indicators as (
    select *,
        case 
            when avg_loss_14 = 0 then 100
            when avg_gain_14 = 0 then 0
            else 100 - (100 / (1 + (avg_gain_14 / avg_loss_14)))
        end as rsi_14,
        
        -- 布林带
        ma_20 + (2 * stddev(close) over (
            partition by symbol 
            order by timestamp 
            rows between 19 preceding and current row
        )) as bollinger_upper,
        
        ma_20 - (2 * stddev(close) over (
            partition by symbol 
            order by timestamp 
            rows between 19 preceding and current row
        )) as bollinger_lower,
        
        -- 价格动量
        case when lag(close, 5) over (partition by symbol order by timestamp) != 0 
            then (close - lag(close, 5) over (partition by symbol order by timestamp)) / 
                 lag(close, 5) over (partition by symbol order by timestamp)
            else 0 
        end as momentum_5d,
        
        case when lag(close, 10) over (partition by symbol order by timestamp) != 0 
            then (close - lag(close, 10) over (partition by symbol order by timestamp)) / 
                 lag(close, 10) over (partition by symbol order by timestamp)
            else 0 
        end as momentum_10d
        
    from rsi_calculation
)

select * from final_indicators
    );
  
  
[0m15:27:20.282922 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:27:20.283291 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m15:27:20.284098 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.alpha_base_data"
[0m15:27:20.284587 [debug] [Thread-4 (]: On model.quant_features.alpha_base_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_base_data"} */

  
    
    

    create  table
      "quant_features"."main"."alpha_base_data__dbt_tmp"
  
    as (
      

-- Alpha 101 基础数据准备
-- 为Alpha因子计算准备所有必要的基础数据

WITH base_ohlc AS (
    SELECT 
        symbol,
        timestamp,
        open,
        high,
        low,
        close,
        volume,
        -- 计算VWAP (简化版本，假设等权重)
        (high + low + close) / 3 AS vwap,
        -- 计算returns
        CASE 
            WHEN LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp) IS NOT NULL
            THEN (close - LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp)) / 
                 LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp)
            ELSE 0
        END AS returns
    FROM "quant_features"."main"."stg_ohlc_data"
    WHERE timestamp >= '2020-01-01'
      AND timestamp <= '2024-12-31'
),

enhanced_data AS (
    SELECT 
        *,
        -- 计算ADV (Average Daily Volume)
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )

 AS adv20,
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )

 AS adv10,
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )

 AS adv5,
        
        -- 预计算一些常用的时间序列指标
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_ma5,
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS close_ma10,
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_ma20,
        
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS volume_ma20,
        
        -- 预计算滚动标准差
        
    STDDEV(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_std20,
        
    STDDEV(returns) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS returns_std20,
        
        -- 预计算一些延迟项
        
    LAG(close, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag1,
        
    LAG(close, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag2,
        
    LAG(close, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag5,
        
    LAG(close, 10) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag10,
        
    LAG(close, 20) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag20,
        
    LAG(volume, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS volume_lag1,
        
    LAG(high, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS high_lag1,
        
    LAG(low, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS low_lag1,
        
    LAG(vwap, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS vwap_lag5,
        
        -- 预计算一些差值项
        
    close - 
    LAG(close, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta1,
        
    close - 
    LAG(close, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta2,
        
    close - 
    LAG(close, 3) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta3,
        
    close - 
    LAG(close, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta5,
        
    close - 
    LAG(close, 7) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta7,
        
    close - 
    LAG(close, 10) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta10,
        
    volume - 
    LAG(volume, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS volume_delta1,
        
    volume - 
    LAG(volume, 3) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS volume_delta3,
        
    high - 
    LAG(high, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS high_delta2,
        
        -- 预计算一些排序项
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY close
    )
 AS close_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY volume
    )
 AS volume_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY high
    )
 AS high_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY low
    )
 AS low_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY vwap
    )
 AS vwap_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY returns
    )
 AS returns_rank,
        
        -- 预计算时间序列排序
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY close
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS close_ts_rank10,
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY volume
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_ts_rank5,
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY high
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS high_ts_rank5,
        
        -- 预计算一些最值项
        
    MIN(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 99 PRECEDING AND CURRENT ROW
    )
 AS close_min100,
        
    MAX(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS close_max3,
        
    MIN(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_min5,
        
    MAX(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_max5,
        
    MIN(low) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS low_min5,
        
    MAX(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS high_max3,
        
        -- 预计算一些求和项
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_sum5,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 7 PRECEDING AND CURRENT ROW
    )
 AS close_sum8,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_sum20,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 99 PRECEDING AND CURRENT ROW
    )
 AS close_sum100,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 199 PRECEDING AND CURRENT ROW
    )
 AS close_sum200,
        
    SUM(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_sum5,
        
    SUM(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS volume_sum20,
        
    SUM(returns) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 249 PRECEDING AND CURRENT ROW
    )
 AS returns_sum250,
        
    SUM(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS high_sum5,
        
    SUM(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS high_sum20,
        
        -- 预计算一些相关性
        
    -- 使用DuckDB的CORR窗口函数
    CORR(close, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS corr_close_volume_10,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(open, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS corr_open_volume_10,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(high, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS corr_high_volume_5,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(vwap, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 5 PRECEDING AND CURRENT ROW
    )
 AS corr_vwap_volume_6,
        
        -- 预计算一些协方差
        
    COVAR_SAMP(close, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS cov_close_volume_5,
        
    COVAR_SAMP(high, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS cov_high_volume_5
        
    FROM base_ohlc
    WHERE timestamp >= '2020-01-01' - INTERVAL '250 days'  -- 扩展时间范围以确保有足够的历史数据
),

-- 过滤回原始时间范围
final_data AS (
    SELECT *
    FROM enhanced_data
    WHERE timestamp >= '2020-01-01'
      AND timestamp <= '2024-12-31'
)

SELECT * FROM final_data
    );
  
  
[0m15:27:20.285755 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m15:27:20.286068 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:27:20.286382 [debug] [Thread-2 (]: On model.quant_features.daily_stock_summary: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.daily_stock_summary"} */

  
    
    

    create  table
      "quant_features"."main"."daily_stock_summary__dbt_tmp"
  
    as (
      

with price_data as (
    select * from "quant_features"."main"."stg_stock_prices"
),

market_data as (
    select * from "quant_features"."main"."stg_market_data"
),

combined_data as (
    select
        p.date,
        p.symbol,
        p.open,
        p.high,
        p.low,
        p.close,
        p.volume,
        p.daily_range,
        p.daily_change,
        p.daily_return,
        m.market_cap,
        m.pe_ratio,
        m.dividend_yield,
        m.sector,
        m.estimated_earnings,
        m.estimated_dividend_payout,
        -- 计算额外的技术指标
        (p.high + p.low + p.close) / 3 as typical_price,
        p.volume * p.close as dollar_volume,
        case 
            when p.daily_return > 0.05 then 'Strong Up'
            when p.daily_return > 0.02 then 'Up'
            when p.daily_return > -0.02 then 'Flat'
            when p.daily_return > -0.05 then 'Down'
            else 'Strong Down'
        end as price_movement_category
    from price_data p
    left join market_data m
        on p.date = m.date
        and p.symbol = m.symbol
)

select * from combined_data
    );
  
  
[0m15:27:20.287316 [debug] [Thread-4 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_base_data"} */

  
    
    

    create  table
      "quant_features"."main"."alpha_base_data__dbt_tmp"
  
    as (
      

-- Alpha 101 基础数据准备
-- 为Alpha因子计算准备所有必要的基础数据

WITH base_ohlc AS (
    SELECT 
        symbol,
        timestamp,
        open,
        high,
        low,
        close,
        volume,
        -- 计算VWAP (简化版本，假设等权重)
        (high + low + close) / 3 AS vwap,
        -- 计算returns
        CASE 
            WHEN LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp) IS NOT NULL
            THEN (close - LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp)) / 
                 LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp)
            ELSE 0
        END AS returns
    FROM "quant_features"."main"."stg_ohlc_data"
    WHERE timestamp >= '2020-01-01'
      AND timestamp <= '2024-12-31'
),

enhanced_data AS (
    SELECT 
        *,
        -- 计算ADV (Average Daily Volume)
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )

 AS adv20,
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )

 AS adv10,
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )

 AS adv5,
        
        -- 预计算一些常用的时间序列指标
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_ma5,
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS close_ma10,
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_ma20,
        
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS volume_ma20,
        
        -- 预计算滚动标准差
        
    STDDEV(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_std20,
        
    STDDEV(returns) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS returns_std20,
        
        -- 预计算一些延迟项
        
    LAG(close, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag1,
        
    LAG(close, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag2,
        
    LAG(close, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag5,
        
    LAG(close, 10) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag10,
        
    LAG(close, 20) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag20,
        
    LAG(volume, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS volume_lag1,
        
    LAG(high, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS high_lag1,
        
    LAG(low, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS low_lag1,
        
    LAG(vwap, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS vwap_lag5,
        
        -- 预计算一些差值项
        
    close - 
    LAG(close, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta1,
        
    close - 
    LAG(close, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta2,
        
    close - 
    LAG(close, 3) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta3,
        
    close - 
    LAG(close, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta5,
        
    close - 
    LAG(close, 7) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta7,
        
    close - 
    LAG(close, 10) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta10,
        
    volume - 
    LAG(volume, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS volume_delta1,
        
    volume - 
    LAG(volume, 3) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS volume_delta3,
        
    high - 
    LAG(high, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS high_delta2,
        
        -- 预计算一些排序项
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY close
    )
 AS close_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY volume
    )
 AS volume_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY high
    )
 AS high_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY low
    )
 AS low_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY vwap
    )
 AS vwap_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY returns
    )
 AS returns_rank,
        
        -- 预计算时间序列排序
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY close
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS close_ts_rank10,
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY volume
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_ts_rank5,
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY high
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS high_ts_rank5,
        
        -- 预计算一些最值项
        
    MIN(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 99 PRECEDING AND CURRENT ROW
    )
 AS close_min100,
        
    MAX(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS close_max3,
        
    MIN(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_min5,
        
    MAX(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_max5,
        
    MIN(low) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS low_min5,
        
    MAX(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS high_max3,
        
        -- 预计算一些求和项
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_sum5,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 7 PRECEDING AND CURRENT ROW
    )
 AS close_sum8,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_sum20,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 99 PRECEDING AND CURRENT ROW
    )
 AS close_sum100,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 199 PRECEDING AND CURRENT ROW
    )
 AS close_sum200,
        
    SUM(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_sum5,
        
    SUM(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS volume_sum20,
        
    SUM(returns) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 249 PRECEDING AND CURRENT ROW
    )
 AS returns_sum250,
        
    SUM(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS high_sum5,
        
    SUM(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS high_sum20,
        
        -- 预计算一些相关性
        
    -- 使用DuckDB的CORR窗口函数
    CORR(close, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS corr_close_volume_10,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(open, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS corr_open_volume_10,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(high, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS corr_high_volume_5,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(vwap, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 5 PRECEDING AND CURRENT ROW
    )
 AS corr_vwap_volume_6,
        
        -- 预计算一些协方差
        
    COVAR_SAMP(close, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS cov_close_volume_5,
        
    COVAR_SAMP(high, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS cov_high_volume_5
        
    FROM base_ohlc
    WHERE timestamp >= '2020-01-01' - INTERVAL '250 days'  -- 扩展时间范围以确保有足够的历史数据
),

-- 过滤回原始时间范围
final_data AS (
    SELECT *
    FROM enhanced_data
    WHERE timestamp >= '2020-01-01'
      AND timestamp <= '2024-12-31'
)

SELECT * FROM final_data
    );
  
  
[0m15:27:20.287984 [debug] [Thread-4 (]: DuckDB adapter: Rolling back transaction.
[0m15:27:20.288307 [debug] [Thread-4 (]: On model.quant_features.alpha_base_data: ROLLBACK
[0m15:27:20.291607 [debug] [Thread-4 (]: Failed to rollback 'model.quant_features.alpha_base_data'
[0m15:27:20.291931 [debug] [Thread-4 (]: On model.quant_features.alpha_base_data: Close
[0m15:27:20.292472 [debug] [Thread-2 (]: SQL status: OK in 0.006 seconds
[0m15:27:20.295281 [debug] [Thread-4 (]: Runtime Error in model alpha_base_data (models/alpha101/alpha_base_data.sql)
  Binder Error: Could not choose a best candidate function for the function call "-(STRING_LITERAL, INTERVAL)". In order to select one, please add explicit type casts.
  	Candidate functions:
  	-(DATE, INTERVAL) -> TIMESTAMP
  	-(TIME, INTERVAL) -> TIME
  	-(TIMESTAMP, INTERVAL) -> TIMESTAMP
  	-(TIME WITH TIME ZONE, INTERVAL) -> TIME WITH TIME ZONE
  	-(TIMESTAMP WITH TIME ZONE, INTERVAL) -> TIMESTAMP WITH TIME ZONE
  	-(INTERVAL, INTERVAL) -> INTERVAL
  
  
  LINE 475:     WHERE timestamp >= '2020-01-01' - INTERVAL '250 days'  -- 扩展时间范围以确保有足够的历史数...
                                                ^
[0m15:27:20.297214 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:27:20.297779 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '546a7400-bea5-4d62-9693-c27be28811e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90c8f8c3d0>]}
[0m15:27:20.298178 [debug] [Thread-3 (]: SQL status: OK in 0.015 seconds
[0m15:27:20.298512 [debug] [Thread-2 (]: On model.quant_features.daily_stock_summary: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.daily_stock_summary"} */
alter table "quant_features"."main"."daily_stock_summary" rename to "daily_stock_summary__dbt_backup"
[0m15:27:20.299003 [error] [Thread-4 (]: 4 of 15 ERROR creating sql table model main.alpha_base_data .................... [[31mERROR[0m in 0.07s]
[0m15:27:20.301779 [debug] [Thread-4 (]: Finished running node model.quant_features.alpha_base_data
[0m15:27:20.300906 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.mart_technical_indicators"
[0m15:27:20.302099 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:27:20.302663 [debug] [Thread-7 (]: Marking all children of 'model.quant_features.alpha_base_data' to be skipped because of status 'error'.  Reason: Runtime Error in model alpha_base_data (models/alpha101/alpha_base_data.sql)
  Binder Error: Could not choose a best candidate function for the function call "-(STRING_LITERAL, INTERVAL)". In order to select one, please add explicit type casts.
  	Candidate functions:
  	-(DATE, INTERVAL) -> TIMESTAMP
  	-(TIME, INTERVAL) -> TIME
  	-(TIMESTAMP, INTERVAL) -> TIMESTAMP
  	-(TIME WITH TIME ZONE, INTERVAL) -> TIME WITH TIME ZONE
  	-(TIMESTAMP WITH TIME ZONE, INTERVAL) -> TIMESTAMP WITH TIME ZONE
  	-(INTERVAL, INTERVAL) -> INTERVAL
  
  
  LINE 475:     WHERE timestamp >= '2020-01-01' - INTERVAL '250 days'  -- 扩展时间范围以确保有足够的历史数...
                                                ^.
[0m15:27:20.303027 [debug] [Thread-3 (]: On model.quant_features.mart_technical_indicators: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.mart_technical_indicators"} */
alter table "quant_features"."main"."mart_technical_indicators__dbt_tmp" rename to "mart_technical_indicators"
[0m15:27:20.305162 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:27:20.306419 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_factors_001_020
[0m15:27:20.306818 [debug] [Thread-4 (]: Began running node model.quant_features.alpha_factors_021_050
[0m15:27:20.307225 [debug] [Thread-2 (]: On model.quant_features.daily_stock_summary: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.daily_stock_summary"} */
alter table "quant_features"."main"."daily_stock_summary__dbt_tmp" rename to "daily_stock_summary"
[0m15:27:20.307609 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m15:27:20.307945 [info ] [Thread-1 (]: 7 of 15 SKIP relation main.alpha_factors_001_020 ............................... [[33mSKIP[0m]
[0m15:27:20.308301 [info ] [Thread-4 (]: 8 of 15 SKIP relation main.alpha_factors_021_050 ............................... [[33mSKIP[0m]
[0m15:27:20.311660 [debug] [Thread-3 (]: On model.quant_features.mart_technical_indicators: COMMIT
[0m15:27:20.312098 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_factors_001_020
[0m15:27:20.312510 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m15:27:20.312814 [debug] [Thread-4 (]: Finished running node model.quant_features.alpha_factors_021_050
[0m15:27:20.313318 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.mart_technical_indicators"
[0m15:27:20.313690 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_factors_051_075
[0m15:27:20.314857 [debug] [Thread-2 (]: On model.quant_features.daily_stock_summary: COMMIT
[0m15:27:20.315184 [debug] [Thread-4 (]: Began running node model.quant_features.alpha_factors_076_101
[0m15:27:20.315470 [debug] [Thread-3 (]: On model.quant_features.mart_technical_indicators: COMMIT
[0m15:27:20.315757 [info ] [Thread-1 (]: 9 of 15 SKIP relation main.alpha_factors_051_075 ............................... [[33mSKIP[0m]
[0m15:27:20.316060 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:27:20.316333 [info ] [Thread-4 (]: 10 of 15 SKIP relation main.alpha_factors_076_101 .............................. [[33mSKIP[0m]
[0m15:27:20.316795 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_factors_051_075
[0m15:27:20.317127 [debug] [Thread-2 (]: On model.quant_features.daily_stock_summary: COMMIT
[0m15:27:20.317803 [debug] [Thread-4 (]: Finished running node model.quant_features.alpha_factors_076_101
[0m15:27:20.318057 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_factors_advanced
[0m15:27:20.318459 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m15:27:20.319017 [info ] [Thread-1 (]: 11 of 15 SKIP relation main.alpha_factors_advanced ............................. [[33mSKIP[0m]
[0m15:27:20.319452 [debug] [Thread-4 (]: Began running node model.quant_features.alpha101_complete
[0m15:27:20.320924 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.mart_technical_indicators"
[0m15:27:20.321291 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m15:27:20.321860 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_factors_advanced
[0m15:27:20.322284 [info ] [Thread-4 (]: 12 of 15 SKIP relation main.alpha101_complete .................................. [[33mSKIP[0m]
[0m15:27:20.322658 [debug] [Thread-3 (]: On model.quant_features.mart_technical_indicators: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.mart_technical_indicators"} */

      drop table if exists "quant_features"."main"."mart_technical_indicators__dbt_backup" cascade
    
[0m15:27:20.324516 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:27:20.325260 [debug] [Thread-4 (]: Finished running node model.quant_features.alpha101_complete
[0m15:27:20.325859 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_factors_final
[0m15:27:20.326219 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m15:27:20.326723 [debug] [Thread-2 (]: On model.quant_features.daily_stock_summary: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.daily_stock_summary"} */

      drop table if exists "quant_features"."main"."daily_stock_summary__dbt_backup" cascade
    
[0m15:27:20.327146 [info ] [Thread-1 (]: 13 of 15 SKIP relation main.alpha_factors_final ................................ [[33mSKIP[0m]
[0m15:27:20.328100 [debug] [Thread-3 (]: On model.quant_features.mart_technical_indicators: Close
[0m15:27:20.328711 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_factors_final
[0m15:27:20.330150 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '546a7400-bea5-4d62-9693-c27be28811e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90c810b850>]}
[0m15:27:20.330515 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m15:27:20.330997 [info ] [Thread-3 (]: 5 of 15 OK created sql table model main.mart_technical_indicators .............. [[32mOK[0m in 0.09s]
[0m15:27:20.331963 [debug] [Thread-2 (]: On model.quant_features.daily_stock_summary: Close
[0m15:27:20.332460 [debug] [Thread-3 (]: Finished running node model.quant_features.mart_technical_indicators
[0m15:27:20.333022 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '546a7400-bea5-4d62-9693-c27be28811e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90ca44c3b0>]}
[0m15:27:20.333386 [debug] [Thread-4 (]: Began running node model.quant_features.features_ohlc_technical
[0m15:27:20.334039 [info ] [Thread-2 (]: 6 of 15 OK created sql table model main.daily_stock_summary .................... [[32mOK[0m in 0.07s]
[0m15:27:20.334494 [info ] [Thread-4 (]: 14 of 15 START sql table model main.features_ohlc_technical .................... [RUN]
[0m15:27:20.334878 [debug] [Thread-2 (]: Finished running node model.quant_features.daily_stock_summary
[0m15:27:20.335305 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.quant_features.alpha_base_data, now model.quant_features.features_ohlc_technical)
[0m15:27:20.335711 [debug] [Thread-4 (]: Began compiling node model.quant_features.features_ohlc_technical
[0m15:27:20.337847 [debug] [Thread-4 (]: Writing injected SQL for node "model.quant_features.features_ohlc_technical"
[0m15:27:20.338452 [debug] [Thread-1 (]: Began running node model.quant_features.stock_features
[0m15:27:20.338796 [debug] [Thread-4 (]: Began executing node model.quant_features.features_ohlc_technical
[0m15:27:20.339187 [info ] [Thread-1 (]: 15 of 15 START sql table model main.stock_features ............................. [RUN]
[0m15:27:20.341399 [debug] [Thread-4 (]: Writing runtime sql for node "model.quant_features.features_ohlc_technical"
[0m15:27:20.341988 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.quant_features.stg_market_data, now model.quant_features.stock_features)
[0m15:27:20.342427 [debug] [Thread-1 (]: Began compiling node model.quant_features.stock_features
[0m15:27:20.344388 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.stock_features"
[0m15:27:20.344790 [debug] [Thread-1 (]: Began executing node model.quant_features.stock_features
[0m15:27:20.346808 [debug] [Thread-1 (]: Writing runtime sql for node "model.quant_features.stock_features"
[0m15:27:20.347273 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:27:20.347650 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.features_ohlc_technical"
[0m15:27:20.347957 [debug] [Thread-1 (]: On model.quant_features.stock_features: BEGIN
[0m15:27:20.348260 [debug] [Thread-4 (]: On model.quant_features.features_ohlc_technical: BEGIN
[0m15:27:20.348794 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:27:20.349286 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m15:27:20.350071 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:27:20.350376 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:27:20.350661 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m15:27:20.351028 [debug] [Thread-1 (]: On model.quant_features.stock_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stock_features"} */

  
    
    

    create  table
      "quant_features"."main"."stock_features__dbt_tmp"
  
    as (
      

with daily_data as (
    select * from "quant_features"."main"."daily_stock_summary"
),

windowed_features as (
    select
        *,
        -- 移动平均
        avg(close) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        ) as ma_5d,
        
        avg(close) over (
            partition by symbol 
            order by date 
            rows between 9 preceding and current row
        ) as ma_10d,
        
        -- 波动率（标准差）
        stddev(daily_return) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        ) as volatility_5d,
        
        -- 价格相对位置
        (close - min(low) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        )) / (max(high) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        ) - min(low) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        )) as price_position_5d,
        
        -- 成交量相对强度
        volume / avg(volume) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        ) as volume_ratio_5d
        
    from daily_data
),

final_features as (
    select
        *,
        -- 技术信号
        case 
            when close > ma_5d and ma_5d > ma_10d then 'Bullish'
            when close < ma_5d and ma_5d < ma_10d then 'Bearish'
            else 'Neutral'
        end as trend_signal,
        
        case 
            when volatility_5d > 0.03 then 'High'
            when volatility_5d > 0.01 then 'Medium'
            else 'Low'
        end as volatility_category
        
    from windowed_features
)

select * from final_features
    );
  
  
[0m15:27:20.351388 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.features_ohlc_technical"
[0m15:27:20.351927 [debug] [Thread-4 (]: On model.quant_features.features_ohlc_technical: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.features_ohlc_technical"} */

  
    
    

    create  table
      "quant_features"."main"."features_ohlc_technical__dbt_tmp"
  
    as (
      

with technical_data as (
    select * from "quant_features"."main"."mart_technical_indicators"
),

feature_engineering as (
    select 
        symbol,
        timestamp,
        
        -- 基础价格特征
        close as price,
        daily_return,
        volatility_20d,
        
        -- 趋势特征
        ma_5,
        ma_10,
        ma_20,
        case when close > ma_5 then 1 else 0 end as price_above_ma5,
        case when close > ma_10 then 1 else 0 end as price_above_ma10,
        case when close > ma_20 then 1 else 0 end as price_above_ma20,
        case when ma_5 > ma_10 then 1 else 0 end as ma5_above_ma10,
        case when ma_10 > ma_20 then 1 else 0 end as ma10_above_ma20,
        
        -- 技术指标特征
        rsi_14,
        case when rsi_14 > 70 then 1 else 0 end as rsi_overbought,
        case when rsi_14 < 30 then 1 else 0 end as rsi_oversold,
        
        stoch_k_14,
        case when stoch_k_14 > 0.8 then 1 else 0 end as stoch_overbought,
        case when stoch_k_14 < 0.2 then 1 else 0 end as stoch_oversold,
        
        -- 布林带特征
        bollinger_upper,
        bollinger_lower,
        case when close > bollinger_upper then 1 else 0 end as price_above_bb_upper,
        case when close < bollinger_lower then 1 else 0 end as price_below_bb_lower,
        case 
            when bollinger_upper - bollinger_lower != 0 
            then (close - bollinger_lower) / (bollinger_upper - bollinger_lower)
            else 0.5
        end as bb_position,
        
        -- 动量特征
        momentum_5d,
        momentum_10d,
        case when momentum_5d > 0 then 1 else 0 end as momentum_5d_positive,
        case when momentum_10d > 0 then 1 else 0 end as momentum_10d_positive,
        
        -- 成交量特征
        volume,
        avg_volume_20d,
        case when avg_volume_20d != 0 then volume / avg_volume_20d else 0 end as volume_ratio,
        case when volume > avg_volume_20d * 1.5 then 1 else 0 end as high_volume,
        
        -- 价格范围特征
        daily_range,
        case when lag(close) over (partition by symbol order by timestamp) != 0 
            then daily_range / lag(close) over (partition by symbol order by timestamp)
            else 0
        end as range_ratio,
        
        -- 组合特征
        case when rsi_14 > 70 and stoch_k_14 > 0.8 then 1 else 0 end as double_overbought,
        case when rsi_14 < 30 and stoch_k_14 < 0.2 then 1 else 0 end as double_oversold,
        
        -- 时间特征
        extract(hour from timestamp) as hour,
        extract(dow from timestamp) as day_of_week,
        extract(month from timestamp) as month,
        
        -- 标识特征用于Feast
        concat(symbol, '_', date_trunc('day', timestamp)::string) as entity_id,
        timestamp as event_timestamp
        
    from technical_data
    where timestamp >= current_date - interval '20' days
)

select * from feature_engineering
    );
  
  
[0m15:27:20.355005 [debug] [Thread-4 (]: SQL status: OK in 0.003 seconds
[0m15:27:20.356799 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.features_ohlc_technical"
[0m15:27:20.357064 [debug] [Thread-4 (]: On model.quant_features.features_ohlc_technical: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.features_ohlc_technical"} */
alter table "quant_features"."main"."features_ohlc_technical__dbt_tmp" rename to "features_ohlc_technical"
[0m15:27:20.357612 [debug] [Thread-1 (]: SQL status: OK in 0.006 seconds
[0m15:27:20.359424 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:27:20.359727 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m15:27:20.360040 [debug] [Thread-1 (]: On model.quant_features.stock_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stock_features"} */
alter table "quant_features"."main"."stock_features" rename to "stock_features__dbt_backup"
[0m15:27:20.361008 [debug] [Thread-4 (]: On model.quant_features.features_ohlc_technical: COMMIT
[0m15:27:20.361646 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.features_ohlc_technical"
[0m15:27:20.362000 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:27:20.362577 [debug] [Thread-4 (]: On model.quant_features.features_ohlc_technical: COMMIT
[0m15:27:20.364673 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:27:20.365061 [debug] [Thread-1 (]: On model.quant_features.stock_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stock_features"} */
alter table "quant_features"."main"."stock_features__dbt_tmp" rename to "stock_features"
[0m15:27:20.365807 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:27:20.366840 [debug] [Thread-1 (]: On model.quant_features.stock_features: COMMIT
[0m15:27:20.367093 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:27:20.367358 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m15:27:20.367643 [debug] [Thread-1 (]: On model.quant_features.stock_features: COMMIT
[0m15:27:20.369025 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.features_ohlc_technical"
[0m15:27:20.369438 [debug] [Thread-4 (]: On model.quant_features.features_ohlc_technical: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.features_ohlc_technical"} */

      drop table if exists "quant_features"."main"."features_ohlc_technical__dbt_backup" cascade
    
[0m15:27:20.371090 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:27:20.372742 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:27:20.373000 [debug] [Thread-1 (]: On model.quant_features.stock_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stock_features"} */

      drop table if exists "quant_features"."main"."stock_features__dbt_backup" cascade
    
[0m15:27:20.373440 [debug] [Thread-4 (]: SQL status: OK in 0.004 seconds
[0m15:27:20.374503 [debug] [Thread-4 (]: On model.quant_features.features_ohlc_technical: Close
[0m15:27:20.374821 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:27:20.376215 [debug] [Thread-1 (]: On model.quant_features.stock_features: Close
[0m15:27:20.376619 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '546a7400-bea5-4d62-9693-c27be28811e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90c9b15e50>]}
[0m15:27:20.377156 [info ] [Thread-4 (]: 14 of 15 OK created sql table model main.features_ohlc_technical ............... [[32mOK[0m in 0.04s]
[0m15:27:20.377497 [debug] [Thread-4 (]: Finished running node model.quant_features.features_ohlc_technical
[0m15:27:20.377937 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '546a7400-bea5-4d62-9693-c27be28811e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90c9b15c70>]}
[0m15:27:20.378542 [info ] [Thread-1 (]: 15 of 15 OK created sql table model main.stock_features ........................ [[32mOK[0m in 0.04s]
[0m15:27:20.378901 [debug] [Thread-1 (]: Finished running node model.quant_features.stock_features
[0m15:27:20.380396 [debug] [MainThread]: Using duckdb connection "master"
[0m15:27:20.380628 [debug] [MainThread]: On master: BEGIN
[0m15:27:20.380788 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:27:20.381147 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:27:20.381362 [debug] [MainThread]: On master: COMMIT
[0m15:27:20.381523 [debug] [MainThread]: Using duckdb connection "master"
[0m15:27:20.381671 [debug] [MainThread]: On master: COMMIT
[0m15:27:20.381951 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:27:20.382121 [debug] [MainThread]: On master: Close
[0m15:27:20.382403 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:27:20.382595 [debug] [MainThread]: Connection 'model.quant_features.stock_features' was properly closed.
[0m15:27:20.382741 [debug] [MainThread]: Connection 'model.quant_features.mart_technical_indicators' was properly closed.
[0m15:27:20.382876 [debug] [MainThread]: Connection 'model.quant_features.daily_stock_summary' was properly closed.
[0m15:27:20.383007 [debug] [MainThread]: Connection 'model.quant_features.features_ohlc_technical' was properly closed.
[0m15:27:20.383232 [info ] [MainThread]: 
[0m15:27:20.383431 [info ] [MainThread]: Finished running 12 table models, 3 view models in 0 hours 0 minutes and 0.32 seconds (0.32s).
[0m15:27:20.384370 [debug] [MainThread]: Command end result
[0m15:27:20.406455 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:27:20.407699 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:27:20.411628 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:27:20.411868 [info ] [MainThread]: 
[0m15:27:20.412075 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:27:20.412262 [info ] [MainThread]: 
[0m15:27:20.412490 [error] [MainThread]: [31mFailure in model alpha_base_data (models/alpha101/alpha_base_data.sql)[0m
[0m15:27:20.412707 [error] [MainThread]:   Runtime Error in model alpha_base_data (models/alpha101/alpha_base_data.sql)
  Binder Error: Could not choose a best candidate function for the function call "-(STRING_LITERAL, INTERVAL)". In order to select one, please add explicit type casts.
  	Candidate functions:
  	-(DATE, INTERVAL) -> TIMESTAMP
  	-(TIME, INTERVAL) -> TIME
  	-(TIMESTAMP, INTERVAL) -> TIMESTAMP
  	-(TIME WITH TIME ZONE, INTERVAL) -> TIME WITH TIME ZONE
  	-(TIMESTAMP WITH TIME ZONE, INTERVAL) -> TIMESTAMP WITH TIME ZONE
  	-(INTERVAL, INTERVAL) -> INTERVAL
  
  
  LINE 475:     WHERE timestamp >= '2020-01-01' - INTERVAL '250 days'  -- 扩展时间范围以确保有足够的历史数...
                                                ^
[0m15:27:20.412892 [info ] [MainThread]: 
[0m15:27:20.413082 [info ] [MainThread]:   compiled code at target/compiled/quant_features/models/alpha101/alpha_base_data.sql
[0m15:27:20.413249 [info ] [MainThread]: 
[0m15:27:20.413427 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=1 SKIP=7 NO-OP=0 TOTAL=15
[0m15:27:20.413977 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.0885947, "process_in_blocks": "0", "process_kernel_time": 0.156504, "process_mem_max_rss": "167864", "process_out_blocks": "5432", "process_user_time": 1.874127}
[0m15:27:20.414305 [debug] [MainThread]: Command `dbt run` failed at 15:27:20.414242 after 1.09 seconds
[0m15:27:20.414540 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90ca4a3d70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90cf6f1c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90c8fbe7b0>]}
[0m15:27:20.414757 [debug] [MainThread]: Flushing usage events
[0m15:27:20.471403 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:27:27.655178 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c61697770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c62cf1a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c6050bd90>]}


============================== 15:27:27.658659 | 35d3798b-4af2-4e77-8cc6-1830e26e304e ==============================
[0m15:27:27.658659 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:27:27.659033 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'printer_width': '80', 'static_parser': 'True', 'warn_error': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'debug': 'False', 'invocation_command': 'dbt run --select stg_stock_prices stg_market_data daily_stock_summary stock_features', 'use_colors': 'True', 'no_print': 'None', 'log_path': '/workspace/dbt_project/logs', 'empty': 'False', 'target_path': 'None', 'partial_parse': 'True', 'write_json': 'True', 'cache_selected_only': 'False', 'introspect': 'True', 'indirect_selection': 'eager', 'version_check': 'True', 'use_experimental_parser': 'False', 'log_format': 'default', 'log_cache_events': 'False', 'profiles_dir': '/workspace/dbt_project'}
[0m15:27:27.807454 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '35d3798b-4af2-4e77-8cc6-1830e26e304e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c61222650>]}
[0m15:27:27.849175 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '35d3798b-4af2-4e77-8cc6-1830e26e304e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c60606be0>]}
[0m15:27:27.850341 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:27:27.881040 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:27:27.971116 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:27:27.971371 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:27:28.008633 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '35d3798b-4af2-4e77-8cc6-1830e26e304e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c5ce1d450>]}
[0m15:27:28.081504 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:27:28.082808 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:27:28.093485 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '35d3798b-4af2-4e77-8cc6-1830e26e304e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c5ce51220>]}
[0m15:27:28.093827 [info ] [MainThread]: Found 15 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:27:28.094036 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '35d3798b-4af2-4e77-8cc6-1830e26e304e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c5ce90130>]}
[0m15:27:28.095779 [info ] [MainThread]: 
[0m15:27:28.096053 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:27:28.096229 [info ] [MainThread]: 
[0m15:27:28.096612 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:27:28.099893 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features'
[0m15:27:28.118373 [debug] [ThreadPool]: Using duckdb connection "list_quant_features"
[0m15:27:28.118644 [debug] [ThreadPool]: On list_quant_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"quant_features"'
    
  
  
[0m15:27:28.118838 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:27:28.131380 [debug] [ThreadPool]: SQL status: OK in 0.012 seconds
[0m15:27:28.132287 [debug] [ThreadPool]: On list_quant_features: Close
[0m15:27:28.133213 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_quant_features, now create_quant_features_main)
[0m15:27:28.133613 [debug] [ThreadPool]: Creating schema "database: "quant_features"
schema: "main"
"
[0m15:27:28.137921 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:27:28.138180 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='quant_features'
        and type='sqlite'
    
  
[0m15:27:28.138372 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:27:28.139410 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:27:28.140246 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:27:28.140490 [debug] [ThreadPool]: On create_quant_features_main: BEGIN
[0m15:27:28.140805 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:27:28.140979 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:27:28.141133 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
    
        create schema if not exists "quant_features"."main"
    
[0m15:27:28.141460 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:27:28.141991 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:27:28.142193 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:27:28.142365 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:27:28.142659 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:27:28.142829 [debug] [ThreadPool]: On create_quant_features_main: Close
[0m15:27:28.144566 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_quant_features_main, now list_quant_features_main)
[0m15:27:28.187382 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:27:28.187629 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:27:28.187791 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:27:28.188183 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:27:28.188392 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:27:28.188552 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:27:28.195138 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m15:27:28.196086 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:27:28.196762 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:27:28.196983 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:27:28.198925 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '35d3798b-4af2-4e77-8cc6-1830e26e304e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c5b6069c0>]}
[0m15:27:28.199375 [debug] [MainThread]: Using duckdb connection "master"
[0m15:27:28.199575 [debug] [MainThread]: On master: BEGIN
[0m15:27:28.199728 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:27:28.200124 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:27:28.200323 [debug] [MainThread]: On master: COMMIT
[0m15:27:28.200494 [debug] [MainThread]: Using duckdb connection "master"
[0m15:27:28.200639 [debug] [MainThread]: On master: COMMIT
[0m15:27:28.200896 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:27:28.201076 [debug] [MainThread]: On master: Close
[0m15:27:28.203618 [debug] [Thread-1 (]: Began running node model.quant_features.stg_market_data
[0m15:27:28.204011 [debug] [Thread-2 (]: Began running node model.quant_features.stg_stock_prices
[0m15:27:28.204497 [info ] [Thread-1 (]: 1 of 4 START sql view model main.stg_market_data ............................... [RUN]
[0m15:27:28.205087 [info ] [Thread-2 (]: 2 of 4 START sql view model main.stg_stock_prices .............................. [RUN]
[0m15:27:28.205536 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.stg_market_data)
[0m15:27:28.206038 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.quant_features.stg_stock_prices'
[0m15:27:28.206439 [debug] [Thread-1 (]: Began compiling node model.quant_features.stg_market_data
[0m15:27:28.206803 [debug] [Thread-2 (]: Began compiling node model.quant_features.stg_stock_prices
[0m15:27:28.211653 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.stg_market_data"
[0m15:27:28.214027 [debug] [Thread-2 (]: Writing injected SQL for node "model.quant_features.stg_stock_prices"
[0m15:27:28.214682 [debug] [Thread-1 (]: Began executing node model.quant_features.stg_market_data
[0m15:27:28.221785 [debug] [Thread-2 (]: Began executing node model.quant_features.stg_stock_prices
[0m15:27:28.241421 [debug] [Thread-2 (]: Writing runtime sql for node "model.quant_features.stg_stock_prices"
[0m15:27:28.242505 [debug] [Thread-1 (]: Writing runtime sql for node "model.quant_features.stg_market_data"
[0m15:27:28.243321 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:27:28.243597 [debug] [Thread-2 (]: On model.quant_features.stg_stock_prices: BEGIN
[0m15:27:28.243788 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:27:28.244334 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:27:28.244630 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: BEGIN
[0m15:27:28.244820 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:27:28.245245 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:27:28.245480 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:27:28.245692 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_market_data"} */

  
  create view "quant_features"."main"."stg_market_data__dbt_tmp" as (
    

with source_data as (
    select
        date,
        symbol,
        market_cap,
        pe_ratio,
        dividend_yield,
        sector,
        -- 添加计算字段
        case 
            when pe_ratio > 0 then market_cap / pe_ratio 
            else null 
        end as estimated_earnings,
        case 
            when dividend_yield > 0 then market_cap * dividend_yield / 100 
            else 0 
        end as estimated_dividend_payout
    from "quant_features"."main"."market_data"
    where date is not null
      and symbol is not null
      and market_cap > 0
)

select * from source_data
  );

[0m15:27:28.246105 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m15:27:28.246421 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:27:28.246668 [debug] [Thread-2 (]: On model.quant_features.stg_stock_prices: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_stock_prices"} */

  
  create view "quant_features"."main"."stg_stock_prices__dbt_tmp" as (
    

with source_data as (
    select
        date,
        symbol,
        open,
        high,
        low,
        close,
        volume,
        -- 计算基础技术指标
        (high - low) as daily_range,
        (close - open) as daily_change,
        (close - open) / open as daily_return,
        -- 添加数据质量检查
        case 
            when high >= low and high >= open and high >= close 
                 and low <= open and low <= close 
            then true 
            else false 
        end as is_valid_ohlc
    from "quant_features"."main"."raw_stock_prices"
    where date is not null
      and symbol is not null
      and open > 0
      and high > 0
      and low > 0
      and close > 0
      and volume >= 0
)

select * from source_data
where is_valid_ohlc = true
  );

[0m15:27:28.246994 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:27:28.251156 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:27:28.251445 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_market_data"} */
alter view "quant_features"."main"."stg_market_data" rename to "stg_market_data__dbt_backup"
[0m15:27:28.251825 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m15:27:28.253739 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:27:28.254027 [debug] [Thread-2 (]: On model.quant_features.stg_stock_prices: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_stock_prices"} */
alter view "quant_features"."main"."stg_stock_prices" rename to "stg_stock_prices__dbt_backup"
[0m15:27:28.254451 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m15:27:28.256527 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:27:28.256794 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_market_data"} */
alter view "quant_features"."main"."stg_market_data__dbt_tmp" rename to "stg_market_data"
[0m15:27:28.257148 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m15:27:28.259034 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:27:28.259310 [debug] [Thread-2 (]: On model.quant_features.stg_stock_prices: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_stock_prices"} */
alter view "quant_features"."main"."stg_stock_prices__dbt_tmp" rename to "stg_stock_prices"
[0m15:27:28.259671 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m15:27:28.265179 [debug] [Thread-2 (]: SQL status: OK in 0.006 seconds
[0m15:27:28.269420 [debug] [Thread-2 (]: On model.quant_features.stg_stock_prices: COMMIT
[0m15:27:28.269725 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:27:28.270802 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: COMMIT
[0m15:27:28.271626 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:27:28.271271 [debug] [Thread-2 (]: On model.quant_features.stg_stock_prices: COMMIT
[0m15:27:28.271841 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: COMMIT
[0m15:27:28.274394 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m15:27:28.278002 [debug] [Thread-2 (]: Using duckdb connection "model.quant_features.stg_stock_prices"
[0m15:27:28.278283 [debug] [Thread-2 (]: On model.quant_features.stg_stock_prices: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_stock_prices"} */

      drop view if exists "quant_features"."main"."stg_stock_prices__dbt_backup" cascade
    
[0m15:27:28.278805 [debug] [Thread-1 (]: SQL status: OK in 0.006 seconds
[0m15:27:28.280855 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stg_market_data"
[0m15:27:28.281109 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stg_market_data"} */

      drop view if exists "quant_features"."main"."stg_market_data__dbt_backup" cascade
    
[0m15:27:28.281512 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m15:27:28.283145 [debug] [Thread-2 (]: On model.quant_features.stg_stock_prices: Close
[0m15:27:28.283611 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:27:28.284601 [debug] [Thread-1 (]: On model.quant_features.stg_market_data: Close
[0m15:27:28.285774 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35d3798b-4af2-4e77-8cc6-1830e26e304e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c5b5dca10>]}
[0m15:27:28.286307 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35d3798b-4af2-4e77-8cc6-1830e26e304e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c5c038f70>]}
[0m15:27:28.286857 [info ] [Thread-1 (]: 1 of 4 OK created sql view model main.stg_market_data .......................... [[32mOK[0m in 0.08s]
[0m15:27:28.287441 [info ] [Thread-2 (]: 2 of 4 OK created sql view model main.stg_stock_prices ......................... [[32mOK[0m in 0.08s]
[0m15:27:28.287864 [debug] [Thread-1 (]: Finished running node model.quant_features.stg_market_data
[0m15:27:28.288329 [debug] [Thread-2 (]: Finished running node model.quant_features.stg_stock_prices
[0m15:27:28.289934 [debug] [Thread-3 (]: Began running node model.quant_features.daily_stock_summary
[0m15:27:28.290335 [info ] [Thread-3 (]: 3 of 4 START sql table model main.daily_stock_summary .......................... [RUN]
[0m15:27:28.290693 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.quant_features.daily_stock_summary'
[0m15:27:28.290903 [debug] [Thread-3 (]: Began compiling node model.quant_features.daily_stock_summary
[0m15:27:28.293238 [debug] [Thread-3 (]: Writing injected SQL for node "model.quant_features.daily_stock_summary"
[0m15:27:28.293662 [debug] [Thread-3 (]: Began executing node model.quant_features.daily_stock_summary
[0m15:27:28.306130 [debug] [Thread-3 (]: Writing runtime sql for node "model.quant_features.daily_stock_summary"
[0m15:27:28.306594 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:27:28.306820 [debug] [Thread-3 (]: On model.quant_features.daily_stock_summary: BEGIN
[0m15:27:28.307008 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:27:28.307482 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:27:28.307708 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:27:28.307940 [debug] [Thread-3 (]: On model.quant_features.daily_stock_summary: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.daily_stock_summary"} */

  
    
    

    create  table
      "quant_features"."main"."daily_stock_summary__dbt_tmp"
  
    as (
      

with price_data as (
    select * from "quant_features"."main"."stg_stock_prices"
),

market_data as (
    select * from "quant_features"."main"."stg_market_data"
),

combined_data as (
    select
        p.date,
        p.symbol,
        p.open,
        p.high,
        p.low,
        p.close,
        p.volume,
        p.daily_range,
        p.daily_change,
        p.daily_return,
        m.market_cap,
        m.pe_ratio,
        m.dividend_yield,
        m.sector,
        m.estimated_earnings,
        m.estimated_dividend_payout,
        -- 计算额外的技术指标
        (p.high + p.low + p.close) / 3 as typical_price,
        p.volume * p.close as dollar_volume,
        case 
            when p.daily_return > 0.05 then 'Strong Up'
            when p.daily_return > 0.02 then 'Up'
            when p.daily_return > -0.02 then 'Flat'
            when p.daily_return > -0.05 then 'Down'
            else 'Strong Down'
        end as price_movement_category
    from price_data p
    left join market_data m
        on p.date = m.date
        and p.symbol = m.symbol
)

select * from combined_data
    );
  
  
[0m15:27:28.311853 [debug] [Thread-3 (]: SQL status: OK in 0.004 seconds
[0m15:27:28.313678 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:27:28.313937 [debug] [Thread-3 (]: On model.quant_features.daily_stock_summary: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.daily_stock_summary"} */
alter table "quant_features"."main"."daily_stock_summary" rename to "daily_stock_summary__dbt_backup"
[0m15:27:28.314421 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:27:28.316137 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:27:28.316406 [debug] [Thread-3 (]: On model.quant_features.daily_stock_summary: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.daily_stock_summary"} */
alter table "quant_features"."main"."daily_stock_summary__dbt_tmp" rename to "daily_stock_summary"
[0m15:27:28.316855 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:27:28.319876 [debug] [Thread-3 (]: On model.quant_features.daily_stock_summary: COMMIT
[0m15:27:28.320152 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:27:28.320367 [debug] [Thread-3 (]: On model.quant_features.daily_stock_summary: COMMIT
[0m15:27:28.322823 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m15:27:28.329038 [debug] [Thread-3 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:27:28.329418 [debug] [Thread-3 (]: On model.quant_features.daily_stock_summary: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.daily_stock_summary"} */

      drop table if exists "quant_features"."main"."daily_stock_summary__dbt_backup" cascade
    
[0m15:27:28.331384 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m15:27:28.332473 [debug] [Thread-3 (]: On model.quant_features.daily_stock_summary: Close
[0m15:27:28.333078 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35d3798b-4af2-4e77-8cc6-1830e26e304e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c5b5c8e10>]}
[0m15:27:28.333540 [info ] [Thread-3 (]: 3 of 4 OK created sql table model main.daily_stock_summary ..................... [[32mOK[0m in 0.04s]
[0m15:27:28.333869 [debug] [Thread-3 (]: Finished running node model.quant_features.daily_stock_summary
[0m15:27:28.334952 [debug] [Thread-4 (]: Began running node model.quant_features.stock_features
[0m15:27:28.335367 [info ] [Thread-4 (]: 4 of 4 START sql table model main.stock_features ............................... [RUN]
[0m15:27:28.335719 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.quant_features.stock_features'
[0m15:27:28.335932 [debug] [Thread-4 (]: Began compiling node model.quant_features.stock_features
[0m15:27:28.338160 [debug] [Thread-4 (]: Writing injected SQL for node "model.quant_features.stock_features"
[0m15:27:28.338600 [debug] [Thread-4 (]: Began executing node model.quant_features.stock_features
[0m15:27:28.340771 [debug] [Thread-4 (]: Writing runtime sql for node "model.quant_features.stock_features"
[0m15:27:28.341205 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:27:28.341427 [debug] [Thread-4 (]: On model.quant_features.stock_features: BEGIN
[0m15:27:28.341616 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m15:27:28.342074 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m15:27:28.342289 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:27:28.342542 [debug] [Thread-4 (]: On model.quant_features.stock_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stock_features"} */

  
    
    

    create  table
      "quant_features"."main"."stock_features__dbt_tmp"
  
    as (
      

with daily_data as (
    select * from "quant_features"."main"."daily_stock_summary"
),

windowed_features as (
    select
        *,
        -- 移动平均
        avg(close) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        ) as ma_5d,
        
        avg(close) over (
            partition by symbol 
            order by date 
            rows between 9 preceding and current row
        ) as ma_10d,
        
        -- 波动率（标准差）
        stddev(daily_return) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        ) as volatility_5d,
        
        -- 价格相对位置
        (close - min(low) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        )) / (max(high) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        ) - min(low) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        )) as price_position_5d,
        
        -- 成交量相对强度
        volume / avg(volume) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        ) as volume_ratio_5d
        
    from daily_data
),

final_features as (
    select
        *,
        -- 技术信号
        case 
            when close > ma_5d and ma_5d > ma_10d then 'Bullish'
            when close < ma_5d and ma_5d < ma_10d then 'Bearish'
            else 'Neutral'
        end as trend_signal,
        
        case 
            when volatility_5d > 0.03 then 'High'
            when volatility_5d > 0.01 then 'Medium'
            else 'Low'
        end as volatility_category
        
    from windowed_features
)

select * from final_features
    );
  
  
[0m15:27:28.349216 [debug] [Thread-4 (]: SQL status: OK in 0.006 seconds
[0m15:27:28.350948 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:27:28.351211 [debug] [Thread-4 (]: On model.quant_features.stock_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stock_features"} */
alter table "quant_features"."main"."stock_features" rename to "stock_features__dbt_backup"
[0m15:27:28.351722 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m15:27:28.353725 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:27:28.353989 [debug] [Thread-4 (]: On model.quant_features.stock_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stock_features"} */
alter table "quant_features"."main"."stock_features__dbt_tmp" rename to "stock_features"
[0m15:27:28.354474 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m15:27:28.355429 [debug] [Thread-4 (]: On model.quant_features.stock_features: COMMIT
[0m15:27:28.355688 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:27:28.355884 [debug] [Thread-4 (]: On model.quant_features.stock_features: COMMIT
[0m15:27:28.357792 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m15:27:28.359163 [debug] [Thread-4 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:27:28.359439 [debug] [Thread-4 (]: On model.quant_features.stock_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stock_features"} */

      drop table if exists "quant_features"."main"."stock_features__dbt_backup" cascade
    
[0m15:27:28.361377 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m15:27:28.362271 [debug] [Thread-4 (]: On model.quant_features.stock_features: Close
[0m15:27:28.363016 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35d3798b-4af2-4e77-8cc6-1830e26e304e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c5b6f1950>]}
[0m15:27:28.363418 [info ] [Thread-4 (]: 4 of 4 OK created sql table model main.stock_features .......................... [[32mOK[0m in 0.03s]
[0m15:27:28.363731 [debug] [Thread-4 (]: Finished running node model.quant_features.stock_features
[0m15:27:28.365457 [debug] [MainThread]: Using duckdb connection "master"
[0m15:27:28.365710 [debug] [MainThread]: On master: BEGIN
[0m15:27:28.365876 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:27:28.366270 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:27:28.366469 [debug] [MainThread]: On master: COMMIT
[0m15:27:28.366627 [debug] [MainThread]: Using duckdb connection "master"
[0m15:27:28.366772 [debug] [MainThread]: On master: COMMIT
[0m15:27:28.367044 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:27:28.367234 [debug] [MainThread]: On master: Close
[0m15:27:28.367524 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:27:28.367689 [debug] [MainThread]: Connection 'model.quant_features.stg_market_data' was properly closed.
[0m15:27:28.367829 [debug] [MainThread]: Connection 'model.quant_features.stg_stock_prices' was properly closed.
[0m15:27:28.367965 [debug] [MainThread]: Connection 'model.quant_features.daily_stock_summary' was properly closed.
[0m15:27:28.368097 [debug] [MainThread]: Connection 'model.quant_features.stock_features' was properly closed.
[0m15:27:28.368330 [info ] [MainThread]: 
[0m15:27:28.368554 [info ] [MainThread]: Finished running 2 table models, 2 view models in 0 hours 0 minutes and 0.27 seconds (0.27s).
[0m15:27:28.369144 [debug] [MainThread]: Command end result
[0m15:27:28.392246 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:27:28.393666 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:27:28.397779 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:27:28.398020 [info ] [MainThread]: 
[0m15:27:28.398251 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:27:28.398440 [info ] [MainThread]: 
[0m15:27:28.398637 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m15:27:28.399287 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.7865642, "process_in_blocks": "0", "process_kernel_time": 0.139911, "process_mem_max_rss": "158996", "process_out_blocks": "3664", "process_user_time": 1.542903}
[0m15:27:28.399617 [debug] [MainThread]: Command `dbt run` succeeded at 15:27:28.399551 after 0.79 seconds
[0m15:27:28.399843 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c60531da0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c6472d400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c6070b650>]}
[0m15:27:28.400054 [debug] [MainThread]: Flushing usage events
[0m15:27:28.456351 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:27:34.598249 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa910623770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa911c75a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa90f493d90>]}


============================== 15:27:34.600755 | 739055df-1b17-40a3-9538-a0b06eaafc9b ==============================
[0m15:27:34.600755 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:27:34.601093 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt show --select stock_features --limit 10', 'introspect': 'True', 'use_experimental_parser': 'False', 'write_json': 'True', 'log_format': 'default', 'log_path': '/workspace/dbt_project/logs', 'profiles_dir': '/workspace/dbt_project', 'no_print': 'None', 'log_cache_events': 'False', 'static_parser': 'True', 'indirect_selection': 'eager', 'use_colors': 'True', 'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'cache_selected_only': 'False', 'partial_parse': 'True', 'version_check': 'True', 'printer_width': '80', 'fail_fast': 'False', 'target_path': 'None', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'empty': 'None', 'quiet': 'False'}
[0m15:27:34.751053 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '739055df-1b17-40a3-9538-a0b06eaafc9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa9101ae650>]}
[0m15:27:34.792715 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '739055df-1b17-40a3-9538-a0b06eaafc9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa90f592be0>]}
[0m15:27:34.801424 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:27:34.832332 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:27:34.921788 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:27:34.922032 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:27:34.959036 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '739055df-1b17-40a3-9538-a0b06eaafc9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa90b3d5650>]}
[0m15:27:35.032448 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:27:35.033708 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:27:35.038511 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '739055df-1b17-40a3-9538-a0b06eaafc9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa90b3c1310>]}
[0m15:27:35.038863 [info ] [MainThread]: Found 15 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:27:35.039074 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '739055df-1b17-40a3-9538-a0b06eaafc9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa90b44c590>]}
[0m15:27:35.040381 [info ] [MainThread]: 
[0m15:27:35.040667 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:27:35.040842 [info ] [MainThread]: 
[0m15:27:35.041145 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:27:35.044800 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features_main'
[0m15:27:35.095974 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:27:35.096231 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:27:35.096423 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:27:35.109649 [debug] [ThreadPool]: SQL status: OK in 0.013 seconds
[0m15:27:35.109907 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:27:35.110092 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:27:35.118306 [debug] [ThreadPool]: SQL status: OK in 0.008 seconds
[0m15:27:35.119308 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:27:35.119961 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:27:35.120192 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:27:35.122090 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '739055df-1b17-40a3-9538-a0b06eaafc9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa90aff0ae0>]}
[0m15:27:35.124428 [debug] [Thread-1 (]: Began running node model.quant_features.stock_features
[0m15:27:35.124773 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.stock_features)
[0m15:27:35.124987 [debug] [Thread-1 (]: Began compiling node model.quant_features.stock_features
[0m15:27:35.129774 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.stock_features"
[0m15:27:35.130231 [debug] [Thread-1 (]: Began executing node model.quant_features.stock_features
[0m15:27:35.134742 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.stock_features"
[0m15:27:35.135090 [debug] [Thread-1 (]: On model.quant_features.stock_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.stock_features"} */

  
  

with daily_data as (
    select * from "quant_features"."main"."daily_stock_summary"
),

windowed_features as (
    select
        *,
        -- 移动平均
        avg(close) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        ) as ma_5d,
        
        avg(close) over (
            partition by symbol 
            order by date 
            rows between 9 preceding and current row
        ) as ma_10d,
        
        -- 波动率（标准差）
        stddev(daily_return) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        ) as volatility_5d,
        
        -- 价格相对位置
        (close - min(low) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        )) / (max(high) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        ) - min(low) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        )) as price_position_5d,
        
        -- 成交量相对强度
        volume / avg(volume) over (
            partition by symbol 
            order by date 
            rows between 4 preceding and current row
        ) as volume_ratio_5d
        
    from daily_data
),

final_features as (
    select
        *,
        -- 技术信号
        case 
            when close > ma_5d and ma_5d > ma_10d then 'Bullish'
            when close < ma_5d and ma_5d < ma_10d then 'Bearish'
            else 'Neutral'
        end as trend_signal,
        
        case 
            when volatility_5d > 0.03 then 'High'
            when volatility_5d > 0.01 then 'Medium'
            else 'Low'
        end as volatility_category
        
    from windowed_features
)

select * from final_features
  
  limit 10

[0m15:27:35.135372 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:27:35.140681 [debug] [Thread-1 (]: SQL status: OK in 0.005 seconds
[0m15:27:35.143092 [debug] [Thread-1 (]: On model.quant_features.stock_features: Close
[0m15:27:35.143865 [debug] [Thread-1 (]: Finished running node model.quant_features.stock_features
[0m15:27:35.145382 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:27:35.145646 [debug] [MainThread]: Connection 'model.quant_features.stock_features' was properly closed.
[0m15:27:35.146063 [debug] [MainThread]: Command end result
[0m15:27:35.168502 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:27:35.169723 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:27:35.173962 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:27:35.175024 [info ] [MainThread]: Previewing node 'stock_features':
|       date | symbol |    open |    high |     low |   close | ... |
| ---------- | ------ | ------- | ------- | ------- | ------- | --- |
| 2024-01-01 | GOOGL  | 2,800.0 | 2,850.0 | 2,780.0 | 2,820.0 | ... |
| 2024-01-02 | GOOGL  | 2,820.0 | 2,870.0 | 2,810.0 | 2,845.0 | ... |
| 2024-01-01 | MSFT   |   380.0 |   385.0 |   378.0 |   382.5 | ... |
| 2024-01-02 | MSFT   |   382.5 |   387.0 |   380.5 |   384.2 | ... |
| 2024-01-03 | MSFT   |   384.2 |   389.0 |   382.8 |   386.7 | ... |
| 2024-01-04 | MSFT   |   386.7 |   391.5 |   385.2 |   388.9 | ... |
| 2024-01-05 | MSFT   |   388.9 |   393.0 |   387.4 |   390.8 | ... |
| 2024-01-08 | MSFT   |   390.8 |   395.2 |   389.5 |   392.6 | ... |
| 2024-01-09 | MSFT   |   392.6 |   397.0 |   391.2 |   394.4 | ... |
| 2024-01-10 | MSFT   |   394.4 |   399.0 |   393.1 |   396.2 | ... |

[0m15:27:35.175668 [debug] [MainThread]: Resource report: {"command_name": "show", "command_success": true, "command_wall_clock_time": 0.61442894, "process_in_blocks": "0", "process_kernel_time": 0.18255, "process_mem_max_rss": "153040", "process_out_blocks": "3456", "process_user_time": 1.34534}
[0m15:27:35.175973 [debug] [MainThread]: Command `dbt show` succeeded at 15:27:35.175911 after 0.61 seconds
[0m15:27:35.176174 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa90f82f110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa90b179390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa90b179440>]}
[0m15:27:35.176387 [debug] [MainThread]: Flushing usage events
[0m15:27:35.241632 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:27:40.918204 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7eca53770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7ee085a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7eb8c7d90>]}


============================== 15:27:40.920538 | d014ec97-0aed-4f57-a9bc-2341530ab41c ==============================
[0m15:27:40.920538 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:27:40.920875 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'empty': 'None', 'target_path': 'None', 'log_cache_events': 'False', 'no_print': 'None', 'introspect': 'True', 'static_parser': 'True', 'debug': 'False', 'quiet': 'False', 'printer_width': '80', 'invocation_command': 'dbt show --select daily_stock_summary --limit 5', 'fail_fast': 'False', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True', 'log_path': '/workspace/dbt_project/logs', 'log_format': 'default', 'warn_error': 'None', 'use_experimental_parser': 'False', 'profiles_dir': '/workspace/dbt_project'}
[0m15:27:41.059488 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd014ec97-0aed-4f57-a9bc-2341530ab41c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7ec5de650>]}
[0m15:27:41.100541 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd014ec97-0aed-4f57-a9bc-2341530ab41c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7eb9c2be0>]}
[0m15:27:41.108482 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:27:41.137560 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:27:41.224187 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:27:41.224441 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:27:41.261069 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd014ec97-0aed-4f57-a9bc-2341530ab41c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7e77e1650>]}
[0m15:27:41.330199 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:27:41.331399 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:27:41.335551 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd014ec97-0aed-4f57-a9bc-2341530ab41c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7e77d1310>]}
[0m15:27:41.335911 [info ] [MainThread]: Found 15 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:27:41.336137 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd014ec97-0aed-4f57-a9bc-2341530ab41c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7e7858590>]}
[0m15:27:41.337339 [info ] [MainThread]: 
[0m15:27:41.337610 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:27:41.337781 [info ] [MainThread]: 
[0m15:27:41.338065 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:27:41.341747 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features_main'
[0m15:27:41.388026 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:27:41.388281 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:27:41.388505 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:27:41.398849 [debug] [ThreadPool]: SQL status: OK in 0.010 seconds
[0m15:27:41.399103 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:27:41.399279 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:27:41.406214 [debug] [ThreadPool]: SQL status: OK in 0.007 seconds
[0m15:27:41.407185 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:27:41.407800 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:27:41.408013 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:27:41.409612 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd014ec97-0aed-4f57-a9bc-2341530ab41c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7e73fcae0>]}
[0m15:27:41.412005 [debug] [Thread-1 (]: Began running node model.quant_features.daily_stock_summary
[0m15:27:41.412392 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.daily_stock_summary)
[0m15:27:41.412626 [debug] [Thread-1 (]: Began compiling node model.quant_features.daily_stock_summary
[0m15:27:41.417513 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.daily_stock_summary"
[0m15:27:41.417971 [debug] [Thread-1 (]: Began executing node model.quant_features.daily_stock_summary
[0m15:27:41.422689 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.daily_stock_summary"
[0m15:27:41.423002 [debug] [Thread-1 (]: On model.quant_features.daily_stock_summary: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.daily_stock_summary"} */

  
  

with price_data as (
    select * from "quant_features"."main"."stg_stock_prices"
),

market_data as (
    select * from "quant_features"."main"."stg_market_data"
),

combined_data as (
    select
        p.date,
        p.symbol,
        p.open,
        p.high,
        p.low,
        p.close,
        p.volume,
        p.daily_range,
        p.daily_change,
        p.daily_return,
        m.market_cap,
        m.pe_ratio,
        m.dividend_yield,
        m.sector,
        m.estimated_earnings,
        m.estimated_dividend_payout,
        -- 计算额外的技术指标
        (p.high + p.low + p.close) / 3 as typical_price,
        p.volume * p.close as dollar_volume,
        case 
            when p.daily_return > 0.05 then 'Strong Up'
            when p.daily_return > 0.02 then 'Up'
            when p.daily_return > -0.02 then 'Flat'
            when p.daily_return > -0.05 then 'Down'
            else 'Strong Down'
        end as price_movement_category
    from price_data p
    left join market_data m
        on p.date = m.date
        and p.symbol = m.symbol
)

select * from combined_data
  
  limit 5

[0m15:27:41.423239 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:27:41.427320 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m15:27:41.429291 [debug] [Thread-1 (]: On model.quant_features.daily_stock_summary: Close
[0m15:27:41.429930 [debug] [Thread-1 (]: Finished running node model.quant_features.daily_stock_summary
[0m15:27:41.431177 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:27:41.431420 [debug] [MainThread]: Connection 'model.quant_features.daily_stock_summary' was properly closed.
[0m15:27:41.431849 [debug] [MainThread]: Command end result
[0m15:27:41.453245 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:27:41.454412 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:27:41.458584 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:27:41.459357 [info ] [MainThread]: Previewing node 'daily_stock_summary':
|       date | symbol |  open |  high |   low | close | ... |
| ---------- | ------ | ----- | ----- | ----- | ----- | --- |
| 2024-01-01 | AAPL   | 150.0 | 152.5 | 149.8 | 151.2 | ... |
| 2024-01-02 | AAPL   | 151.2 | 153.0 | 150.5 | 152.8 | ... |
| 2024-01-03 | AAPL   | 152.8 | 154.2 | 151.9 | 153.5 | ... |
| 2024-01-04 | AAPL   | 153.5 | 155.0 | 152.8 | 154.3 | ... |
| 2024-01-05 | AAPL   | 154.3 | 156.2 | 153.5 | 155.8 | ... |

[0m15:27:41.459957 [debug] [MainThread]: Resource report: {"command_name": "show", "command_success": true, "command_wall_clock_time": 0.5773786, "process_in_blocks": "0", "process_kernel_time": 0.124516, "process_mem_max_rss": "150792", "process_out_blocks": "3448", "process_user_time": 1.32949}
[0m15:27:41.460248 [debug] [MainThread]: Command `dbt show` succeeded at 15:27:41.460191 after 0.58 seconds
[0m15:27:41.460465 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7eba63110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7e753d390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7e753d440>]}
[0m15:27:41.460692 [debug] [MainThread]: Flushing usage events
[0m15:27:41.483239 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:30:03.178375 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8390b57770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f83921a5a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f838f9c7d90>]}


============================== 15:30:03.180816 | f583f5e6-b791-4144-83d2-01fc394916a1 ==============================
[0m15:30:03.180816 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:30:03.181150 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'introspect': 'True', 'log_format': 'default', 'version_check': 'True', 'log_path': '/workspace/dbt_project/logs', 'log_cache_events': 'False', 'write_json': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'warn_error': 'None', 'printer_width': '80', 'fail_fast': 'False', 'empty': 'False', 'quiet': 'False', 'debug': 'False', 'invocation_command': 'dbt run --select alpha_base_data', 'target_path': 'None', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'cache_selected_only': 'False', 'static_parser': 'True', 'profiles_dir': '/workspace/dbt_project'}
[0m15:30:03.331077 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f583f5e6-b791-4144-83d2-01fc394916a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f83906de650>]}
[0m15:30:03.372902 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f583f5e6-b791-4144-83d2-01fc394916a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f838fac2be0>]}
[0m15:30:03.373971 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:30:03.404541 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:30:03.493217 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:30:03.493728 [debug] [MainThread]: Partial parsing: updated file: quant_features://models/alpha101/alpha_base_data.sql
[0m15:30:03.755448 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `dbt_utils.accepted_range`. Arguments to
generic tests should be nested under the `arguments` property.`
[0m15:30:03.755927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'f583f5e6-b791-4144-83d2-01fc394916a1', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f838c2d9a50>]}
[0m15:30:03.866042 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f583f5e6-b791-4144-83d2-01fc394916a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f838b4d59a0>]}
[0m15:30:03.935517 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:30:03.936813 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:30:03.948350 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f583f5e6-b791-4144-83d2-01fc394916a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f838b45f850>]}
[0m15:30:03.948699 [info ] [MainThread]: Found 15 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:30:03.948913 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f583f5e6-b791-4144-83d2-01fc394916a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f838aa28c80>]}
[0m15:30:03.950153 [info ] [MainThread]: 
[0m15:30:03.950436 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:30:03.950619 [info ] [MainThread]: 
[0m15:30:03.950924 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:30:03.951903 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features'
[0m15:30:03.968917 [debug] [ThreadPool]: Using duckdb connection "list_quant_features"
[0m15:30:03.969191 [debug] [ThreadPool]: On list_quant_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"quant_features"'
    
  
  
[0m15:30:03.969382 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:30:03.983983 [debug] [ThreadPool]: SQL status: OK in 0.015 seconds
[0m15:30:03.984956 [debug] [ThreadPool]: On list_quant_features: Close
[0m15:30:03.985665 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_quant_features, now create_quant_features_main)
[0m15:30:03.985977 [debug] [ThreadPool]: Creating schema "database: "quant_features"
schema: "main"
"
[0m15:30:03.990355 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:30:03.990611 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='quant_features'
        and type='sqlite'
    
  
[0m15:30:03.990784 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:30:03.991532 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:30:03.992396 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:30:03.992611 [debug] [ThreadPool]: On create_quant_features_main: BEGIN
[0m15:30:03.992970 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:30:03.993139 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:30:03.993308 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
    
        create schema if not exists "quant_features"."main"
    
[0m15:30:03.993657 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:30:03.994184 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:30:03.994405 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:30:03.994572 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:30:03.994863 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:30:03.995040 [debug] [ThreadPool]: On create_quant_features_main: Close
[0m15:30:03.999096 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_quant_features_main, now list_quant_features_main)
[0m15:30:04.002908 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:30:04.003162 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:30:04.003332 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:30:04.003720 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:30:04.003905 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:30:04.004059 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:30:04.011096 [debug] [ThreadPool]: SQL status: OK in 0.007 seconds
[0m15:30:04.012039 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:30:04.012722 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:30:04.012938 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:30:04.014650 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f583f5e6-b791-4144-83d2-01fc394916a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f838a018410>]}
[0m15:30:04.014954 [debug] [MainThread]: Using duckdb connection "master"
[0m15:30:04.015120 [debug] [MainThread]: On master: BEGIN
[0m15:30:04.015280 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:30:04.015663 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:30:04.015868 [debug] [MainThread]: On master: COMMIT
[0m15:30:04.016036 [debug] [MainThread]: Using duckdb connection "master"
[0m15:30:04.016204 [debug] [MainThread]: On master: COMMIT
[0m15:30:04.016928 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:30:04.017236 [debug] [MainThread]: On master: Close
[0m15:30:04.019678 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_base_data
[0m15:30:04.020074 [info ] [Thread-1 (]: 1 of 1 START sql table model main.alpha_base_data .............................. [RUN]
[0m15:30:04.020386 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.alpha_base_data)
[0m15:30:04.020596 [debug] [Thread-1 (]: Began compiling node model.quant_features.alpha_base_data
[0m15:30:04.038870 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.alpha_base_data"
[0m15:30:04.039507 [debug] [Thread-1 (]: Began executing node model.quant_features.alpha_base_data
[0m15:30:04.060089 [debug] [Thread-1 (]: Writing runtime sql for node "model.quant_features.alpha_base_data"
[0m15:30:04.060662 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_base_data"
[0m15:30:04.060914 [debug] [Thread-1 (]: On model.quant_features.alpha_base_data: BEGIN
[0m15:30:04.061119 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:30:04.061596 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:30:04.061805 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_base_data"
[0m15:30:04.062246 [debug] [Thread-1 (]: On model.quant_features.alpha_base_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_base_data"} */

  
    
    

    create  table
      "quant_features"."main"."alpha_base_data__dbt_tmp"
  
    as (
      

-- Alpha 101 基础数据准备
-- 为Alpha因子计算准备所有必要的基础数据

WITH base_ohlc AS (
    SELECT 
        symbol,
        timestamp,
        open,
        high,
        low,
        close,
        volume,
        -- 计算VWAP (简化版本，假设等权重)
        (high + low + close) / 3 AS vwap,
        -- 计算returns
        CASE 
            WHEN LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp) IS NOT NULL
            THEN (close - LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp)) / 
                 LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp)
            ELSE 0
        END AS returns
    FROM "quant_features"."main"."stg_ohlc_data"
    WHERE timestamp >= CAST('2020-01-01' AS DATE)
      AND timestamp <= CAST('2024-12-31' AS DATE)
),

enhanced_data AS (
    SELECT 
        *,
        -- 计算ADV (Average Daily Volume)
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )

 AS adv20,
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )

 AS adv10,
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )

 AS adv5,
        
        -- 预计算一些常用的时间序列指标
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_ma5,
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS close_ma10,
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_ma20,
        
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS volume_ma20,
        
        -- 预计算滚动标准差
        
    STDDEV(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_std20,
        
    STDDEV(returns) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS returns_std20,
        
        -- 预计算一些延迟项
        
    LAG(close, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag1,
        
    LAG(close, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag2,
        
    LAG(close, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag5,
        
    LAG(close, 10) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag10,
        
    LAG(close, 20) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag20,
        
    LAG(volume, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS volume_lag1,
        
    LAG(high, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS high_lag1,
        
    LAG(low, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS low_lag1,
        
    LAG(vwap, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS vwap_lag5,
        
        -- 预计算一些差值项
        
    close - 
    LAG(close, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta1,
        
    close - 
    LAG(close, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta2,
        
    close - 
    LAG(close, 3) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta3,
        
    close - 
    LAG(close, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta5,
        
    close - 
    LAG(close, 7) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta7,
        
    close - 
    LAG(close, 10) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta10,
        
    volume - 
    LAG(volume, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS volume_delta1,
        
    volume - 
    LAG(volume, 3) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS volume_delta3,
        
    high - 
    LAG(high, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS high_delta2,
        
        -- 预计算一些排序项
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY close
    )
 AS close_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY volume
    )
 AS volume_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY high
    )
 AS high_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY low
    )
 AS low_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY vwap
    )
 AS vwap_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY returns
    )
 AS returns_rank,
        
        -- 预计算时间序列排序
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY close
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS close_ts_rank10,
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY volume
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_ts_rank5,
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY high
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS high_ts_rank5,
        
        -- 预计算一些最值项
        
    MIN(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 99 PRECEDING AND CURRENT ROW
    )
 AS close_min100,
        
    MAX(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS close_max3,
        
    MIN(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_min5,
        
    MAX(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_max5,
        
    MIN(low) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS low_min5,
        
    MAX(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS high_max3,
        
        -- 预计算一些求和项
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_sum5,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 7 PRECEDING AND CURRENT ROW
    )
 AS close_sum8,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_sum20,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 99 PRECEDING AND CURRENT ROW
    )
 AS close_sum100,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 199 PRECEDING AND CURRENT ROW
    )
 AS close_sum200,
        
    SUM(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_sum5,
        
    SUM(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS volume_sum20,
        
    SUM(returns) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 249 PRECEDING AND CURRENT ROW
    )
 AS returns_sum250,
        
    SUM(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS high_sum5,
        
    SUM(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS high_sum20,
        
        -- 预计算一些相关性
        
    -- 使用DuckDB的CORR窗口函数
    CORR(close, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS corr_close_volume_10,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(open, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS corr_open_volume_10,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(high, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS corr_high_volume_5,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(vwap, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 5 PRECEDING AND CURRENT ROW
    )
 AS corr_vwap_volume_6,
        
        -- 预计算一些协方差
        
    COVAR_SAMP(close, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS cov_close_volume_5,
        
    COVAR_SAMP(high, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS cov_high_volume_5
        
    FROM base_ohlc
    WHERE timestamp >= CAST(CAST('2020-01-01' AS DATE) AS DATE) - INTERVAL '250 days'  -- 扩展时间范围以确保有足够的历史数据
),

-- 过滤回原始时间范围
final_data AS (
    SELECT *
    FROM enhanced_data
    WHERE timestamp >= CAST('2020-01-01' AS DATE)
      AND timestamp <= CAST('2024-12-31' AS DATE)
)

SELECT * FROM final_data
    );
  
  
[0m15:30:04.093347 [debug] [Thread-1 (]: SQL status: OK in 0.031 seconds
[0m15:30:04.097372 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_base_data"
[0m15:30:04.097643 [debug] [Thread-1 (]: On model.quant_features.alpha_base_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_base_data"} */
alter table "quant_features"."main"."alpha_base_data__dbt_tmp" rename to "alpha_base_data"
[0m15:30:04.098200 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:30:04.106619 [debug] [Thread-1 (]: On model.quant_features.alpha_base_data: COMMIT
[0m15:30:04.106913 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_base_data"
[0m15:30:04.107129 [debug] [Thread-1 (]: On model.quant_features.alpha_base_data: COMMIT
[0m15:30:04.110184 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m15:30:04.114005 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_base_data"
[0m15:30:04.114299 [debug] [Thread-1 (]: On model.quant_features.alpha_base_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_base_data"} */

      drop table if exists "quant_features"."main"."alpha_base_data__dbt_backup" cascade
    
[0m15:30:04.114739 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:30:04.116460 [debug] [Thread-1 (]: On model.quant_features.alpha_base_data: Close
[0m15:30:04.119312 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f583f5e6-b791-4144-83d2-01fc394916a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f838b646c50>]}
[0m15:30:04.119910 [info ] [Thread-1 (]: 1 of 1 OK created sql table model main.alpha_base_data ......................... [[32mOK[0m in 0.10s]
[0m15:30:04.120310 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_base_data
[0m15:30:04.122468 [debug] [MainThread]: Using duckdb connection "master"
[0m15:30:04.122719 [debug] [MainThread]: On master: BEGIN
[0m15:30:04.122886 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:30:04.123329 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:30:04.123520 [debug] [MainThread]: On master: COMMIT
[0m15:30:04.123679 [debug] [MainThread]: Using duckdb connection "master"
[0m15:30:04.123827 [debug] [MainThread]: On master: COMMIT
[0m15:30:04.124088 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:30:04.124285 [debug] [MainThread]: On master: Close
[0m15:30:04.124581 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:30:04.124741 [debug] [MainThread]: Connection 'model.quant_features.alpha_base_data' was properly closed.
[0m15:30:04.124920 [info ] [MainThread]: 
[0m15:30:04.125107 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.17 seconds (0.17s).
[0m15:30:04.125496 [debug] [MainThread]: Command end result
[0m15:30:04.149326 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:30:04.150623 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:30:04.154379 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:30:04.154608 [info ] [MainThread]: 
[0m15:30:04.154846 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:30:04.155017 [info ] [MainThread]: 
[0m15:30:04.155209 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m15:30:04.155562 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 4 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m15:30:04.156240 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 1.0142612, "process_in_blocks": "0", "process_kernel_time": 0.158677, "process_mem_max_rss": "184732", "process_out_blocks": "5048", "process_user_time": 1.793172}
[0m15:30:04.156573 [debug] [MainThread]: Command `dbt run` succeeded at 15:30:04.156511 after 1.01 seconds
[0m15:30:04.156806 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f838b5f2df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f838abdd910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f838b792bd0>]}
[0m15:30:04.157013 [debug] [MainThread]: Flushing usage events
[0m15:30:04.209971 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:30:11.172607 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a5592f770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a56f8da90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a547a7d90>]}


============================== 15:30:11.175104 | 1ab1ef80-2fd2-4638-9b1b-959a0187c48b ==============================
[0m15:30:11.175104 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:30:11.175463 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'printer_width': '80', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'write_json': 'True', 'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'profiles_dir': '/workspace/dbt_project', 'quiet': 'False', 'invocation_command': 'dbt run --select alpha_factors_001_020', 'log_path': '/workspace/dbt_project/logs', 'empty': 'False', 'use_colors': 'True', 'static_parser': 'True', 'warn_error': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'fail_fast': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None'}
[0m15:30:11.315415 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1ab1ef80-2fd2-4638-9b1b-959a0187c48b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a554be650>]}
[0m15:30:11.363020 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1ab1ef80-2fd2-4638-9b1b-959a0187c48b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a5489ebe0>]}
[0m15:30:11.364051 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:30:11.394736 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:30:11.482911 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:30:11.483144 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:30:11.519767 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1ab1ef80-2fd2-4638-9b1b-959a0187c48b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a510b9450>]}
[0m15:30:11.587986 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:30:11.589199 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:30:11.599932 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1ab1ef80-2fd2-4638-9b1b-959a0187c48b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a510e9220>]}
[0m15:30:11.600265 [info ] [MainThread]: Found 15 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:30:11.600487 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1ab1ef80-2fd2-4638-9b1b-959a0187c48b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a5112c050>]}
[0m15:30:11.601697 [info ] [MainThread]: 
[0m15:30:11.601950 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:30:11.602117 [info ] [MainThread]: 
[0m15:30:11.602416 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:30:11.603117 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features'
[0m15:30:11.621766 [debug] [ThreadPool]: Using duckdb connection "list_quant_features"
[0m15:30:11.622027 [debug] [ThreadPool]: On list_quant_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"quant_features"'
    
  
  
[0m15:30:11.622237 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:30:11.636589 [debug] [ThreadPool]: SQL status: OK in 0.014 seconds
[0m15:30:11.637555 [debug] [ThreadPool]: On list_quant_features: Close
[0m15:30:11.638278 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_quant_features, now create_quant_features_main)
[0m15:30:11.638587 [debug] [ThreadPool]: Creating schema "database: "quant_features"
schema: "main"
"
[0m15:30:11.642784 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:30:11.643048 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='quant_features'
        and type='sqlite'
    
  
[0m15:30:11.643238 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:30:11.644203 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:30:11.645082 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:30:11.645323 [debug] [ThreadPool]: On create_quant_features_main: BEGIN
[0m15:30:11.645684 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:30:11.645864 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:30:11.646012 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
    
        create schema if not exists "quant_features"."main"
    
[0m15:30:11.646347 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:30:11.646875 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:30:11.647056 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:30:11.647224 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:30:11.647511 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:30:11.647698 [debug] [ThreadPool]: On create_quant_features_main: Close
[0m15:30:11.652082 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_quant_features_main, now list_quant_features_main)
[0m15:30:11.688777 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:30:11.689039 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:30:11.689223 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:30:11.689638 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:30:11.689843 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:30:11.690006 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:30:11.696792 [debug] [ThreadPool]: SQL status: OK in 0.007 seconds
[0m15:30:11.697788 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:30:11.698411 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:30:11.698621 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:30:11.700312 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1ab1ef80-2fd2-4638-9b1b-959a0187c48b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a54863520>]}
[0m15:30:11.700643 [debug] [MainThread]: Using duckdb connection "master"
[0m15:30:11.700813 [debug] [MainThread]: On master: BEGIN
[0m15:30:11.700957 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:30:11.701342 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:30:11.701543 [debug] [MainThread]: On master: COMMIT
[0m15:30:11.701703 [debug] [MainThread]: Using duckdb connection "master"
[0m15:30:11.701852 [debug] [MainThread]: On master: COMMIT
[0m15:30:11.702085 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:30:11.702256 [debug] [MainThread]: On master: Close
[0m15:30:11.704410 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_factors_001_020
[0m15:30:11.704791 [info ] [Thread-1 (]: 1 of 1 START sql table model main.alpha_factors_001_020 ........................ [RUN]
[0m15:30:11.705073 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.alpha_factors_001_020)
[0m15:30:11.705290 [debug] [Thread-1 (]: Began compiling node model.quant_features.alpha_factors_001_020
[0m15:30:11.725609 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.alpha_factors_001_020"
[0m15:30:11.726092 [debug] [Thread-1 (]: Began executing node model.quant_features.alpha_factors_001_020
[0m15:30:11.746900 [debug] [Thread-1 (]: Writing runtime sql for node "model.quant_features.alpha_factors_001_020"
[0m15:30:11.747416 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_001_020"
[0m15:30:11.747650 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_001_020: BEGIN
[0m15:30:11.747848 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:30:11.748278 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:30:11.748514 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_001_020"
[0m15:30:11.748999 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_001_020: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_factors_001_020"} */

  
    
    

    create  table
      "quant_features"."main"."alpha_factors_001_020__dbt_tmp"
  
    as (
      

-- Alpha 101 因子计算 (001-020)
-- 基于预处理的基础数据计算前20个Alpha因子

WITH base_data AS (
    SELECT * FROM "quant_features"."main"."alpha_base_data"
),

-- 预计算一些复杂的中间变量
intermediate_calcs AS (
    SELECT 
        *,
        -- Alpha001 相关计算
        
    -- 使用ROW_NUMBER()来找到最大值的位置
    (5 - 1) - (
        ROW_NUMBER() OVER (
            PARTITION BY symbol, 
            (CASE WHEN returns < 0 THEN returns_std20 ELSE close END = 
    MAX(CASE WHEN returns < 0 THEN returns_std20 ELSE close END) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
)
            ORDER BY timestamp DESC
        ) - 1
    )
 AS alpha001_argmax,
        
        -- Alpha002 相关计算
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY 
    
    CASE 
        WHEN volume > 0 THEN LN(volume)
        ELSE NULL
    END
 - 
    LAG(
    CASE 
        WHEN volume > 0 THEN LN(volume)
        ELSE NULL
    END
, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )


    )
 AS alpha002_rank_delta_log_vol,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY 
    CASE 
        WHEN open = 0 OR open IS NULL THEN NULL
        WHEN ABS(open) < 1e-10 THEN NULL
        ELSE close - open / open
    END

    )
 AS alpha002_rank_ret,
        
        -- Alpha005 相关计算
        close_ma10 AS alpha005_mean_vwap,
        
        -- Alpha007 相关计算
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY 
    ABS(close_delta7)

        ROWS BETWEEN 59 PRECEDING AND CURRENT ROW
    )
 AS alpha007_ts_rank,
        
    CASE 
        WHEN close_delta7 > 0 THEN 1
        WHEN close_delta7 < 0 THEN -1
        ELSE 0
    END
 AS alpha007_sign,
        
        -- Alpha008 相关计算
        (open * 5 + returns_sum250 / 50) AS alpha008_sum_open_returns,  -- 简化计算
        
    LAG((open * 5 + returns_sum250 / 50), 10) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS alpha008_delay_sum,
        
        -- Alpha009-010 逻辑
        CASE 
            WHEN 
    MIN(close_delta1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 > 0 THEN close_delta1
            WHEN 
    MAX(close_delta1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 < 0 THEN close_delta1
            ELSE -1 * close_delta1
        END AS alpha009_logic,
        
        -- Alpha011 相关计算
        
    MAX(vwap - close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha011_max_vwap_close,
        
    MIN(vwap - close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha011_min_vwap_close,
        
        -- Alpha014 相关计算
        
    returns - 
    LAG(returns, 3) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS alpha014_delta_returns,
        
        -- Alpha015 相关计算
        
    -- 使用DuckDB的CORR窗口函数
    CORR(high_rank, volume_rank) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha015_corr_high_vol,
        
        -- Alpha017 相关计算
        
    close_delta1 - 
    LAG(close_delta1, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS alpha017_delta_delta_close,
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY 
    CASE 
        WHEN adv20 = 0 OR adv20 IS NULL THEN NULL
        WHEN ABS(adv20) < 1e-10 THEN NULL
        ELSE volume / adv20
    END

        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS alpha017_ts_rank_vol_adv,
        
        -- Alpha018 相关计算
        
    STDDEV(
    ABS(close - open)
) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS alpha018_stddev,
        close - open AS alpha018_close_open_diff,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(close, open) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS alpha018_corr_close_open,
        
        -- Alpha019 相关计算
        (close - close_lag7) + close_delta7 AS alpha019_close_diff_plus_delta,
        1 + returns_sum250 AS alpha019_sum_returns,
        
        -- Alpha020 相关计算
        open - high_lag1 AS alpha020_open_delay_high,
        open - close_lag1 AS alpha020_open_delay_close,
        open - low_lag1 AS alpha020_open_delay_low
        
    FROM base_data
),

-- 计算Alpha因子
alpha_factors AS (
    SELECT 
        symbol,
        timestamp,
        
        -- Alpha 001: RANK(Ts_ArgMax(SignedPower(((returns < 0) ? stddev(returns, 20) : close), 2.), 5)) - 0.5
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha001_argmax
    )
 - 0.5 AS alpha001,
        
        -- Alpha 002: (-1 * correlation(rank(delta(log(volume), 2)), rank(((close - open) / open)), 6))
        -1 * 
    -- 使用DuckDB的CORR窗口函数
    CORR(alpha002_rank_delta_log_vol, alpha002_rank_ret) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 5 PRECEDING AND CURRENT ROW
    )
 AS alpha002,
        
        -- Alpha 003: (-1 * correlation(rank(open), rank(volume), 10))
        -1 * corr_open_volume_10 AS alpha003,
        
        -- Alpha 004: (-1 * Ts_Rank(rank(low), 9))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY low_rank
        ROWS BETWEEN 8 PRECEDING AND CURRENT ROW
    )
 AS alpha004,
        
        -- Alpha 005: (rank((open - (sum(vwap, 10) / 10))) * (-1 * abs(rank((close - vwap)))))
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY open - alpha005_mean_vwap
    )
 * (-1 * 
    ABS(
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY close - vwap
    )
)
) AS alpha005,
        
        -- Alpha 006: (-1 * correlation(open, volume, 10))
        -1 * corr_open_volume_10 AS alpha006,
        
        -- Alpha 007: ((adv20 < volume) ? ((-1 * ts_rank(abs(delta(close, 7)), 60)) * sign(delta(close, 7))) : (-1))
        CASE 
            WHEN adv20 < volume THEN (-1 * alpha007_ts_rank) * alpha007_sign
            ELSE -1
        END AS alpha007,
        
        -- Alpha 008: (-1 * rank(((sum(open, 5) * sum(returns, 5)) - delay((sum(open, 5) * sum(returns, 5)), 10))))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha008_sum_open_returns - alpha008_delay_sum
    )
 AS alpha008,
        
        -- Alpha 009: ((0 < ts_min(delta(close, 1), 5)) ? delta(close, 1) : ((ts_max(delta(close, 1), 5) < 0) ? delta(close, 1) : (-1 * delta(close, 1))))
        alpha009_logic AS alpha009,
        
        -- Alpha 010: rank(((0 < ts_min(delta(close, 1), 4)) ? delta(close, 1) : ((ts_max(delta(close, 1), 4) < 0) ? delta(close, 1) : (-1 * delta(close, 1)))))
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha009_logic
    )
 AS alpha010,
        
        -- Alpha 011: ((rank(ts_max((vwap - close), 3)) + rank(ts_min((vwap - close), 3))) * rank(delta(volume, 3)))
        (
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha011_max_vwap_close
    )
 + 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha011_min_vwap_close
    )
) * 
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY volume_delta3
    )
 AS alpha011,
        
        -- Alpha 012: (sign(delta(volume, 1)) * (-1 * delta(close, 1)))
        
    CASE 
        WHEN volume_delta1 > 0 THEN 1
        WHEN volume_delta1 < 0 THEN -1
        ELSE 0
    END
 * (-1 * close_delta1) AS alpha012,
        
        -- Alpha 013: (-1 * rank(covariance(rank(close), rank(volume), 5)))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY cov_close_volume_5
    )
 AS alpha013,
        
        -- Alpha 014: ((-1 * rank(delta(returns, 3))) * correlation(open, volume, 10))
        (-1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha014_delta_returns
    )
) * corr_open_volume_10 AS alpha014,
        
        -- Alpha 015: (-1 * sum(rank(correlation(rank(high), rank(volume), 3)), 3))
        -1 * 
    SUM(
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha015_corr_high_vol
    )
) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha015,
        
        -- Alpha 016: (-1 * rank(covariance(rank(high), rank(volume), 5)))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY cov_high_volume_5
    )
 AS alpha016,
        
        -- Alpha 017: (((-1 * rank(ts_rank(close, 10))) * rank(delta(delta(close, 1), 1))) * rank(ts_rank((volume / adv20), 5)))
        ((-1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY close_ts_rank10
    )
) * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha017_delta_delta_close
    )
) * 
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha017_ts_rank_vol_adv
    )
 AS alpha017,
        
        -- Alpha 018: (-1 * rank(((stddev(abs((close - open)), 5) + (close - open)) + correlation(close, open, 10))))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha018_stddev + alpha018_close_open_diff + alpha018_corr_close_open
    )
 AS alpha018,
        
        -- Alpha 019: ((-1 * sign(((close - delay(close, 7)) + delta(close, 7)))) * (1 + rank((1 + sum(returns, 250)))))
        (-1 * 
    CASE 
        WHEN alpha019_close_diff_plus_delta > 0 THEN 1
        WHEN alpha019_close_diff_plus_delta < 0 THEN -1
        ELSE 0
    END
) * 
        (1 + 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha019_sum_returns
    )
) AS alpha019,
        
        -- Alpha 020: (((-1 * rank((open - delay(high, 1)))) * rank((open - delay(close, 1)))) * rank((open - delay(low, 1))))
        ((-1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha020_open_delay_high
    )
) * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha020_open_delay_close
    )
) * 
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha020_open_delay_low
    )
 AS alpha020
        
    FROM intermediate_calcs
)

SELECT * FROM alpha_factors
    );
  
  
[0m15:30:11.750503 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_factors_001_020"} */

  
    
    

    create  table
      "quant_features"."main"."alpha_factors_001_020__dbt_tmp"
  
    as (
      

-- Alpha 101 因子计算 (001-020)
-- 基于预处理的基础数据计算前20个Alpha因子

WITH base_data AS (
    SELECT * FROM "quant_features"."main"."alpha_base_data"
),

-- 预计算一些复杂的中间变量
intermediate_calcs AS (
    SELECT 
        *,
        -- Alpha001 相关计算
        
    -- 使用ROW_NUMBER()来找到最大值的位置
    (5 - 1) - (
        ROW_NUMBER() OVER (
            PARTITION BY symbol, 
            (CASE WHEN returns < 0 THEN returns_std20 ELSE close END = 
    MAX(CASE WHEN returns < 0 THEN returns_std20 ELSE close END) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
)
            ORDER BY timestamp DESC
        ) - 1
    )
 AS alpha001_argmax,
        
        -- Alpha002 相关计算
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY 
    
    CASE 
        WHEN volume > 0 THEN LN(volume)
        ELSE NULL
    END
 - 
    LAG(
    CASE 
        WHEN volume > 0 THEN LN(volume)
        ELSE NULL
    END
, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )


    )
 AS alpha002_rank_delta_log_vol,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY 
    CASE 
        WHEN open = 0 OR open IS NULL THEN NULL
        WHEN ABS(open) < 1e-10 THEN NULL
        ELSE close - open / open
    END

    )
 AS alpha002_rank_ret,
        
        -- Alpha005 相关计算
        close_ma10 AS alpha005_mean_vwap,
        
        -- Alpha007 相关计算
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY 
    ABS(close_delta7)

        ROWS BETWEEN 59 PRECEDING AND CURRENT ROW
    )
 AS alpha007_ts_rank,
        
    CASE 
        WHEN close_delta7 > 0 THEN 1
        WHEN close_delta7 < 0 THEN -1
        ELSE 0
    END
 AS alpha007_sign,
        
        -- Alpha008 相关计算
        (open * 5 + returns_sum250 / 50) AS alpha008_sum_open_returns,  -- 简化计算
        
    LAG((open * 5 + returns_sum250 / 50), 10) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS alpha008_delay_sum,
        
        -- Alpha009-010 逻辑
        CASE 
            WHEN 
    MIN(close_delta1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 > 0 THEN close_delta1
            WHEN 
    MAX(close_delta1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 < 0 THEN close_delta1
            ELSE -1 * close_delta1
        END AS alpha009_logic,
        
        -- Alpha011 相关计算
        
    MAX(vwap - close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha011_max_vwap_close,
        
    MIN(vwap - close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha011_min_vwap_close,
        
        -- Alpha014 相关计算
        
    returns - 
    LAG(returns, 3) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS alpha014_delta_returns,
        
        -- Alpha015 相关计算
        
    -- 使用DuckDB的CORR窗口函数
    CORR(high_rank, volume_rank) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha015_corr_high_vol,
        
        -- Alpha017 相关计算
        
    close_delta1 - 
    LAG(close_delta1, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS alpha017_delta_delta_close,
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY 
    CASE 
        WHEN adv20 = 0 OR adv20 IS NULL THEN NULL
        WHEN ABS(adv20) < 1e-10 THEN NULL
        ELSE volume / adv20
    END

        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS alpha017_ts_rank_vol_adv,
        
        -- Alpha018 相关计算
        
    STDDEV(
    ABS(close - open)
) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS alpha018_stddev,
        close - open AS alpha018_close_open_diff,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(close, open) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS alpha018_corr_close_open,
        
        -- Alpha019 相关计算
        (close - close_lag7) + close_delta7 AS alpha019_close_diff_plus_delta,
        1 + returns_sum250 AS alpha019_sum_returns,
        
        -- Alpha020 相关计算
        open - high_lag1 AS alpha020_open_delay_high,
        open - close_lag1 AS alpha020_open_delay_close,
        open - low_lag1 AS alpha020_open_delay_low
        
    FROM base_data
),

-- 计算Alpha因子
alpha_factors AS (
    SELECT 
        symbol,
        timestamp,
        
        -- Alpha 001: RANK(Ts_ArgMax(SignedPower(((returns < 0) ? stddev(returns, 20) : close), 2.), 5)) - 0.5
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha001_argmax
    )
 - 0.5 AS alpha001,
        
        -- Alpha 002: (-1 * correlation(rank(delta(log(volume), 2)), rank(((close - open) / open)), 6))
        -1 * 
    -- 使用DuckDB的CORR窗口函数
    CORR(alpha002_rank_delta_log_vol, alpha002_rank_ret) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 5 PRECEDING AND CURRENT ROW
    )
 AS alpha002,
        
        -- Alpha 003: (-1 * correlation(rank(open), rank(volume), 10))
        -1 * corr_open_volume_10 AS alpha003,
        
        -- Alpha 004: (-1 * Ts_Rank(rank(low), 9))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY low_rank
        ROWS BETWEEN 8 PRECEDING AND CURRENT ROW
    )
 AS alpha004,
        
        -- Alpha 005: (rank((open - (sum(vwap, 10) / 10))) * (-1 * abs(rank((close - vwap)))))
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY open - alpha005_mean_vwap
    )
 * (-1 * 
    ABS(
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY close - vwap
    )
)
) AS alpha005,
        
        -- Alpha 006: (-1 * correlation(open, volume, 10))
        -1 * corr_open_volume_10 AS alpha006,
        
        -- Alpha 007: ((adv20 < volume) ? ((-1 * ts_rank(abs(delta(close, 7)), 60)) * sign(delta(close, 7))) : (-1))
        CASE 
            WHEN adv20 < volume THEN (-1 * alpha007_ts_rank) * alpha007_sign
            ELSE -1
        END AS alpha007,
        
        -- Alpha 008: (-1 * rank(((sum(open, 5) * sum(returns, 5)) - delay((sum(open, 5) * sum(returns, 5)), 10))))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha008_sum_open_returns - alpha008_delay_sum
    )
 AS alpha008,
        
        -- Alpha 009: ((0 < ts_min(delta(close, 1), 5)) ? delta(close, 1) : ((ts_max(delta(close, 1), 5) < 0) ? delta(close, 1) : (-1 * delta(close, 1))))
        alpha009_logic AS alpha009,
        
        -- Alpha 010: rank(((0 < ts_min(delta(close, 1), 4)) ? delta(close, 1) : ((ts_max(delta(close, 1), 4) < 0) ? delta(close, 1) : (-1 * delta(close, 1)))))
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha009_logic
    )
 AS alpha010,
        
        -- Alpha 011: ((rank(ts_max((vwap - close), 3)) + rank(ts_min((vwap - close), 3))) * rank(delta(volume, 3)))
        (
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha011_max_vwap_close
    )
 + 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha011_min_vwap_close
    )
) * 
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY volume_delta3
    )
 AS alpha011,
        
        -- Alpha 012: (sign(delta(volume, 1)) * (-1 * delta(close, 1)))
        
    CASE 
        WHEN volume_delta1 > 0 THEN 1
        WHEN volume_delta1 < 0 THEN -1
        ELSE 0
    END
 * (-1 * close_delta1) AS alpha012,
        
        -- Alpha 013: (-1 * rank(covariance(rank(close), rank(volume), 5)))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY cov_close_volume_5
    )
 AS alpha013,
        
        -- Alpha 014: ((-1 * rank(delta(returns, 3))) * correlation(open, volume, 10))
        (-1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha014_delta_returns
    )
) * corr_open_volume_10 AS alpha014,
        
        -- Alpha 015: (-1 * sum(rank(correlation(rank(high), rank(volume), 3)), 3))
        -1 * 
    SUM(
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha015_corr_high_vol
    )
) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha015,
        
        -- Alpha 016: (-1 * rank(covariance(rank(high), rank(volume), 5)))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY cov_high_volume_5
    )
 AS alpha016,
        
        -- Alpha 017: (((-1 * rank(ts_rank(close, 10))) * rank(delta(delta(close, 1), 1))) * rank(ts_rank((volume / adv20), 5)))
        ((-1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY close_ts_rank10
    )
) * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha017_delta_delta_close
    )
) * 
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha017_ts_rank_vol_adv
    )
 AS alpha017,
        
        -- Alpha 018: (-1 * rank(((stddev(abs((close - open)), 5) + (close - open)) + correlation(close, open, 10))))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha018_stddev + alpha018_close_open_diff + alpha018_corr_close_open
    )
 AS alpha018,
        
        -- Alpha 019: ((-1 * sign(((close - delay(close, 7)) + delta(close, 7)))) * (1 + rank((1 + sum(returns, 250)))))
        (-1 * 
    CASE 
        WHEN alpha019_close_diff_plus_delta > 0 THEN 1
        WHEN alpha019_close_diff_plus_delta < 0 THEN -1
        ELSE 0
    END
) * 
        (1 + 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha019_sum_returns
    )
) AS alpha019,
        
        -- Alpha 020: (((-1 * rank((open - delay(high, 1)))) * rank((open - delay(close, 1)))) * rank((open - delay(low, 1))))
        ((-1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha020_open_delay_high
    )
) * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha020_open_delay_close
    )
) * 
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha020_open_delay_low
    )
 AS alpha020
        
    FROM intermediate_calcs
)

SELECT * FROM alpha_factors
    );
  
  
[0m15:30:11.751057 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m15:30:11.751335 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_001_020: ROLLBACK
[0m15:30:11.754636 [debug] [Thread-1 (]: Failed to rollback 'model.quant_features.alpha_factors_001_020'
[0m15:30:11.754906 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_001_020: Close
[0m15:30:11.757327 [debug] [Thread-1 (]: Runtime Error in model alpha_factors_001_020 (models/alpha101/alpha_factors_001_020.sql)
  Parser Error: window functions are not allowed in window definitions
[0m15:30:11.758492 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1ab1ef80-2fd2-4638-9b1b-959a0187c48b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a4f748f50>]}
[0m15:30:11.758960 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model main.alpha_factors_001_020 ............... [[31mERROR[0m in 0.05s]
[0m15:30:11.759318 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_factors_001_020
[0m15:30:11.760068 [debug] [Thread-7 (]: Marking all children of 'model.quant_features.alpha_factors_001_020' to be skipped because of status 'error'.  Reason: Runtime Error in model alpha_factors_001_020 (models/alpha101/alpha_factors_001_020.sql)
  Parser Error: window functions are not allowed in window definitions.
[0m15:30:11.762556 [debug] [MainThread]: Using duckdb connection "master"
[0m15:30:11.762801 [debug] [MainThread]: On master: BEGIN
[0m15:30:11.762964 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:30:11.763345 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:30:11.763530 [debug] [MainThread]: On master: COMMIT
[0m15:30:11.763691 [debug] [MainThread]: Using duckdb connection "master"
[0m15:30:11.763841 [debug] [MainThread]: On master: COMMIT
[0m15:30:11.764192 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:30:11.764401 [debug] [MainThread]: On master: Close
[0m15:30:11.764677 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:30:11.764834 [debug] [MainThread]: Connection 'model.quant_features.alpha_factors_001_020' was properly closed.
[0m15:30:11.764999 [info ] [MainThread]: 
[0m15:30:11.765179 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m15:30:11.765545 [debug] [MainThread]: Command end result
[0m15:30:11.787508 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:30:11.788702 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:30:11.792411 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:30:11.792637 [info ] [MainThread]: 
[0m15:30:11.792851 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:30:11.793030 [info ] [MainThread]: 
[0m15:30:11.793250 [error] [MainThread]: [31mFailure in model alpha_factors_001_020 (models/alpha101/alpha_factors_001_020.sql)[0m
[0m15:30:11.793453 [error] [MainThread]:   Runtime Error in model alpha_factors_001_020 (models/alpha101/alpha_factors_001_020.sql)
  Parser Error: window functions are not allowed in window definitions
[0m15:30:11.793606 [info ] [MainThread]: 
[0m15:30:11.793788 [info ] [MainThread]:   compiled code at target/compiled/quant_features/models/alpha101/alpha_factors_001_020.sql
[0m15:30:11.793931 [info ] [MainThread]: 
[0m15:30:11.794095 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m15:30:11.794633 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.65997976, "process_in_blocks": "0", "process_kernel_time": 0.159904, "process_mem_max_rss": "150912", "process_out_blocks": "3576", "process_user_time": 1.403141}
[0m15:30:11.794924 [debug] [MainThread]: Command `dbt run` failed at 15:30:11.794866 after 0.66 seconds
[0m15:30:11.795133 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a502475f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a54779810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a547799f0>]}
[0m15:30:11.795348 [debug] [MainThread]: Flushing usage events
[0m15:30:11.847239 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:31:28.548983 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43cf9e7770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43d102da90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43ce84fd90>]}


============================== 15:31:28.551268 | ab870e2e-37b6-4994-88c1-d5ee27c74a02 ==============================
[0m15:31:28.551268 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:31:28.551585 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'partial_parse': 'True', 'indirect_selection': 'eager', 'warn_error': 'None', 'use_experimental_parser': 'False', 'debug': 'False', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'quiet': 'False', 'introspect': 'True', 'use_colors': 'True', 'version_check': 'True', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'profiles_dir': '/workspace/dbt_project', 'static_parser': 'True', 'log_path': '/workspace/dbt_project/logs', 'log_format': 'default', 'invocation_command': 'dbt show --select alpha_base_data --limit 5', 'empty': 'None', 'no_print': 'None', 'log_cache_events': 'False'}
[0m15:31:28.691105 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ab870e2e-37b6-4994-88c1-d5ee27c74a02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43cf566650>]}
[0m15:31:28.732205 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ab870e2e-37b6-4994-88c1-d5ee27c74a02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43ce94abe0>]}
[0m15:31:28.740038 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:31:28.768921 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:31:28.855311 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m15:31:28.855702 [debug] [MainThread]: Partial parsing: added file: quant_features://models/alpha101/alpha_factors_simple.sql
[0m15:31:29.041758 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ab870e2e-37b6-4994-88c1-d5ee27c74a02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43ca4ca250>]}
[0m15:31:29.144932 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:31:29.146241 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:31:29.150148 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ab870e2e-37b6-4994-88c1-d5ee27c74a02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43ca7bd310>]}
[0m15:31:29.150493 [info ] [MainThread]: Found 16 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:31:29.150726 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ab870e2e-37b6-4994-88c1-d5ee27c74a02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43ca3d4f30>]}
[0m15:31:29.152100 [info ] [MainThread]: 
[0m15:31:29.152400 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:31:29.152582 [info ] [MainThread]: 
[0m15:31:29.152884 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:31:29.156587 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features_main'
[0m15:31:29.170523 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:31:29.170776 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:31:29.170958 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:31:29.183092 [debug] [ThreadPool]: SQL status: OK in 0.012 seconds
[0m15:31:29.183362 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:31:29.183547 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:31:29.190739 [debug] [ThreadPool]: SQL status: OK in 0.007 seconds
[0m15:31:29.191728 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:31:29.192393 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:31:29.192614 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:31:29.194405 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ab870e2e-37b6-4994-88c1-d5ee27c74a02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43ca2dc050>]}
[0m15:31:29.197371 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_base_data
[0m15:31:29.197773 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.alpha_base_data)
[0m15:31:29.198016 [debug] [Thread-1 (]: Began compiling node model.quant_features.alpha_base_data
[0m15:31:29.220181 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.alpha_base_data"
[0m15:31:29.220736 [debug] [Thread-1 (]: Began executing node model.quant_features.alpha_base_data
[0m15:31:29.225281 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_base_data"
[0m15:31:29.225826 [debug] [Thread-1 (]: On model.quant_features.alpha_base_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_base_data"} */

  
  

-- Alpha 101 基础数据准备
-- 为Alpha因子计算准备所有必要的基础数据

WITH base_ohlc AS (
    SELECT 
        symbol,
        timestamp,
        open,
        high,
        low,
        close,
        volume,
        -- 计算VWAP (简化版本，假设等权重)
        (high + low + close) / 3 AS vwap,
        -- 计算returns
        CASE 
            WHEN LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp) IS NOT NULL
            THEN (close - LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp)) / 
                 LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp)
            ELSE 0
        END AS returns
    FROM "quant_features"."main"."stg_ohlc_data"
    WHERE timestamp >= CAST('2020-01-01' AS DATE)
      AND timestamp <= CAST('2024-12-31' AS DATE)
),

enhanced_data AS (
    SELECT 
        *,
        -- 计算ADV (Average Daily Volume)
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )

 AS adv20,
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )

 AS adv10,
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )

 AS adv5,
        
        -- 预计算一些常用的时间序列指标
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_ma5,
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS close_ma10,
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_ma20,
        
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS volume_ma20,
        
        -- 预计算滚动标准差
        
    STDDEV(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_std20,
        
    STDDEV(returns) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS returns_std20,
        
        -- 预计算一些延迟项
        
    LAG(close, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag1,
        
    LAG(close, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag2,
        
    LAG(close, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag5,
        
    LAG(close, 10) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag10,
        
    LAG(close, 20) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag20,
        
    LAG(volume, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS volume_lag1,
        
    LAG(high, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS high_lag1,
        
    LAG(low, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS low_lag1,
        
    LAG(vwap, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS vwap_lag5,
        
        -- 预计算一些差值项
        
    close - 
    LAG(close, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta1,
        
    close - 
    LAG(close, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta2,
        
    close - 
    LAG(close, 3) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta3,
        
    close - 
    LAG(close, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta5,
        
    close - 
    LAG(close, 7) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta7,
        
    close - 
    LAG(close, 10) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta10,
        
    volume - 
    LAG(volume, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS volume_delta1,
        
    volume - 
    LAG(volume, 3) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS volume_delta3,
        
    high - 
    LAG(high, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS high_delta2,
        
        -- 预计算一些排序项
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY close
    )
 AS close_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY volume
    )
 AS volume_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY high
    )
 AS high_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY low
    )
 AS low_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY vwap
    )
 AS vwap_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY returns
    )
 AS returns_rank,
        
        -- 预计算时间序列排序
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY close
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS close_ts_rank10,
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY volume
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_ts_rank5,
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY high
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS high_ts_rank5,
        
        -- 预计算一些最值项
        
    MIN(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 99 PRECEDING AND CURRENT ROW
    )
 AS close_min100,
        
    MAX(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS close_max3,
        
    MIN(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_min5,
        
    MAX(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_max5,
        
    MIN(low) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS low_min5,
        
    MAX(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS high_max3,
        
        -- 预计算一些求和项
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_sum5,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 7 PRECEDING AND CURRENT ROW
    )
 AS close_sum8,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_sum20,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 99 PRECEDING AND CURRENT ROW
    )
 AS close_sum100,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 199 PRECEDING AND CURRENT ROW
    )
 AS close_sum200,
        
    SUM(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_sum5,
        
    SUM(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS volume_sum20,
        
    SUM(returns) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 249 PRECEDING AND CURRENT ROW
    )
 AS returns_sum250,
        
    SUM(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS high_sum5,
        
    SUM(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS high_sum20,
        
        -- 预计算一些相关性
        
    -- 使用DuckDB的CORR窗口函数
    CORR(close, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS corr_close_volume_10,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(open, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS corr_open_volume_10,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(high, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS corr_high_volume_5,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(vwap, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 5 PRECEDING AND CURRENT ROW
    )
 AS corr_vwap_volume_6,
        
        -- 预计算一些协方差
        
    COVAR_SAMP(close, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS cov_close_volume_5,
        
    COVAR_SAMP(high, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS cov_high_volume_5
        
    FROM base_ohlc
    WHERE timestamp >= CAST(CAST('2020-01-01' AS DATE) AS DATE) - INTERVAL '250 days'  -- 扩展时间范围以确保有足够的历史数据
),

-- 过滤回原始时间范围
final_data AS (
    SELECT *
    FROM enhanced_data
    WHERE timestamp >= CAST('2020-01-01' AS DATE)
      AND timestamp <= CAST('2024-12-31' AS DATE)
)

SELECT * FROM final_data
  
  limit 5

[0m15:31:29.226321 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:31:29.250169 [debug] [Thread-1 (]: SQL status: OK in 0.024 seconds
[0m15:31:29.254379 [debug] [Thread-1 (]: On model.quant_features.alpha_base_data: Close
[0m15:31:29.255662 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_base_data
[0m15:31:29.256634 [debug] [Thread-2 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78
[0m15:31:29.257149 [debug] [Thread-2 (]: Acquiring new duckdb connection 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78'
[0m15:31:29.257441 [debug] [Thread-1 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0
[0m15:31:29.257976 [debug] [Thread-3 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d
[0m15:31:29.258566 [debug] [Thread-2 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78
[0m15:31:29.258930 [debug] [Thread-4 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352
[0m15:31:29.259276 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.quant_features.alpha_base_data, now test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0)
[0m15:31:29.259776 [debug] [Thread-3 (]: Acquiring new duckdb connection 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d'
[0m15:31:29.264702 [debug] [Thread-4 (]: Acquiring new duckdb connection 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352'
[0m15:31:29.265183 [debug] [Thread-1 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0
[0m15:31:29.266581 [debug] [Thread-3 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d
[0m15:31:29.267276 [debug] [Thread-4 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352
[0m15:31:29.279020 [debug] [Thread-4 (]: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_volume__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:31:29.279437 [debug] [Thread-4 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352
[0m15:31:29.279820 [debug] [Thread-4 (]: Began running node test.quant_features.not_null_alpha_base_data_close.6db60d82aa
[0m15:31:29.280983 [debug] [Thread-3 (]: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_returns__10___1 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:31:29.282354 [debug] [Thread-1 (]: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_vwap__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:31:29.282798 [debug] [Thread-7 (]: Marking all children of 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352' to be skipped because of status 'error'.  Reason: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_volume__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m15:31:29.283330 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352, now test.quant_features.not_null_alpha_base_data_close.6db60d82aa)
[0m15:31:29.284595 [debug] [Thread-2 (]: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_close__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:31:29.285232 [debug] [Thread-3 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d
[0m15:31:29.285872 [debug] [Thread-1 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0
[0m15:31:29.286689 [debug] [Thread-4 (]: Began compiling node test.quant_features.not_null_alpha_base_data_close.6db60d82aa
[0m15:31:29.287148 [debug] [Thread-2 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78
[0m15:31:29.287495 [debug] [Thread-3 (]: Began running node test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd
[0m15:31:29.287829 [debug] [Thread-7 (]: Marking all children of 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d' to be skipped because of status 'error'.  Reason: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_returns__10___1 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m15:31:29.288228 [debug] [Thread-1 (]: Began running node test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974
[0m15:31:29.294244 [debug] [Thread-2 (]: Began running node test.quant_features.not_null_alpha_base_data_volume.21044032e6
[0m15:31:29.293848 [debug] [Thread-4 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_close.6db60d82aa"
[0m15:31:29.294920 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d, now test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd)
[0m15:31:29.295350 [debug] [Thread-7 (]: Marking all children of 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0' to be skipped because of status 'error'.  Reason: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_vwap__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m15:31:29.295666 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0, now test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974)
[0m15:31:29.296045 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78, now test.quant_features.not_null_alpha_base_data_volume.21044032e6)
[0m15:31:29.296435 [debug] [Thread-3 (]: Began compiling node test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd
[0m15:31:29.296974 [debug] [Thread-7 (]: Marking all children of 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78' to be skipped because of status 'error'.  Reason: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_close__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m15:31:29.297309 [debug] [Thread-4 (]: Began executing node test.quant_features.not_null_alpha_base_data_close.6db60d82aa
[0m15:31:29.297794 [debug] [Thread-1 (]: Began compiling node test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974
[0m15:31:29.298123 [debug] [Thread-2 (]: Began compiling node test.quant_features.not_null_alpha_base_data_volume.21044032e6
[0m15:31:29.300912 [debug] [Thread-3 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd"
[0m15:31:29.303383 [debug] [Thread-4 (]: Using duckdb connection "test.quant_features.not_null_alpha_base_data_close.6db60d82aa"
[0m15:31:29.306019 [debug] [Thread-1 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974"
[0m15:31:29.308671 [debug] [Thread-2 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_volume.21044032e6"
[0m15:31:29.309291 [debug] [Thread-3 (]: Began executing node test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd
[0m15:31:29.309813 [debug] [Thread-4 (]: On test.quant_features.not_null_alpha_base_data_close.6db60d82aa: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "test.quant_features.not_null_alpha_base_data_close.6db60d82aa"} */

  
  
    
    



select close
from "quant_features"."main"."alpha_base_data"
where close is null



  
  limit 5

[0m15:31:29.310358 [debug] [Thread-1 (]: Began executing node test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974
[0m15:31:29.312716 [debug] [Thread-3 (]: Using duckdb connection "test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd"
[0m15:31:29.313233 [debug] [Thread-2 (]: Began executing node test.quant_features.not_null_alpha_base_data_volume.21044032e6
[0m15:31:29.313745 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m15:31:29.315979 [debug] [Thread-1 (]: Using duckdb connection "test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974"
[0m15:31:29.316393 [debug] [Thread-3 (]: On test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd"} */

  
  
    
    



select symbol
from "quant_features"."main"."alpha_base_data"
where symbol is null



  
  limit 5

[0m15:31:29.318335 [debug] [Thread-2 (]: Using duckdb connection "test.quant_features.not_null_alpha_base_data_volume.21044032e6"
[0m15:31:29.319065 [debug] [Thread-1 (]: On test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974"} */

  
  
    
    



select timestamp
from "quant_features"."main"."alpha_base_data"
where timestamp is null



  
  limit 5

[0m15:31:29.319417 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:31:29.319720 [debug] [Thread-4 (]: SQL status: OK in 0.006 seconds
[0m15:31:29.320270 [debug] [Thread-2 (]: On test.quant_features.not_null_alpha_base_data_volume.21044032e6: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "test.quant_features.not_null_alpha_base_data_volume.21044032e6"} */

  
  
    
    



select volume
from "quant_features"."main"."alpha_base_data"
where volume is null



  
  limit 5

[0m15:31:29.320841 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:31:29.322086 [debug] [Thread-4 (]: On test.quant_features.not_null_alpha_base_data_close.6db60d82aa: Close
[0m15:31:29.322609 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:31:29.323844 [debug] [Thread-3 (]: SQL status: OK in 0.004 seconds
[0m15:31:29.324388 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m15:31:29.325220 [debug] [Thread-3 (]: On test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd: Close
[0m15:31:29.325770 [debug] [Thread-3 (]: Finished running node test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd
[0m15:31:29.326014 [debug] [Thread-3 (]: Began running node test.quant_features.not_null_alpha_base_data_vwap.380f9f922d
[0m15:31:29.326256 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd, now test.quant_features.not_null_alpha_base_data_vwap.380f9f922d)
[0m15:31:29.327064 [debug] [Thread-1 (]: On test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974: Close
[0m15:31:29.327696 [debug] [Thread-4 (]: Finished running node test.quant_features.not_null_alpha_base_data_close.6db60d82aa
[0m15:31:29.328239 [debug] [Thread-2 (]: SQL status: OK in 0.006 seconds
[0m15:31:29.329045 [debug] [Thread-3 (]: Began compiling node test.quant_features.not_null_alpha_base_data_vwap.380f9f922d
[0m15:31:29.329942 [debug] [Thread-1 (]: Finished running node test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974
[0m15:31:29.330926 [debug] [Thread-2 (]: On test.quant_features.not_null_alpha_base_data_volume.21044032e6: Close
[0m15:31:29.333416 [debug] [Thread-3 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_vwap.380f9f922d"
[0m15:31:29.334515 [debug] [Thread-2 (]: Finished running node test.quant_features.not_null_alpha_base_data_volume.21044032e6
[0m15:31:29.335047 [debug] [Thread-3 (]: Began executing node test.quant_features.not_null_alpha_base_data_vwap.380f9f922d
[0m15:31:29.337190 [debug] [Thread-3 (]: Using duckdb connection "test.quant_features.not_null_alpha_base_data_vwap.380f9f922d"
[0m15:31:29.337471 [debug] [Thread-3 (]: On test.quant_features.not_null_alpha_base_data_vwap.380f9f922d: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "test.quant_features.not_null_alpha_base_data_vwap.380f9f922d"} */

  
  
    
    



select vwap
from "quant_features"."main"."alpha_base_data"
where vwap is null



  
  limit 5

[0m15:31:29.337673 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:31:29.338266 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m15:31:29.338993 [debug] [Thread-3 (]: On test.quant_features.not_null_alpha_base_data_vwap.380f9f922d: Close
[0m15:31:29.339485 [debug] [Thread-3 (]: Finished running node test.quant_features.not_null_alpha_base_data_vwap.380f9f922d
[0m15:31:29.339873 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:31:29.340240 [debug] [MainThread]: Connection 'test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974' was properly closed.
[0m15:31:29.340424 [debug] [MainThread]: Connection 'test.quant_features.not_null_alpha_base_data_volume.21044032e6' was properly closed.
[0m15:31:29.340565 [debug] [MainThread]: Connection 'test.quant_features.not_null_alpha_base_data_vwap.380f9f922d' was properly closed.
[0m15:31:29.340699 [debug] [MainThread]: Connection 'test.quant_features.not_null_alpha_base_data_close.6db60d82aa' was properly closed.
[0m15:31:29.340954 [error] [MainThread]: Encountered an error:
Runtime Error
  Compilation Error in test dbt_utils_accepted_range_alpha_base_data_close__0 (models/alpha101/schema.yml)
    'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:31:29.341590 [debug] [MainThread]: Resource report: {"command_name": "show", "command_success": false, "command_wall_clock_time": 0.8283336, "process_in_blocks": "0", "process_kernel_time": 0.179873, "process_mem_max_rss": "182048", "process_out_blocks": "3464", "process_user_time": 1.558881}
[0m15:31:29.341876 [debug] [MainThread]: Command `dbt show` failed at 15:31:29.341818 after 0.83 seconds
[0m15:31:29.342086 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43cebe7110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43ca3ce780>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43ca3ced00>]}
[0m15:31:29.342306 [debug] [MainThread]: Flushing usage events
[0m15:31:29.397316 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:31:32.644304 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0dbcc6b770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0dbe2a5a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0dbbae7d90>]}


============================== 15:31:32.646549 | fa49536c-51a7-40fe-ac5a-0f7ec7d5d401 ==============================
[0m15:31:32.646549 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:31:32.646869 [debug] [MainThread]: running dbt with arguments {'cache_selected_only': 'False', 'profiles_dir': '/workspace/dbt_project', 'static_parser': 'True', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'quiet': 'False', 'printer_width': '80', 'log_path': '/workspace/dbt_project/logs', 'send_anonymous_usage_stats': 'True', 'version_check': 'True', 'target_path': 'None', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'partial_parse': 'True', 'warn_error': 'None', 'use_colors': 'True', 'log_format': 'default', 'indirect_selection': 'eager', 'no_print': 'None', 'invocation_command': 'dbt show --select alpha_base_data --limit 3'}
[0m15:31:32.780687 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fa49536c-51a7-40fe-ac5a-0f7ec7d5d401', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0dbc7fe650>]}
[0m15:31:32.821815 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fa49536c-51a7-40fe-ac5a-0f7ec7d5d401', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0dbbbdebe0>]}
[0m15:31:32.829796 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:31:32.859234 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:31:32.945552 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:31:32.945800 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:31:32.982384 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fa49536c-51a7-40fe-ac5a-0f7ec7d5d401', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0db79e1650>]}
[0m15:31:33.053911 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:31:33.055078 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:31:33.059146 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fa49536c-51a7-40fe-ac5a-0f7ec7d5d401', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0db79cd310>]}
[0m15:31:33.059485 [info ] [MainThread]: Found 16 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:31:33.059696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fa49536c-51a7-40fe-ac5a-0f7ec7d5d401', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0db7a58590>]}
[0m15:31:33.061053 [info ] [MainThread]: 
[0m15:31:33.061324 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:31:33.061495 [info ] [MainThread]: 
[0m15:31:33.061784 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:31:33.065511 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features_main'
[0m15:31:33.111711 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:31:33.111960 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:31:33.112128 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:31:33.123594 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m15:31:33.123846 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:31:33.124025 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:31:33.131279 [debug] [ThreadPool]: SQL status: OK in 0.007 seconds
[0m15:31:33.132244 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:31:33.132876 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:31:33.133087 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:31:33.134771 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fa49536c-51a7-40fe-ac5a-0f7ec7d5d401', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0db75fcbb0>]}
[0m15:31:33.137578 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_base_data
[0m15:31:33.137966 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.alpha_base_data)
[0m15:31:33.138194 [debug] [Thread-1 (]: Began compiling node model.quant_features.alpha_base_data
[0m15:31:33.160286 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.alpha_base_data"
[0m15:31:33.160761 [debug] [Thread-1 (]: Began executing node model.quant_features.alpha_base_data
[0m15:31:33.165176 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_base_data"
[0m15:31:33.165726 [debug] [Thread-1 (]: On model.quant_features.alpha_base_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_base_data"} */

  
  

-- Alpha 101 基础数据准备
-- 为Alpha因子计算准备所有必要的基础数据

WITH base_ohlc AS (
    SELECT 
        symbol,
        timestamp,
        open,
        high,
        low,
        close,
        volume,
        -- 计算VWAP (简化版本，假设等权重)
        (high + low + close) / 3 AS vwap,
        -- 计算returns
        CASE 
            WHEN LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp) IS NOT NULL
            THEN (close - LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp)) / 
                 LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp)
            ELSE 0
        END AS returns
    FROM "quant_features"."main"."stg_ohlc_data"
    WHERE timestamp >= CAST('2020-01-01' AS DATE)
      AND timestamp <= CAST('2024-12-31' AS DATE)
),

enhanced_data AS (
    SELECT 
        *,
        -- 计算ADV (Average Daily Volume)
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )

 AS adv20,
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )

 AS adv10,
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )

 AS adv5,
        
        -- 预计算一些常用的时间序列指标
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_ma5,
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS close_ma10,
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_ma20,
        
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS volume_ma20,
        
        -- 预计算滚动标准差
        
    STDDEV(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_std20,
        
    STDDEV(returns) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS returns_std20,
        
        -- 预计算一些延迟项
        
    LAG(close, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag1,
        
    LAG(close, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag2,
        
    LAG(close, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag5,
        
    LAG(close, 10) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag10,
        
    LAG(close, 20) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag20,
        
    LAG(volume, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS volume_lag1,
        
    LAG(high, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS high_lag1,
        
    LAG(low, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS low_lag1,
        
    LAG(vwap, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS vwap_lag5,
        
        -- 预计算一些差值项
        
    close - 
    LAG(close, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta1,
        
    close - 
    LAG(close, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta2,
        
    close - 
    LAG(close, 3) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta3,
        
    close - 
    LAG(close, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta5,
        
    close - 
    LAG(close, 7) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta7,
        
    close - 
    LAG(close, 10) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta10,
        
    volume - 
    LAG(volume, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS volume_delta1,
        
    volume - 
    LAG(volume, 3) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS volume_delta3,
        
    high - 
    LAG(high, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS high_delta2,
        
        -- 预计算一些排序项
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY close
    )
 AS close_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY volume
    )
 AS volume_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY high
    )
 AS high_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY low
    )
 AS low_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY vwap
    )
 AS vwap_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY returns
    )
 AS returns_rank,
        
        -- 预计算时间序列排序
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY close
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS close_ts_rank10,
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY volume
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_ts_rank5,
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY high
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS high_ts_rank5,
        
        -- 预计算一些最值项
        
    MIN(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 99 PRECEDING AND CURRENT ROW
    )
 AS close_min100,
        
    MAX(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS close_max3,
        
    MIN(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_min5,
        
    MAX(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_max5,
        
    MIN(low) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS low_min5,
        
    MAX(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS high_max3,
        
        -- 预计算一些求和项
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_sum5,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 7 PRECEDING AND CURRENT ROW
    )
 AS close_sum8,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_sum20,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 99 PRECEDING AND CURRENT ROW
    )
 AS close_sum100,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 199 PRECEDING AND CURRENT ROW
    )
 AS close_sum200,
        
    SUM(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_sum5,
        
    SUM(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS volume_sum20,
        
    SUM(returns) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 249 PRECEDING AND CURRENT ROW
    )
 AS returns_sum250,
        
    SUM(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS high_sum5,
        
    SUM(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS high_sum20,
        
        -- 预计算一些相关性
        
    -- 使用DuckDB的CORR窗口函数
    CORR(close, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS corr_close_volume_10,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(open, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS corr_open_volume_10,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(high, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS corr_high_volume_5,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(vwap, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 5 PRECEDING AND CURRENT ROW
    )
 AS corr_vwap_volume_6,
        
        -- 预计算一些协方差
        
    COVAR_SAMP(close, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS cov_close_volume_5,
        
    COVAR_SAMP(high, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS cov_high_volume_5
        
    FROM base_ohlc
    WHERE timestamp >= CAST(CAST('2020-01-01' AS DATE) AS DATE) - INTERVAL '250 days'  -- 扩展时间范围以确保有足够的历史数据
),

-- 过滤回原始时间范围
final_data AS (
    SELECT *
    FROM enhanced_data
    WHERE timestamp >= CAST('2020-01-01' AS DATE)
      AND timestamp <= CAST('2024-12-31' AS DATE)
)

SELECT * FROM final_data
  
  limit 3

[0m15:31:33.166221 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:31:33.188353 [debug] [Thread-1 (]: SQL status: OK in 0.022 seconds
[0m15:31:33.191771 [debug] [Thread-1 (]: On model.quant_features.alpha_base_data: Close
[0m15:31:33.193056 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_base_data
[0m15:31:33.194321 [debug] [Thread-3 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d
[0m15:31:33.194771 [debug] [Thread-4 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352
[0m15:31:33.195250 [debug] [Thread-2 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78
[0m15:31:33.195580 [debug] [Thread-1 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0
[0m15:31:33.196028 [debug] [Thread-3 (]: Acquiring new duckdb connection 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d'
[0m15:31:33.196704 [debug] [Thread-4 (]: Acquiring new duckdb connection 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352'
[0m15:31:33.197386 [debug] [Thread-2 (]: Acquiring new duckdb connection 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78'
[0m15:31:33.197924 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.quant_features.alpha_base_data, now test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0)
[0m15:31:33.198419 [debug] [Thread-3 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d
[0m15:31:33.198850 [debug] [Thread-4 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352
[0m15:31:33.199298 [debug] [Thread-2 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78
[0m15:31:33.199872 [debug] [Thread-1 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0
[0m15:31:33.213079 [debug] [Thread-3 (]: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_returns__10___1 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:31:33.217263 [debug] [Thread-1 (]: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_vwap__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:31:33.218539 [debug] [Thread-2 (]: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_close__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:31:33.219835 [debug] [Thread-4 (]: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_volume__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:31:33.220518 [debug] [Thread-3 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d
[0m15:31:33.221227 [debug] [Thread-1 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0
[0m15:31:33.221644 [debug] [Thread-2 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78
[0m15:31:33.222320 [debug] [Thread-4 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352
[0m15:31:33.222684 [debug] [Thread-3 (]: Began running node test.quant_features.not_null_alpha_base_data_close.6db60d82aa
[0m15:31:33.223106 [debug] [Thread-7 (]: Marking all children of 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d' to be skipped because of status 'error'.  Reason: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_returns__10___1 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m15:31:33.223510 [debug] [Thread-1 (]: Began running node test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd
[0m15:31:33.223869 [debug] [Thread-2 (]: Began running node test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974
[0m15:31:33.224195 [debug] [Thread-4 (]: Began running node test.quant_features.not_null_alpha_base_data_volume.21044032e6
[0m15:31:33.224518 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d, now test.quant_features.not_null_alpha_base_data_close.6db60d82aa)
[0m15:31:33.225424 [debug] [Thread-7 (]: Marking all children of 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0' to be skipped because of status 'error'.  Reason: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_vwap__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m15:31:33.225941 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0, now test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd)
[0m15:31:33.226393 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78, now test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974)
[0m15:31:33.226798 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352, now test.quant_features.not_null_alpha_base_data_volume.21044032e6)
[0m15:31:33.227178 [debug] [Thread-3 (]: Began compiling node test.quant_features.not_null_alpha_base_data_close.6db60d82aa
[0m15:31:33.227586 [debug] [Thread-7 (]: Marking all children of 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78' to be skipped because of status 'error'.  Reason: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_close__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m15:31:33.227877 [debug] [Thread-1 (]: Began compiling node test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd
[0m15:31:33.228188 [debug] [Thread-2 (]: Began compiling node test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974
[0m15:31:33.228553 [debug] [Thread-4 (]: Began compiling node test.quant_features.not_null_alpha_base_data_volume.21044032e6
[0m15:31:33.234155 [debug] [Thread-3 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_close.6db60d82aa"
[0m15:31:33.234763 [debug] [Thread-7 (]: Marking all children of 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352' to be skipped because of status 'error'.  Reason: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_volume__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m15:31:33.237503 [debug] [Thread-1 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd"
[0m15:31:33.240464 [debug] [Thread-2 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974"
[0m15:31:33.243340 [debug] [Thread-4 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_volume.21044032e6"
[0m15:31:33.244091 [debug] [Thread-3 (]: Began executing node test.quant_features.not_null_alpha_base_data_close.6db60d82aa
[0m15:31:33.244651 [debug] [Thread-1 (]: Began executing node test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd
[0m15:31:33.245348 [debug] [Thread-2 (]: Began executing node test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974
[0m15:31:33.246152 [debug] [Thread-4 (]: Began executing node test.quant_features.not_null_alpha_base_data_volume.21044032e6
[0m15:31:33.248416 [debug] [Thread-3 (]: Using duckdb connection "test.quant_features.not_null_alpha_base_data_close.6db60d82aa"
[0m15:31:33.250717 [debug] [Thread-1 (]: Using duckdb connection "test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd"
[0m15:31:33.252983 [debug] [Thread-2 (]: Using duckdb connection "test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974"
[0m15:31:33.255257 [debug] [Thread-4 (]: Using duckdb connection "test.quant_features.not_null_alpha_base_data_volume.21044032e6"
[0m15:31:33.255703 [debug] [Thread-3 (]: On test.quant_features.not_null_alpha_base_data_close.6db60d82aa: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "test.quant_features.not_null_alpha_base_data_close.6db60d82aa"} */

  
  
    
    



select close
from "quant_features"."main"."alpha_base_data"
where close is null



  
  limit 3

[0m15:31:33.256103 [debug] [Thread-1 (]: On test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd"} */

  
  
    
    



select symbol
from "quant_features"."main"."alpha_base_data"
where symbol is null



  
  limit 3

[0m15:31:33.256489 [debug] [Thread-2 (]: On test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974"} */

  
  
    
    



select timestamp
from "quant_features"."main"."alpha_base_data"
where timestamp is null



  
  limit 3

[0m15:31:33.257000 [debug] [Thread-4 (]: On test.quant_features.not_null_alpha_base_data_volume.21044032e6: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "test.quant_features.not_null_alpha_base_data_volume.21044032e6"} */

  
  
    
    



select volume
from "quant_features"."main"."alpha_base_data"
where volume is null



  
  limit 3

[0m15:31:33.257499 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:31:33.257860 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:31:33.258357 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:31:33.258695 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m15:31:33.259590 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m15:31:33.260097 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:31:33.260457 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m15:31:33.261310 [debug] [Thread-3 (]: On test.quant_features.not_null_alpha_base_data_close.6db60d82aa: Close
[0m15:31:33.262441 [debug] [Thread-1 (]: On test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd: Close
[0m15:31:33.263457 [debug] [Thread-2 (]: On test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974: Close
[0m15:31:33.263960 [debug] [Thread-4 (]: SQL status: OK in 0.005 seconds
[0m15:31:33.264713 [debug] [Thread-3 (]: Finished running node test.quant_features.not_null_alpha_base_data_close.6db60d82aa
[0m15:31:33.265455 [debug] [Thread-1 (]: Finished running node test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd
[0m15:31:33.266193 [debug] [Thread-2 (]: Finished running node test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974
[0m15:31:33.267208 [debug] [Thread-4 (]: On test.quant_features.not_null_alpha_base_data_volume.21044032e6: Close
[0m15:31:33.267582 [debug] [Thread-3 (]: Began running node test.quant_features.not_null_alpha_base_data_vwap.380f9f922d
[0m15:31:33.268097 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.quant_features.not_null_alpha_base_data_close.6db60d82aa, now test.quant_features.not_null_alpha_base_data_vwap.380f9f922d)
[0m15:31:33.268599 [debug] [Thread-4 (]: Finished running node test.quant_features.not_null_alpha_base_data_volume.21044032e6
[0m15:31:33.269031 [debug] [Thread-3 (]: Began compiling node test.quant_features.not_null_alpha_base_data_vwap.380f9f922d
[0m15:31:33.272333 [debug] [Thread-3 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_vwap.380f9f922d"
[0m15:31:33.272805 [debug] [Thread-3 (]: Began executing node test.quant_features.not_null_alpha_base_data_vwap.380f9f922d
[0m15:31:33.274905 [debug] [Thread-3 (]: Using duckdb connection "test.quant_features.not_null_alpha_base_data_vwap.380f9f922d"
[0m15:31:33.275177 [debug] [Thread-3 (]: On test.quant_features.not_null_alpha_base_data_vwap.380f9f922d: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "test.quant_features.not_null_alpha_base_data_vwap.380f9f922d"} */

  
  
    
    



select vwap
from "quant_features"."main"."alpha_base_data"
where vwap is null



  
  limit 3

[0m15:31:33.275395 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:31:33.275964 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m15:31:33.276730 [debug] [Thread-3 (]: On test.quant_features.not_null_alpha_base_data_vwap.380f9f922d: Close
[0m15:31:33.277234 [debug] [Thread-3 (]: Finished running node test.quant_features.not_null_alpha_base_data_vwap.380f9f922d
[0m15:31:33.277837 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:31:33.278154 [debug] [MainThread]: Connection 'test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd' was properly closed.
[0m15:31:33.278343 [debug] [MainThread]: Connection 'test.quant_features.not_null_alpha_base_data_vwap.380f9f922d' was properly closed.
[0m15:31:33.278510 [debug] [MainThread]: Connection 'test.quant_features.not_null_alpha_base_data_volume.21044032e6' was properly closed.
[0m15:31:33.278647 [debug] [MainThread]: Connection 'test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974' was properly closed.
[0m15:31:33.278899 [error] [MainThread]: Encountered an error:
Runtime Error
  Compilation Error in test dbt_utils_accepted_range_alpha_base_data_volume__0 (models/alpha101/schema.yml)
    'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:31:33.279580 [debug] [MainThread]: Resource report: {"command_name": "show", "command_success": false, "command_wall_clock_time": 0.67171174, "process_in_blocks": "0", "process_kernel_time": 0.128841, "process_mem_max_rss": "178868", "process_out_blocks": "2000", "process_user_time": 1.449423}
[0m15:31:33.279875 [debug] [MainThread]: Command `dbt show` failed at 15:31:33.279817 after 0.67 seconds
[0m15:31:33.280077 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0dbbc83110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0db773d7b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0db773dff0>]}
[0m15:31:33.280304 [debug] [MainThread]: Flushing usage events
[0m15:31:33.324302 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:31:40.651720 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d7c2d3770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d7d925a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d7b13fd90>]}


============================== 15:31:40.654086 | 18b0e479-fd97-4763-98ef-e9cea82926cc ==============================
[0m15:31:40.654086 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:31:40.654426 [debug] [MainThread]: running dbt with arguments {'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'quiet': 'False', 'profiles_dir': '/workspace/dbt_project', 'log_path': '/workspace/dbt_project/logs', 'use_colors': 'True', 'printer_width': '80', 'empty': 'None', 'warn_error': 'None', 'indirect_selection': 'eager', 'invocation_command': 'dbt show --select alpha_base_data --limit 3', 'no_print': 'None', 'version_check': 'True', 'cache_selected_only': 'False', 'log_cache_events': 'False', 'static_parser': 'True', 'log_format': 'default', 'send_anonymous_usage_stats': 'True', 'write_json': 'True', 'target_path': 'None', 'use_experimental_parser': 'False', 'debug': 'False', 'fail_fast': 'False', 'introspect': 'True'}
[0m15:31:40.795194 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '18b0e479-fd97-4763-98ef-e9cea82926cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d7be56650>]}
[0m15:31:40.836026 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '18b0e479-fd97-4763-98ef-e9cea82926cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d7b236be0>]}
[0m15:31:40.843888 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:31:40.873484 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:31:40.960017 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:31:40.960266 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:31:40.996900 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '18b0e479-fd97-4763-98ef-e9cea82926cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d770d1650>]}
[0m15:31:41.067745 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:31:41.068924 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:31:41.073139 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '18b0e479-fd97-4763-98ef-e9cea82926cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d770c1310>]}
[0m15:31:41.073473 [info ] [MainThread]: Found 16 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:31:41.073679 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '18b0e479-fd97-4763-98ef-e9cea82926cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d77148590>]}
[0m15:31:41.074979 [info ] [MainThread]: 
[0m15:31:41.075249 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:31:41.075426 [info ] [MainThread]: 
[0m15:31:41.075720 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:31:41.079452 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features_main'
[0m15:31:41.126884 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:31:41.127146 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:31:41.127333 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:31:41.138852 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m15:31:41.139105 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:31:41.139297 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:31:41.146224 [debug] [ThreadPool]: SQL status: OK in 0.007 seconds
[0m15:31:41.147188 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:31:41.147792 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:31:41.148001 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:31:41.149842 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '18b0e479-fd97-4763-98ef-e9cea82926cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d76cecbb0>]}
[0m15:31:41.151974 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_base_data
[0m15:31:41.152312 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.alpha_base_data)
[0m15:31:41.152541 [debug] [Thread-1 (]: Began compiling node model.quant_features.alpha_base_data
[0m15:31:41.174853 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.alpha_base_data"
[0m15:31:41.175299 [debug] [Thread-1 (]: Began executing node model.quant_features.alpha_base_data
[0m15:31:41.179730 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_base_data"
[0m15:31:41.180283 [debug] [Thread-1 (]: On model.quant_features.alpha_base_data: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_base_data"} */

  
  

-- Alpha 101 基础数据准备
-- 为Alpha因子计算准备所有必要的基础数据

WITH base_ohlc AS (
    SELECT 
        symbol,
        timestamp,
        open,
        high,
        low,
        close,
        volume,
        -- 计算VWAP (简化版本，假设等权重)
        (high + low + close) / 3 AS vwap,
        -- 计算returns
        CASE 
            WHEN LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp) IS NOT NULL
            THEN (close - LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp)) / 
                 LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp)
            ELSE 0
        END AS returns
    FROM "quant_features"."main"."stg_ohlc_data"
    WHERE timestamp >= CAST('2020-01-01' AS DATE)
      AND timestamp <= CAST('2024-12-31' AS DATE)
),

enhanced_data AS (
    SELECT 
        *,
        -- 计算ADV (Average Daily Volume)
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )

 AS adv20,
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )

 AS adv10,
        
    
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )

 AS adv5,
        
        -- 预计算一些常用的时间序列指标
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_ma5,
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS close_ma10,
        
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_ma20,
        
    AVG(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS volume_ma20,
        
        -- 预计算滚动标准差
        
    STDDEV(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_std20,
        
    STDDEV(returns) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS returns_std20,
        
        -- 预计算一些延迟项
        
    LAG(close, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag1,
        
    LAG(close, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag2,
        
    LAG(close, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag5,
        
    LAG(close, 10) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag10,
        
    LAG(close, 20) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS close_lag20,
        
    LAG(volume, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS volume_lag1,
        
    LAG(high, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS high_lag1,
        
    LAG(low, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS low_lag1,
        
    LAG(vwap, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS vwap_lag5,
        
        -- 预计算一些差值项
        
    close - 
    LAG(close, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta1,
        
    close - 
    LAG(close, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta2,
        
    close - 
    LAG(close, 3) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta3,
        
    close - 
    LAG(close, 5) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta5,
        
    close - 
    LAG(close, 7) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta7,
        
    close - 
    LAG(close, 10) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS close_delta10,
        
    volume - 
    LAG(volume, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS volume_delta1,
        
    volume - 
    LAG(volume, 3) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS volume_delta3,
        
    high - 
    LAG(high, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS high_delta2,
        
        -- 预计算一些排序项
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY close
    )
 AS close_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY volume
    )
 AS volume_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY high
    )
 AS high_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY low
    )
 AS low_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY vwap
    )
 AS vwap_rank,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY returns
    )
 AS returns_rank,
        
        -- 预计算时间序列排序
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY close
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS close_ts_rank10,
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY volume
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_ts_rank5,
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY high
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS high_ts_rank5,
        
        -- 预计算一些最值项
        
    MIN(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 99 PRECEDING AND CURRENT ROW
    )
 AS close_min100,
        
    MAX(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS close_max3,
        
    MIN(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_min5,
        
    MAX(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_max5,
        
    MIN(low) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS low_min5,
        
    MAX(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS high_max3,
        
        -- 预计算一些求和项
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS close_sum5,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 7 PRECEDING AND CURRENT ROW
    )
 AS close_sum8,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS close_sum20,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 99 PRECEDING AND CURRENT ROW
    )
 AS close_sum100,
        
    SUM(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 199 PRECEDING AND CURRENT ROW
    )
 AS close_sum200,
        
    SUM(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS volume_sum5,
        
    SUM(volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS volume_sum20,
        
    SUM(returns) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 249 PRECEDING AND CURRENT ROW
    )
 AS returns_sum250,
        
    SUM(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS high_sum5,
        
    SUM(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS high_sum20,
        
        -- 预计算一些相关性
        
    -- 使用DuckDB的CORR窗口函数
    CORR(close, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS corr_close_volume_10,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(open, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS corr_open_volume_10,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(high, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS corr_high_volume_5,
        
    -- 使用DuckDB的CORR窗口函数
    CORR(vwap, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 5 PRECEDING AND CURRENT ROW
    )
 AS corr_vwap_volume_6,
        
        -- 预计算一些协方差
        
    COVAR_SAMP(close, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS cov_close_volume_5,
        
    COVAR_SAMP(high, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS cov_high_volume_5
        
    FROM base_ohlc
    WHERE timestamp >= CAST(CAST('2020-01-01' AS DATE) AS DATE) - INTERVAL '250 days'  -- 扩展时间范围以确保有足够的历史数据
),

-- 过滤回原始时间范围
final_data AS (
    SELECT *
    FROM enhanced_data
    WHERE timestamp >= CAST('2020-01-01' AS DATE)
      AND timestamp <= CAST('2024-12-31' AS DATE)
)

SELECT * FROM final_data
  
  limit 3

[0m15:31:41.180802 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:31:41.203177 [debug] [Thread-1 (]: SQL status: OK in 0.022 seconds
[0m15:31:41.207043 [debug] [Thread-1 (]: On model.quant_features.alpha_base_data: Close
[0m15:31:41.208395 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_base_data
[0m15:31:41.209365 [debug] [Thread-3 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d
[0m15:31:41.209799 [debug] [Thread-2 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78
[0m15:31:41.210330 [debug] [Thread-4 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352
[0m15:31:41.210785 [debug] [Thread-3 (]: Acquiring new duckdb connection 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d'
[0m15:31:41.212472 [debug] [Thread-3 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d
[0m15:31:41.211140 [debug] [Thread-1 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0
[0m15:31:41.212172 [debug] [Thread-4 (]: Acquiring new duckdb connection 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352'
[0m15:31:41.211694 [debug] [Thread-2 (]: Acquiring new duckdb connection 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78'
[0m15:31:41.217243 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.quant_features.alpha_base_data, now test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0)
[0m15:31:41.217732 [debug] [Thread-4 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352
[0m15:31:41.218982 [debug] [Thread-2 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78
[0m15:31:41.219426 [debug] [Thread-1 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0
[0m15:31:41.226811 [debug] [Thread-2 (]: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_close__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:31:41.228043 [debug] [Thread-4 (]: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_volume__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:31:41.229391 [debug] [Thread-3 (]: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_returns__10___1 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:31:41.232679 [debug] [Thread-2 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78
[0m15:31:41.233796 [debug] [Thread-1 (]: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_vwap__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:31:41.234359 [debug] [Thread-4 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352
[0m15:31:41.235009 [debug] [Thread-3 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d
[0m15:31:41.235384 [debug] [Thread-2 (]: Began running node test.quant_features.not_null_alpha_base_data_close.6db60d82aa
[0m15:31:41.235809 [debug] [Thread-1 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0
[0m15:31:41.236114 [debug] [Thread-4 (]: Began running node test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd
[0m15:31:41.236457 [debug] [Thread-3 (]: Began running node test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974
[0m15:31:41.236946 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78, now test.quant_features.not_null_alpha_base_data_close.6db60d82aa)
[0m15:31:41.237637 [debug] [Thread-7 (]: Marking all children of 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78' to be skipped because of status 'error'.  Reason: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_close__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m15:31:41.238220 [debug] [Thread-1 (]: Began running node test.quant_features.not_null_alpha_base_data_volume.21044032e6
[0m15:31:41.238634 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352, now test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd)
[0m15:31:41.239028 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d, now test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974)
[0m15:31:41.239348 [debug] [Thread-2 (]: Began compiling node test.quant_features.not_null_alpha_base_data_close.6db60d82aa
[0m15:31:41.240338 [debug] [Thread-7 (]: Marking all children of 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352' to be skipped because of status 'error'.  Reason: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_volume__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m15:31:41.240843 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0, now test.quant_features.not_null_alpha_base_data_volume.21044032e6)
[0m15:31:41.241227 [debug] [Thread-4 (]: Began compiling node test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd
[0m15:31:41.241733 [debug] [Thread-3 (]: Began compiling node test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974
[0m15:31:41.247155 [debug] [Thread-2 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_close.6db60d82aa"
[0m15:31:41.247787 [debug] [Thread-7 (]: Marking all children of 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d' to be skipped because of status 'error'.  Reason: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_returns__10___1 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m15:31:41.248096 [debug] [Thread-1 (]: Began compiling node test.quant_features.not_null_alpha_base_data_volume.21044032e6
[0m15:31:41.250897 [debug] [Thread-4 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd"
[0m15:31:41.253657 [debug] [Thread-3 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974"
[0m15:31:41.254276 [debug] [Thread-7 (]: Marking all children of 'test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0' to be skipped because of status 'error'.  Reason: Compilation Error in test dbt_utils_accepted_range_alpha_base_data_vwap__0 (models/alpha101/schema.yml)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m15:31:41.254632 [debug] [Thread-2 (]: Began executing node test.quant_features.not_null_alpha_base_data_close.6db60d82aa
[0m15:31:41.257433 [debug] [Thread-1 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_volume.21044032e6"
[0m15:31:41.258074 [debug] [Thread-4 (]: Began executing node test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd
[0m15:31:41.258614 [debug] [Thread-3 (]: Began executing node test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974
[0m15:31:41.260756 [debug] [Thread-2 (]: Using duckdb connection "test.quant_features.not_null_alpha_base_data_close.6db60d82aa"
[0m15:31:41.263109 [debug] [Thread-4 (]: Using duckdb connection "test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd"
[0m15:31:41.263530 [debug] [Thread-1 (]: Began executing node test.quant_features.not_null_alpha_base_data_volume.21044032e6
[0m15:31:41.265640 [debug] [Thread-3 (]: Using duckdb connection "test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974"
[0m15:31:41.266124 [debug] [Thread-2 (]: On test.quant_features.not_null_alpha_base_data_close.6db60d82aa: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "test.quant_features.not_null_alpha_base_data_close.6db60d82aa"} */

  
  
    
    



select close
from "quant_features"."main"."alpha_base_data"
where close is null



  
  limit 3

[0m15:31:41.266688 [debug] [Thread-4 (]: On test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd"} */

  
  
    
    



select symbol
from "quant_features"."main"."alpha_base_data"
where symbol is null



  
  limit 3

[0m15:31:41.268819 [debug] [Thread-1 (]: Using duckdb connection "test.quant_features.not_null_alpha_base_data_volume.21044032e6"
[0m15:31:41.269344 [debug] [Thread-3 (]: On test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974"} */

  
  
    
    



select timestamp
from "quant_features"."main"."alpha_base_data"
where timestamp is null



  
  limit 3

[0m15:31:41.269838 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:31:41.270388 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m15:31:41.270784 [debug] [Thread-1 (]: On test.quant_features.not_null_alpha_base_data_volume.21044032e6: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "test.quant_features.not_null_alpha_base_data_volume.21044032e6"} */

  
  
    
    



select volume
from "quant_features"."main"."alpha_base_data"
where volume is null



  
  limit 3

[0m15:31:41.271162 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:31:41.271927 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:31:41.272419 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m15:31:41.273019 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:31:41.273840 [debug] [Thread-2 (]: On test.quant_features.not_null_alpha_base_data_close.6db60d82aa: Close
[0m15:31:41.274183 [debug] [Thread-4 (]: SQL status: OK in 0.004 seconds
[0m15:31:41.274491 [debug] [Thread-3 (]: SQL status: OK in 0.003 seconds
[0m15:31:41.275342 [debug] [Thread-1 (]: On test.quant_features.not_null_alpha_base_data_volume.21044032e6: Close
[0m15:31:41.276339 [debug] [Thread-4 (]: On test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd: Close
[0m15:31:41.277182 [debug] [Thread-3 (]: On test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974: Close
[0m15:31:41.277973 [debug] [Thread-2 (]: Finished running node test.quant_features.not_null_alpha_base_data_close.6db60d82aa
[0m15:31:41.278747 [debug] [Thread-4 (]: Finished running node test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd
[0m15:31:41.279315 [debug] [Thread-3 (]: Finished running node test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974
[0m15:31:41.279900 [debug] [Thread-1 (]: Finished running node test.quant_features.not_null_alpha_base_data_volume.21044032e6
[0m15:31:41.280227 [debug] [Thread-2 (]: Began running node test.quant_features.not_null_alpha_base_data_vwap.380f9f922d
[0m15:31:41.281429 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.quant_features.not_null_alpha_base_data_close.6db60d82aa, now test.quant_features.not_null_alpha_base_data_vwap.380f9f922d)
[0m15:31:41.281714 [debug] [Thread-2 (]: Began compiling node test.quant_features.not_null_alpha_base_data_vwap.380f9f922d
[0m15:31:41.284093 [debug] [Thread-2 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_vwap.380f9f922d"
[0m15:31:41.284536 [debug] [Thread-2 (]: Began executing node test.quant_features.not_null_alpha_base_data_vwap.380f9f922d
[0m15:31:41.286615 [debug] [Thread-2 (]: Using duckdb connection "test.quant_features.not_null_alpha_base_data_vwap.380f9f922d"
[0m15:31:41.286883 [debug] [Thread-2 (]: On test.quant_features.not_null_alpha_base_data_vwap.380f9f922d: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "test.quant_features.not_null_alpha_base_data_vwap.380f9f922d"} */

  
  
    
    



select vwap
from "quant_features"."main"."alpha_base_data"
where vwap is null



  
  limit 3

[0m15:31:41.287083 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:31:41.287695 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:31:41.288485 [debug] [Thread-2 (]: On test.quant_features.not_null_alpha_base_data_vwap.380f9f922d: Close
[0m15:31:41.288979 [debug] [Thread-2 (]: Finished running node test.quant_features.not_null_alpha_base_data_vwap.380f9f922d
[0m15:31:41.289583 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:31:41.289979 [debug] [MainThread]: Connection 'test.quant_features.not_null_alpha_base_data_volume.21044032e6' was properly closed.
[0m15:31:41.290190 [debug] [MainThread]: Connection 'test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974' was properly closed.
[0m15:31:41.290348 [debug] [MainThread]: Connection 'test.quant_features.not_null_alpha_base_data_vwap.380f9f922d' was properly closed.
[0m15:31:41.290482 [debug] [MainThread]: Connection 'test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd' was properly closed.
[0m15:31:41.290735 [error] [MainThread]: Encountered an error:
Runtime Error
  Compilation Error in test dbt_utils_accepted_range_alpha_base_data_vwap__0 (models/alpha101/schema.yml)
    'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:31:41.291300 [debug] [MainThread]: Resource report: {"command_name": "show", "command_success": false, "command_wall_clock_time": 0.6760595, "process_in_blocks": "0", "process_kernel_time": 0.164336, "process_mem_max_rss": "179924", "process_out_blocks": "2000", "process_user_time": 1.418995}
[0m15:31:41.291591 [debug] [MainThread]: Command `dbt show` failed at 15:31:41.291529 after 0.68 seconds
[0m15:31:41.291810 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d7b4d7110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d76e29a70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d76e29ff0>]}
[0m15:31:41.292033 [debug] [MainThread]: Flushing usage events
[0m15:31:41.342101 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:31:51.518486 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f81db027770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f81dc65da90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f81d9e5fd90>]}


============================== 15:31:51.520953 | 3b4ebca7-5a17-4210-bf3f-4ef58d6c49bb ==============================
[0m15:31:51.520953 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:31:51.521282 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'empty': 'None', 'profiles_dir': '/workspace/dbt_project', 'invocation_command': 'dbt deps', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/workspace/dbt_project/logs', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_cache_events': 'False', 'warn_error': 'None', 'version_check': 'True', 'log_format': 'default', 'fail_fast': 'False', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'no_print': 'None', 'debug': 'False', 'printer_width': '80', 'write_json': 'True', 'use_colors': 'True'}
[0m15:31:51.601081 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3b4ebca7-5a17-4210-bf3f-4ef58d6c49bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f81dabae650>]}
[0m15:31:51.609103 [debug] [MainThread]: Set downloads directory='/tmp/dbt-downloads-62g_k9c0'
[0m15:31:51.609462 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m15:31:51.660318 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m15:31:51.661312 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m15:31:51.695738 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m15:31:51.703183 [info ] [MainThread]: Updating lock file in file path: /workspace/dbt_project/package-lock.yml
[0m15:31:51.704653 [debug] [MainThread]: Set downloads directory='/tmp/dbt-downloads-i2i6nkzo'
[0m15:31:51.706394 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m15:31:51.753742 [info ] [MainThread]: Installed from version 1.1.1
[0m15:31:51.753993 [info ] [MainThread]: Updated version available: 1.3.0
[0m15:31:51.754202 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '3b4ebca7-5a17-4210-bf3f-4ef58d6c49bb', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f81d9f62250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f81d9f61bf0>]}
[0m15:31:51.754412 [info ] [MainThread]: 
[0m15:31:51.754578 [info ] [MainThread]: Updates available for packages: ['dbt-labs/dbt_utils']                 
Update your versions in packages.yml, then run dbt deps
[0m15:31:51.755267 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 0.2728091, "process_in_blocks": "0", "process_kernel_time": 0.111867, "process_mem_max_rss": "104552", "process_out_blocks": "2224", "process_user_time": 0.950863}
[0m15:31:51.755575 [debug] [MainThread]: Command `dbt deps` succeeded at 15:31:51.755514 after 0.27 seconds
[0m15:31:51.755778 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f81d9c4d250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f81da68f980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f81d9ddc230>]}
[0m15:31:51.755991 [debug] [MainThread]: Flushing usage events
[0m15:31:51.796724 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:32:09.993871 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3aa0953770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3aa1f85a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a9f783d90>]}


============================== 15:32:09.996378 | dbdad144-d48d-41ca-a851-3e11f6a68c1e ==============================
[0m15:32:09.996378 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:32:09.996751 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'warn_error': 'None', 'no_print': 'None', 'profiles_dir': '/workspace/dbt_project', 'introspect': 'True', 'version_check': 'True', 'fail_fast': 'False', 'indirect_selection': 'eager', 'use_colors': 'True', 'log_path': '/workspace/dbt_project/logs', 'debug': 'False', 'log_format': 'default', 'quiet': 'False', 'static_parser': 'True', 'invocation_command': 'dbt run --select feast_features', 'log_cache_events': 'False', 'cache_selected_only': 'False', 'target_path': 'None', 'empty': 'False', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'write_json': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80'}
[0m15:32:10.143920 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'dbdad144-d48d-41ca-a851-3e11f6a68c1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3aa063f360>]}
[0m15:32:10.186853 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'dbdad144-d48d-41ca-a851-3e11f6a68c1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a9f882be0>]}
[0m15:32:10.187899 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:32:10.219853 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:32:10.300075 [info ] [MainThread]: Unable to do partial parsing because a project dependency has been added
[0m15:32:10.300428 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'dbdad144-d48d-41ca-a851-3e11f6a68c1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a9f84f850>]}
[0m15:32:11.396575 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `accepted_values`. Arguments to generic tests
should be nested under the `arguments` property.`
[0m15:32:11.396890 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'dbdad144-d48d-41ca-a851-3e11f6a68c1e', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a9b4d34d0>]}
[0m15:32:11.627087 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'dbdad144-d48d-41ca-a851-3e11f6a68c1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a9a80c210>]}
[0m15:32:11.697554 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:32:11.698745 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:32:11.709339 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'dbdad144-d48d-41ca-a851-3e11f6a68c1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a9a6553d0>]}
[0m15:32:11.709720 [info ] [MainThread]: Found 17 models, 2 seeds, 37 data tests, 1 source, 679 macros
[0m15:32:11.709967 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dbdad144-d48d-41ca-a851-3e11f6a68c1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a9a7622d0>]}
[0m15:32:11.711217 [info ] [MainThread]: 
[0m15:32:11.711479 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:32:11.711658 [info ] [MainThread]: 
[0m15:32:11.711949 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:32:11.712743 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features'
[0m15:32:11.728483 [debug] [ThreadPool]: Using duckdb connection "list_quant_features"
[0m15:32:11.728778 [debug] [ThreadPool]: On list_quant_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"quant_features"'
    
  
  
[0m15:32:11.728980 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:32:11.742378 [debug] [ThreadPool]: SQL status: OK in 0.013 seconds
[0m15:32:11.743268 [debug] [ThreadPool]: On list_quant_features: Close
[0m15:32:11.743830 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_quant_features, now create_quant_features_main)
[0m15:32:11.744141 [debug] [ThreadPool]: Creating schema "database: "quant_features"
schema: "main"
"
[0m15:32:11.748414 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:32:11.748671 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='quant_features'
        and type='sqlite'
    
  
[0m15:32:11.748844 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:32:11.749549 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:32:11.750463 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:32:11.750673 [debug] [ThreadPool]: On create_quant_features_main: BEGIN
[0m15:32:11.750981 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:32:11.751148 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:32:11.751313 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
    
        create schema if not exists "quant_features"."main"
    
[0m15:32:11.751629 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:32:11.752163 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:32:11.752382 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:32:11.752550 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:32:11.752827 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:32:11.753000 [debug] [ThreadPool]: On create_quant_features_main: Close
[0m15:32:11.757659 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_quant_features_main, now list_quant_features_main)
[0m15:32:11.761724 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:32:11.762036 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:32:11.762220 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:32:11.762603 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:32:11.762785 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:32:11.762947 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:32:11.769404 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m15:32:11.770348 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:32:11.770957 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:32:11.771163 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:32:11.772804 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dbdad144-d48d-41ca-a851-3e11f6a68c1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a9a6d0050>]}
[0m15:32:11.773105 [debug] [MainThread]: Using duckdb connection "master"
[0m15:32:11.773282 [debug] [MainThread]: On master: BEGIN
[0m15:32:11.773437 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:32:11.773810 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:32:11.773994 [debug] [MainThread]: On master: COMMIT
[0m15:32:11.774143 [debug] [MainThread]: Using duckdb connection "master"
[0m15:32:11.774295 [debug] [MainThread]: On master: COMMIT
[0m15:32:11.774540 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:32:11.774703 [debug] [MainThread]: On master: Close
[0m15:32:11.776505 [debug] [Thread-1 (]: Began running node model.quant_features.feast_features
[0m15:32:11.776883 [info ] [Thread-1 (]: 1 of 1 START sql table model main.feast_features ............................... [RUN]
[0m15:32:11.777168 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.feast_features)
[0m15:32:11.777386 [debug] [Thread-1 (]: Began compiling node model.quant_features.feast_features
[0m15:32:11.782166 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.feast_features"
[0m15:32:11.782606 [debug] [Thread-1 (]: Began executing node model.quant_features.feast_features
[0m15:32:11.803102 [debug] [Thread-1 (]: Writing runtime sql for node "model.quant_features.feast_features"
[0m15:32:11.803589 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.feast_features"
[0m15:32:11.803807 [debug] [Thread-1 (]: On model.quant_features.feast_features: BEGIN
[0m15:32:11.803987 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:32:11.804532 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:32:11.804786 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.feast_features"
[0m15:32:11.805030 [debug] [Thread-1 (]: On model.quant_features.feast_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.feast_features"} */

  
    
    

    create  table
      "quant_features"."main"."feast_features__dbt_tmp"
  
    as (
      

-- Feast 特征表
-- 整合所有特征用于 Feast 消费

WITH base_features AS (
    SELECT 
        symbol,
        timestamp as event_timestamp,
        close,
        volume,
        returns,
        close_ma5,
        close_ma10,
        close_ma20,
        volume_ma20,
        returns_std20,
        adv20,
        adv10,
        adv5,
        close_lag1,
        close_lag5,
        volume_lag1,
        close_delta1,
        close_delta5,
        volume_delta1
    FROM "quant_features"."main"."alpha_base_data"
    WHERE close_ma20 IS NOT NULL
      AND returns_std20 IS NOT NULL
      AND adv20 IS NOT NULL
),

technical_features AS (
    SELECT 
        symbol,
        event_timestamp,
        close,
        volume,
        
        -- 价格特征
        close_ma5 / NULLIF(close_ma20, 0) - 1 as price_momentum_5_20,
        close_ma10 / NULLIF(close_ma20, 0) - 1 as price_momentum_10_20,
        (close - close_lag5) / NULLIF(close_lag5, 0) as price_return_5d,
        close_delta1 / NULLIF(close_lag1, 0) as price_return_1d,
        
        -- 成交量特征
        volume / NULLIF(volume_ma20, 0) - 1 as volume_ratio_20d,
        volume / NULLIF(adv20, 0) - 1 as volume_ratio_adv20,
        (volume - volume_lag1) / NULLIF(volume_lag1, 0) as volume_change_1d,
        
        -- 波动率特征
        returns_std20 as volatility_20d,
        returns / NULLIF(returns_std20, 0) as risk_adjusted_return,
        
        -- 排序特征
        PERCENT_RANK() OVER (PARTITION BY event_timestamp ORDER BY close) as price_rank,
        PERCENT_RANK() OVER (PARTITION BY event_timestamp ORDER BY volume) as volume_rank,
        PERCENT_RANK() OVER (PARTITION BY event_timestamp ORDER BY returns) as return_rank,
        
        -- 原始数据
        returns,
        close_ma5,
        close_ma10,
        close_ma20,
        volume_ma20,
        returns_std20,
        adv20
        
    FROM base_features
)

SELECT * FROM technical_features
    );
  
  
[0m15:32:11.813115 [debug] [Thread-1 (]: SQL status: OK in 0.008 seconds
[0m15:32:11.818763 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.feast_features"
[0m15:32:11.819041 [debug] [Thread-1 (]: On model.quant_features.feast_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.feast_features"} */
alter table "quant_features"."main"."feast_features__dbt_tmp" rename to "feast_features"
[0m15:32:11.819624 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:32:11.827916 [debug] [Thread-1 (]: On model.quant_features.feast_features: COMMIT
[0m15:32:11.828239 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.feast_features"
[0m15:32:11.828461 [debug] [Thread-1 (]: On model.quant_features.feast_features: COMMIT
[0m15:32:11.831047 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:32:11.834845 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.feast_features"
[0m15:32:11.835122 [debug] [Thread-1 (]: On model.quant_features.feast_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.feast_features"} */

      drop table if exists "quant_features"."main"."feast_features__dbt_backup" cascade
    
[0m15:32:11.835777 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:32:11.837516 [debug] [Thread-1 (]: On model.quant_features.feast_features: Close
[0m15:32:11.839041 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'dbdad144-d48d-41ca-a851-3e11f6a68c1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3aa0490b90>]}
[0m15:32:11.839505 [info ] [Thread-1 (]: 1 of 1 OK created sql table model main.feast_features .......................... [[32mOK[0m in 0.06s]
[0m15:32:11.839874 [debug] [Thread-1 (]: Finished running node model.quant_features.feast_features
[0m15:32:11.841876 [debug] [MainThread]: Using duckdb connection "master"
[0m15:32:11.842131 [debug] [MainThread]: On master: BEGIN
[0m15:32:11.842308 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:32:11.842673 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:32:11.842855 [debug] [MainThread]: On master: COMMIT
[0m15:32:11.843012 [debug] [MainThread]: Using duckdb connection "master"
[0m15:32:11.843155 [debug] [MainThread]: On master: COMMIT
[0m15:32:11.843466 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:32:11.843651 [debug] [MainThread]: On master: Close
[0m15:32:11.843919 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:32:11.844077 [debug] [MainThread]: Connection 'model.quant_features.feast_features' was properly closed.
[0m15:32:11.844259 [info ] [MainThread]: 
[0m15:32:11.844450 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.13 seconds (0.13s).
[0m15:32:11.844815 [debug] [MainThread]: Command end result
[0m15:32:11.870059 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:32:11.871266 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:32:11.874820 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:32:11.875056 [info ] [MainThread]: 
[0m15:32:11.875293 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:32:11.875464 [info ] [MainThread]: 
[0m15:32:11.875639 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m15:32:11.875936 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 12 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m15:32:11.876582 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 1.9192353, "process_in_blocks": "0", "process_kernel_time": 0.168249, "process_mem_max_rss": "163936", "process_out_blocks": "5704", "process_user_time": 2.631843}
[0m15:32:11.876892 [debug] [MainThread]: Command `dbt run` succeeded at 15:32:11.876831 after 1.92 seconds
[0m15:32:11.877108 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a9a9bc1d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3aa04960d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a9f7a9e10>]}
[0m15:32:11.877334 [debug] [MainThread]: Flushing usage events
[0m15:32:11.927545 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:32:18.802206 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f08871b7770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0888815a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f088602fd90>]}


============================== 15:32:18.804563 | be6ddc75-4d24-46f5-b53b-2ccb3d947e75 ==============================
[0m15:32:18.804563 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:32:18.804885 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'static_parser': 'True', 'target_path': 'None', 'write_json': 'True', 'introspect': 'True', 'send_anonymous_usage_stats': 'True', 'printer_width': '80', 'empty': 'None', 'fail_fast': 'False', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'profiles_dir': '/workspace/dbt_project', 'log_format': 'default', 'invocation_command': 'dbt show --select feast_features --limit 5', 'cache_selected_only': 'False', 'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'log_path': '/workspace/dbt_project/logs', 'version_check': 'True', 'no_print': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False'}
[0m15:32:18.949877 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'be6ddc75-4d24-46f5-b53b-2ccb3d947e75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0886eb3360>]}
[0m15:32:18.992098 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'be6ddc75-4d24-46f5-b53b-2ccb3d947e75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f088612abe0>]}
[0m15:32:19.000222 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:32:19.033145 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:32:19.123264 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:32:19.123509 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:32:19.160503 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'be6ddc75-4d24-46f5-b53b-2ccb3d947e75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0881fcd150>]}
[0m15:32:19.235339 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:32:19.236535 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:32:19.242095 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'be6ddc75-4d24-46f5-b53b-2ccb3d947e75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0881fc19a0>]}
[0m15:32:19.242447 [info ] [MainThread]: Found 17 models, 2 seeds, 37 data tests, 1 source, 679 macros
[0m15:32:19.242685 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'be6ddc75-4d24-46f5-b53b-2ccb3d947e75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0882044050>]}
[0m15:32:19.243892 [info ] [MainThread]: 
[0m15:32:19.244143 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:32:19.244324 [info ] [MainThread]: 
[0m15:32:19.244633 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:32:19.248474 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features_main'
[0m15:32:19.294930 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:32:19.295181 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:32:19.295365 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:32:19.307278 [debug] [ThreadPool]: SQL status: OK in 0.012 seconds
[0m15:32:19.307542 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:32:19.307729 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:32:19.314748 [debug] [ThreadPool]: SQL status: OK in 0.007 seconds
[0m15:32:19.315776 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:32:19.316405 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:32:19.316626 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:32:19.318325 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'be6ddc75-4d24-46f5-b53b-2ccb3d947e75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0881bc35f0>]}
[0m15:32:19.320452 [debug] [Thread-1 (]: Began running node model.quant_features.feast_features
[0m15:32:19.320797 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.feast_features)
[0m15:32:19.321018 [debug] [Thread-1 (]: Began compiling node model.quant_features.feast_features
[0m15:32:19.326147 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.feast_features"
[0m15:32:19.326590 [debug] [Thread-1 (]: Began executing node model.quant_features.feast_features
[0m15:32:19.331029 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.feast_features"
[0m15:32:19.331416 [debug] [Thread-1 (]: On model.quant_features.feast_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.feast_features"} */

  
  

-- Feast 特征表
-- 整合所有特征用于 Feast 消费

WITH base_features AS (
    SELECT 
        symbol,
        timestamp as event_timestamp,
        close,
        volume,
        returns,
        close_ma5,
        close_ma10,
        close_ma20,
        volume_ma20,
        returns_std20,
        adv20,
        adv10,
        adv5,
        close_lag1,
        close_lag5,
        volume_lag1,
        close_delta1,
        close_delta5,
        volume_delta1
    FROM "quant_features"."main"."alpha_base_data"
    WHERE close_ma20 IS NOT NULL
      AND returns_std20 IS NOT NULL
      AND adv20 IS NOT NULL
),

technical_features AS (
    SELECT 
        symbol,
        event_timestamp,
        close,
        volume,
        
        -- 价格特征
        close_ma5 / NULLIF(close_ma20, 0) - 1 as price_momentum_5_20,
        close_ma10 / NULLIF(close_ma20, 0) - 1 as price_momentum_10_20,
        (close - close_lag5) / NULLIF(close_lag5, 0) as price_return_5d,
        close_delta1 / NULLIF(close_lag1, 0) as price_return_1d,
        
        -- 成交量特征
        volume / NULLIF(volume_ma20, 0) - 1 as volume_ratio_20d,
        volume / NULLIF(adv20, 0) - 1 as volume_ratio_adv20,
        (volume - volume_lag1) / NULLIF(volume_lag1, 0) as volume_change_1d,
        
        -- 波动率特征
        returns_std20 as volatility_20d,
        returns / NULLIF(returns_std20, 0) as risk_adjusted_return,
        
        -- 排序特征
        PERCENT_RANK() OVER (PARTITION BY event_timestamp ORDER BY close) as price_rank,
        PERCENT_RANK() OVER (PARTITION BY event_timestamp ORDER BY volume) as volume_rank,
        PERCENT_RANK() OVER (PARTITION BY event_timestamp ORDER BY returns) as return_rank,
        
        -- 原始数据
        returns,
        close_ma5,
        close_ma10,
        close_ma20,
        volume_ma20,
        returns_std20,
        adv20
        
    FROM base_features
)

SELECT * FROM technical_features
  
  limit 5

[0m15:32:19.331678 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:32:19.336637 [debug] [Thread-1 (]: SQL status: OK in 0.005 seconds
[0m15:32:19.338760 [debug] [Thread-1 (]: On model.quant_features.feast_features: Close
[0m15:32:19.339488 [debug] [Thread-1 (]: Finished running node model.quant_features.feast_features
[0m15:32:19.340510 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:32:19.340752 [debug] [MainThread]: Connection 'model.quant_features.feast_features' was properly closed.
[0m15:32:19.341143 [debug] [MainThread]: Command end result
[0m15:32:19.367627 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:32:19.368819 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:32:19.372959 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:32:19.373764 [info ] [MainThread]: Previewing node 'feast_features':
| symbol | event_timestamp |   close |  volume | price_momentum_5_20 | price_momentum_10_20 | ... |
| ------ | --------------- | ------- | ------- | ------------------- | -------------------- | --- |
| MSFT   |      2024-01-10 |   396.2 |  975000 |              0.008… |                    0 | ... |
| AAPL   |      2024-01-10 |   158.2 | 1450000 |              0.010… |                    0 | ... |
| GOOGL  |      2024-01-10 | 2,935.0 | 1150000 |              0.008… |                    0 | ... |
| TSLA   |      2024-01-10 |   269.9 | 2700000 |              0.014… |                    0 | ... |
| MSFT   |      2024-01-08 |   392.6 |  925000 |              0.003… |                    0 | ... |

[0m15:32:19.374400 [debug] [MainThread]: Resource report: {"command_name": "show", "command_success": true, "command_wall_clock_time": 0.60872763, "process_in_blocks": "0", "process_kernel_time": 0.127668, "process_mem_max_rss": "152776", "process_out_blocks": "3984", "process_user_time": 1.356463}
[0m15:32:19.374703 [debug] [MainThread]: Command `dbt show` succeeded at 15:32:19.374642 after 0.61 seconds
[0m15:32:19.374922 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f08863c7110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0881bc5f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0881bc5ff0>]}
[0m15:32:19.375137 [debug] [MainThread]: Flushing usage events
[0m15:32:19.437792 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:34:50.443229 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f441fdfb770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4421471a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f441ec6bd90>]}


============================== 15:34:50.445724 | d9789e33-0cdb-4607-9a29-3fc6b611b8a4 ==============================
[0m15:34:50.445724 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:34:50.446817 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'profiles_dir': '/workspace/dbt_project', 'empty': 'False', 'fail_fast': 'False', 'printer_width': '80', 'write_json': 'True', 'log_cache_events': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'partial_parse': 'True', 'indirect_selection': 'eager', 'use_colors': 'True', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'introspect': 'True', 'invocation_command': 'dbt compile --select models/alpha101/alpha101_complete.sql models/alpha101/alpha_base_data.sql models/alpha101/alpha_factors_001_020.sql models/alpha101/alpha_factors_021_050.sql models/alpha101/alpha_factors_051_075.sql models/alpha101/alpha_factors_076_101.sql models/alpha101/alpha_factors_advanced.sql models/alpha101/alpha_factors_final.sql models/alpha101/alpha_factors_simple.sql models/alpha101/schema.yml', 'log_path': '/workspace/dbt_project/logs', 'use_experimental_parser': 'False', 'version_check': 'True', 'cache_selected_only': 'False', 'static_parser': 'True', 'target_path': 'None', 'warn_error': 'None'}
[0m15:34:50.593648 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd9789e33-0cdb-4607-9a29-3fc6b611b8a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f441faeb360>]}
[0m15:34:50.635701 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd9789e33-0cdb-4607-9a29-3fc6b611b8a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f441ed5ebe0>]}
[0m15:34:50.636730 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:34:50.668633 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:34:50.760124 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:34:50.760387 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:34:50.797478 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd9789e33-0cdb-4607-9a29-3fc6b611b8a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f441ad24650>]}
[0m15:34:50.872273 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:34:50.873563 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:34:50.885927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd9789e33-0cdb-4607-9a29-3fc6b611b8a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f441a9a1b80>]}
[0m15:34:50.886289 [info ] [MainThread]: Found 17 models, 2 seeds, 37 data tests, 1 source, 679 macros
[0m15:34:50.886504 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd9789e33-0cdb-4607-9a29-3fc6b611b8a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f441ecb7cb0>]}
[0m15:34:50.896121 [info ] [MainThread]: 
[0m15:34:50.896417 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:34:50.896592 [info ] [MainThread]: 
[0m15:34:50.896874 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:34:50.900716 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features_main'
[0m15:34:50.946980 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:34:50.947243 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:34:50.947412 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:34:50.960917 [debug] [ThreadPool]: SQL status: OK in 0.013 seconds
[0m15:34:50.961176 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:34:50.961375 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:34:50.969429 [debug] [ThreadPool]: SQL status: OK in 0.008 seconds
[0m15:34:50.970441 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:34:50.971069 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:34:50.971296 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:34:50.973322 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd9789e33-0cdb-4607-9a29-3fc6b611b8a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4419d1b860>]}
[0m15:34:50.976029 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_base_data
[0m15:34:50.976415 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.alpha_base_data)
[0m15:34:50.976643 [debug] [Thread-1 (]: Began compiling node model.quant_features.alpha_base_data
[0m15:34:50.999145 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.alpha_base_data"
[0m15:34:50.999615 [debug] [Thread-1 (]: Began executing node model.quant_features.alpha_base_data
[0m15:34:51.000058 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_base_data
[0m15:34:51.000831 [debug] [Thread-3 (]: Began running node model.quant_features.alpha_factors_021_050
[0m15:34:51.001383 [debug] [Thread-4 (]: Began running node model.quant_features.alpha_factors_051_075
[0m15:34:51.002123 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.quant_features.alpha_factors_021_050'
[0m15:34:51.002721 [debug] [Thread-2 (]: Began running node model.quant_features.alpha_factors_001_020
[0m15:34:51.003230 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_factors_076_101
[0m15:34:51.003640 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.quant_features.alpha_factors_051_075'
[0m15:34:51.004153 [debug] [Thread-3 (]: Began compiling node model.quant_features.alpha_factors_021_050
[0m15:34:51.004820 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.quant_features.alpha_factors_001_020'
[0m15:34:51.005357 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.quant_features.alpha_base_data, now model.quant_features.alpha_factors_076_101)
[0m15:34:51.005887 [debug] [Thread-4 (]: Began compiling node model.quant_features.alpha_factors_051_075
[0m15:34:51.019242 [debug] [Thread-2 (]: Began compiling node model.quant_features.alpha_factors_001_020
[0m15:34:51.019790 [debug] [Thread-1 (]: Began compiling node model.quant_features.alpha_factors_076_101
[0m15:34:51.024882 [debug] [Thread-3 (]: Writing injected SQL for node "model.quant_features.alpha_factors_021_050"
[0m15:34:51.045481 [debug] [Thread-2 (]: Writing injected SQL for node "model.quant_features.alpha_factors_001_020"
[0m15:34:51.079539 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.alpha_factors_076_101"
[0m15:34:51.081845 [debug] [Thread-4 (]: Writing injected SQL for node "model.quant_features.alpha_factors_051_075"
[0m15:34:51.082997 [debug] [Thread-2 (]: Began executing node model.quant_features.alpha_factors_001_020
[0m15:34:51.083445 [debug] [Thread-1 (]: Began executing node model.quant_features.alpha_factors_076_101
[0m15:34:51.083840 [debug] [Thread-3 (]: Began executing node model.quant_features.alpha_factors_021_050
[0m15:34:51.084302 [debug] [Thread-4 (]: Began executing node model.quant_features.alpha_factors_051_075
[0m15:34:51.085041 [debug] [Thread-2 (]: Finished running node model.quant_features.alpha_factors_001_020
[0m15:34:51.085723 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_factors_076_101
[0m15:34:51.086408 [debug] [Thread-3 (]: Finished running node model.quant_features.alpha_factors_021_050
[0m15:34:51.087173 [debug] [Thread-4 (]: Finished running node model.quant_features.alpha_factors_051_075
[0m15:34:51.087600 [debug] [Thread-2 (]: Began running node model.quant_features.alpha_factors_advanced
[0m15:34:51.088042 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_factors_simple
[0m15:34:51.088817 [debug] [Thread-3 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78
[0m15:34:51.089330 [debug] [Thread-4 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d
[0m15:34:51.089958 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.quant_features.alpha_factors_001_020, now model.quant_features.alpha_factors_advanced)
[0m15:34:51.090396 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.quant_features.alpha_factors_076_101, now model.quant_features.alpha_factors_simple)
[0m15:34:51.090878 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.quant_features.alpha_factors_021_050, now test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78)
[0m15:34:51.091235 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.quant_features.alpha_factors_051_075, now test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d)
[0m15:34:51.091528 [debug] [Thread-2 (]: Began compiling node model.quant_features.alpha_factors_advanced
[0m15:34:51.091805 [debug] [Thread-1 (]: Began compiling node model.quant_features.alpha_factors_simple
[0m15:34:51.092075 [debug] [Thread-3 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78
[0m15:34:51.092356 [debug] [Thread-4 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d
[0m15:34:51.104939 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.alpha_factors_simple"
[0m15:34:51.112949 [debug] [Thread-2 (]: Writing injected SQL for node "model.quant_features.alpha_factors_advanced"
[0m15:34:51.123638 [debug] [Thread-4 (]: Writing injected SQL for node "test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d"
[0m15:34:51.124277 [debug] [Thread-3 (]: Writing injected SQL for node "test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78"
[0m15:34:51.125023 [debug] [Thread-1 (]: Began executing node model.quant_features.alpha_factors_simple
[0m15:34:51.125470 [debug] [Thread-2 (]: Began executing node model.quant_features.alpha_factors_advanced
[0m15:34:51.126049 [debug] [Thread-4 (]: Began executing node test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d
[0m15:34:51.126403 [debug] [Thread-3 (]: Began executing node test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78
[0m15:34:51.127041 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_factors_simple
[0m15:34:51.127578 [debug] [Thread-2 (]: Finished running node model.quant_features.alpha_factors_advanced
[0m15:34:51.128133 [debug] [Thread-4 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d
[0m15:34:51.128600 [debug] [Thread-3 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78
[0m15:34:51.129043 [debug] [Thread-1 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352
[0m15:34:51.129732 [debug] [Thread-2 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0
[0m15:34:51.130200 [debug] [Thread-4 (]: Began running node test.quant_features.not_null_alpha_base_data_close.6db60d82aa
[0m15:34:51.130846 [debug] [Thread-3 (]: Began running node test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd
[0m15:34:51.131237 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.quant_features.alpha_factors_simple, now test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352)
[0m15:34:51.131620 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.quant_features.alpha_factors_advanced, now test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0)
[0m15:34:51.131977 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_base_data_returns__10___1.274ec2f74d, now test.quant_features.not_null_alpha_base_data_close.6db60d82aa)
[0m15:34:51.132292 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_base_data_close__0.e024872b78, now test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd)
[0m15:34:51.132843 [debug] [Thread-1 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352
[0m15:34:51.133218 [debug] [Thread-2 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0
[0m15:34:51.133517 [debug] [Thread-4 (]: Began compiling node test.quant_features.not_null_alpha_base_data_close.6db60d82aa
[0m15:34:51.133781 [debug] [Thread-3 (]: Began compiling node test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd
[0m15:34:51.136559 [debug] [Thread-1 (]: Writing injected SQL for node "test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352"
[0m15:34:51.139242 [debug] [Thread-2 (]: Writing injected SQL for node "test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0"
[0m15:34:51.150050 [debug] [Thread-3 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd"
[0m15:34:51.150720 [debug] [Thread-4 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_close.6db60d82aa"
[0m15:34:51.151379 [debug] [Thread-1 (]: Began executing node test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352
[0m15:34:51.151734 [debug] [Thread-2 (]: Began executing node test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0
[0m15:34:51.153117 [debug] [Thread-2 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0
[0m15:34:51.152586 [debug] [Thread-1 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352
[0m15:34:51.153584 [debug] [Thread-1 (]: Began running node test.quant_features.not_null_alpha_base_data_volume.21044032e6
[0m15:34:51.153839 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_base_data_volume__0.7582535352, now test.quant_features.not_null_alpha_base_data_volume.21044032e6)
[0m15:34:51.153351 [debug] [Thread-2 (]: Began running node test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974
[0m15:34:51.154031 [debug] [Thread-1 (]: Began compiling node test.quant_features.not_null_alpha_base_data_volume.21044032e6
[0m15:34:51.154363 [debug] [Thread-4 (]: Began executing node test.quant_features.not_null_alpha_base_data_close.6db60d82aa
[0m15:34:51.154745 [debug] [Thread-3 (]: Began executing node test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd
[0m15:34:51.155244 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_base_data_vwap__0.227d5f8db0, now test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974)
[0m15:34:51.158017 [debug] [Thread-1 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_volume.21044032e6"
[0m15:34:51.158779 [debug] [Thread-4 (]: Finished running node test.quant_features.not_null_alpha_base_data_close.6db60d82aa
[0m15:34:51.159307 [debug] [Thread-3 (]: Finished running node test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd
[0m15:34:51.160772 [debug] [Thread-3 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_factors_001_020_alpha001__5___5.2f30cdf1fb
[0m15:34:51.161085 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.quant_features.not_null_alpha_base_data_symbol.23a0c51dfd, now test.quant_features.dbt_utils_accepted_range_alpha_factors_001_020_alpha001__5___5.2f30cdf1fb)
[0m15:34:51.160418 [debug] [Thread-4 (]: Began running node test.quant_features.not_null_alpha_base_data_vwap.380f9f922d
[0m15:34:51.159590 [debug] [Thread-2 (]: Began compiling node test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974
[0m15:34:51.159997 [debug] [Thread-1 (]: Began executing node test.quant_features.not_null_alpha_base_data_volume.21044032e6
[0m15:34:51.161312 [debug] [Thread-3 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_factors_001_020_alpha001__5___5.2f30cdf1fb
[0m15:34:51.161809 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.quant_features.not_null_alpha_base_data_close.6db60d82aa, now test.quant_features.not_null_alpha_base_data_vwap.380f9f922d)
[0m15:34:51.164408 [debug] [Thread-2 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974"
[0m15:34:51.165123 [debug] [Thread-1 (]: Finished running node test.quant_features.not_null_alpha_base_data_volume.21044032e6
[0m15:34:51.167992 [debug] [Thread-3 (]: Writing injected SQL for node "test.quant_features.dbt_utils_accepted_range_alpha_factors_001_020_alpha001__5___5.2f30cdf1fb"
[0m15:34:51.168478 [debug] [Thread-4 (]: Began compiling node test.quant_features.not_null_alpha_base_data_vwap.380f9f922d
[0m15:34:51.169206 [debug] [Thread-2 (]: Began executing node test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974
[0m15:34:51.169803 [debug] [Thread-1 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_factors_001_020_alpha003__2___2.c318c44fa7
[0m15:34:51.172491 [debug] [Thread-4 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_base_data_vwap.380f9f922d"
[0m15:34:51.172822 [debug] [Thread-3 (]: Began executing node test.quant_features.dbt_utils_accepted_range_alpha_factors_001_020_alpha001__5___5.2f30cdf1fb
[0m15:34:51.173568 [debug] [Thread-2 (]: Finished running node test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974
[0m15:34:51.174120 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.quant_features.not_null_alpha_base_data_volume.21044032e6, now test.quant_features.dbt_utils_accepted_range_alpha_factors_001_020_alpha003__2___2.c318c44fa7)
[0m15:34:51.174808 [debug] [Thread-4 (]: Began executing node test.quant_features.not_null_alpha_base_data_vwap.380f9f922d
[0m15:34:51.175467 [debug] [Thread-3 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_factors_001_020_alpha001__5___5.2f30cdf1fb
[0m15:34:51.175790 [debug] [Thread-2 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_factors_001_020_alpha006__2___2.6e4658ad31
[0m15:34:51.176379 [debug] [Thread-1 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_factors_001_020_alpha003__2___2.c318c44fa7
[0m15:34:51.177146 [debug] [Thread-4 (]: Finished running node test.quant_features.not_null_alpha_base_data_vwap.380f9f922d
[0m15:34:51.177636 [debug] [Thread-3 (]: Began running node test.quant_features.not_null_alpha_factors_001_020_symbol.adb386d222
[0m15:34:51.178095 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.quant_features.not_null_alpha_base_data_timestamp.ee0637f974, now test.quant_features.dbt_utils_accepted_range_alpha_factors_001_020_alpha006__2___2.6e4658ad31)
[0m15:34:51.181054 [debug] [Thread-1 (]: Writing injected SQL for node "test.quant_features.dbt_utils_accepted_range_alpha_factors_001_020_alpha003__2___2.c318c44fa7"
[0m15:34:51.181590 [debug] [Thread-4 (]: Began running node test.quant_features.not_null_alpha_factors_001_020_timestamp.67f68aed90
[0m15:34:51.182119 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_factors_001_020_alpha001__5___5.2f30cdf1fb, now test.quant_features.not_null_alpha_factors_001_020_symbol.adb386d222)
[0m15:34:51.182871 [debug] [Thread-2 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_factors_001_020_alpha006__2___2.6e4658ad31
[0m15:34:51.183339 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.quant_features.not_null_alpha_base_data_vwap.380f9f922d, now test.quant_features.not_null_alpha_factors_001_020_timestamp.67f68aed90)
[0m15:34:51.183678 [debug] [Thread-3 (]: Began compiling node test.quant_features.not_null_alpha_factors_001_020_symbol.adb386d222
[0m15:34:51.186327 [debug] [Thread-2 (]: Writing injected SQL for node "test.quant_features.dbt_utils_accepted_range_alpha_factors_001_020_alpha006__2___2.6e4658ad31"
[0m15:34:51.186701 [debug] [Thread-1 (]: Began executing node test.quant_features.dbt_utils_accepted_range_alpha_factors_001_020_alpha003__2___2.c318c44fa7
[0m15:34:51.187229 [debug] [Thread-4 (]: Began compiling node test.quant_features.not_null_alpha_factors_001_020_timestamp.67f68aed90
[0m15:34:51.190045 [debug] [Thread-3 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_factors_001_020_symbol.adb386d222"
[0m15:34:51.190763 [debug] [Thread-1 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_factors_001_020_alpha003__2___2.c318c44fa7
[0m15:34:51.191082 [debug] [Thread-2 (]: Began executing node test.quant_features.dbt_utils_accepted_range_alpha_factors_001_020_alpha006__2___2.6e4658ad31
[0m15:34:51.193709 [debug] [Thread-4 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_factors_001_020_timestamp.67f68aed90"
[0m15:34:51.194268 [debug] [Thread-1 (]: Began running node model.quant_features.alpha101_complete
[0m15:34:51.194976 [debug] [Thread-2 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_factors_001_020_alpha006__2___2.6e4658ad31
[0m15:34:51.195397 [debug] [Thread-3 (]: Began executing node test.quant_features.not_null_alpha_factors_001_020_symbol.adb386d222
[0m15:34:51.195754 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_factors_001_020_alpha003__2___2.c318c44fa7, now model.quant_features.alpha101_complete)
[0m15:34:51.196100 [debug] [Thread-4 (]: Began executing node test.quant_features.not_null_alpha_factors_001_020_timestamp.67f68aed90
[0m15:34:51.196421 [debug] [Thread-2 (]: Began running node test.quant_features.not_null_alpha_factors_021_050_symbol.18b55ec362
[0m15:34:51.197179 [debug] [Thread-3 (]: Finished running node test.quant_features.not_null_alpha_factors_001_020_symbol.adb386d222
[0m15:34:51.197676 [debug] [Thread-1 (]: Began compiling node model.quant_features.alpha101_complete
[0m15:34:51.198194 [debug] [Thread-4 (]: Finished running node test.quant_features.not_null_alpha_factors_001_020_timestamp.67f68aed90
[0m15:34:51.198501 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_factors_001_020_alpha006__2___2.6e4658ad31, now test.quant_features.not_null_alpha_factors_021_050_symbol.18b55ec362)
[0m15:34:51.198827 [debug] [Thread-3 (]: Began running node test.quant_features.not_null_alpha_factors_021_050_timestamp.8c6319bdb9
[0m15:34:51.202684 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.alpha101_complete"
[0m15:34:51.203226 [debug] [Thread-4 (]: Began running node model.quant_features.alpha_factors_final
[0m15:34:51.203662 [debug] [Thread-2 (]: Began compiling node test.quant_features.not_null_alpha_factors_021_050_symbol.18b55ec362
[0m15:34:51.204037 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.quant_features.not_null_alpha_factors_001_020_symbol.adb386d222, now test.quant_features.not_null_alpha_factors_021_050_timestamp.8c6319bdb9)
[0m15:34:51.204420 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.quant_features.not_null_alpha_factors_001_020_timestamp.67f68aed90, now model.quant_features.alpha_factors_final)
[0m15:34:51.206930 [debug] [Thread-2 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_factors_021_050_symbol.18b55ec362"
[0m15:34:51.207424 [debug] [Thread-3 (]: Began compiling node test.quant_features.not_null_alpha_factors_021_050_timestamp.8c6319bdb9
[0m15:34:51.207844 [debug] [Thread-1 (]: Began executing node model.quant_features.alpha101_complete
[0m15:34:51.208173 [debug] [Thread-4 (]: Began compiling node model.quant_features.alpha_factors_final
[0m15:34:51.208622 [debug] [Thread-2 (]: Began executing node test.quant_features.not_null_alpha_factors_021_050_symbol.18b55ec362
[0m15:34:51.211424 [debug] [Thread-3 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_factors_021_050_timestamp.8c6319bdb9"
[0m15:34:51.212203 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha101_complete
[0m15:34:51.215704 [debug] [Thread-4 (]: Writing injected SQL for node "model.quant_features.alpha_factors_final"
[0m15:34:51.216450 [debug] [Thread-2 (]: Finished running node test.quant_features.not_null_alpha_factors_021_050_symbol.18b55ec362
[0m15:34:51.217091 [debug] [Thread-3 (]: Began executing node test.quant_features.not_null_alpha_factors_021_050_timestamp.8c6319bdb9
[0m15:34:51.217701 [debug] [Thread-1 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_factors_advanced_momentum_reversal_norm__3___3.54e286d921
[0m15:34:51.218103 [debug] [Thread-2 (]: Began running node test.quant_features.not_null_alpha_factors_advanced_symbol.602739b6af
[0m15:34:51.218464 [debug] [Thread-4 (]: Began executing node model.quant_features.alpha_factors_final
[0m15:34:51.219124 [debug] [Thread-3 (]: Finished running node test.quant_features.not_null_alpha_factors_021_050_timestamp.8c6319bdb9
[0m15:34:51.219473 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.quant_features.alpha101_complete, now test.quant_features.dbt_utils_accepted_range_alpha_factors_advanced_momentum_reversal_norm__3___3.54e286d921)
[0m15:34:51.219828 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.quant_features.not_null_alpha_factors_021_050_symbol.18b55ec362, now test.quant_features.not_null_alpha_factors_advanced_symbol.602739b6af)
[0m15:34:51.220340 [debug] [Thread-4 (]: Finished running node model.quant_features.alpha_factors_final
[0m15:34:51.220816 [debug] [Thread-3 (]: Began running node test.quant_features.not_null_alpha_factors_advanced_timestamp.75848dd75a
[0m15:34:51.221237 [debug] [Thread-1 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_factors_advanced_momentum_reversal_norm__3___3.54e286d921
[0m15:34:51.221729 [debug] [Thread-2 (]: Began compiling node test.quant_features.not_null_alpha_factors_advanced_symbol.602739b6af
[0m15:34:51.222285 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.quant_features.not_null_alpha_factors_021_050_timestamp.8c6319bdb9, now test.quant_features.not_null_alpha_factors_advanced_timestamp.75848dd75a)
[0m15:34:51.222869 [debug] [Thread-4 (]: Began running node test.quant_features.accepted_values_alpha_factors_final_market_regime__TRENDING__MEAN_REVERTING__SIDEWAYS.4b9f12aab9
[0m15:34:51.225721 [debug] [Thread-1 (]: Writing injected SQL for node "test.quant_features.dbt_utils_accepted_range_alpha_factors_advanced_momentum_reversal_norm__3___3.54e286d921"
[0m15:34:51.228575 [debug] [Thread-2 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_factors_advanced_symbol.602739b6af"
[0m15:34:51.229068 [debug] [Thread-3 (]: Began compiling node test.quant_features.not_null_alpha_factors_advanced_timestamp.75848dd75a
[0m15:34:51.229665 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.quant_features.alpha_factors_final, now test.quant_features.accepted_values_alpha_factors_final_market_regime__TRENDING__MEAN_REVERTING__SIDEWAYS.4b9f12aab9)
[0m15:34:51.230291 [debug] [Thread-1 (]: Began executing node test.quant_features.dbt_utils_accepted_range_alpha_factors_advanced_momentum_reversal_norm__3___3.54e286d921
[0m15:34:51.233190 [debug] [Thread-3 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_factors_advanced_timestamp.75848dd75a"
[0m15:34:51.233775 [debug] [Thread-2 (]: Began executing node test.quant_features.not_null_alpha_factors_advanced_symbol.602739b6af
[0m15:34:51.234200 [debug] [Thread-4 (]: Began compiling node test.quant_features.accepted_values_alpha_factors_final_market_regime__TRENDING__MEAN_REVERTING__SIDEWAYS.4b9f12aab9
[0m15:34:51.234969 [debug] [Thread-1 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_factors_advanced_momentum_reversal_norm__3___3.54e286d921
[0m15:34:51.235563 [debug] [Thread-2 (]: Finished running node test.quant_features.not_null_alpha_factors_advanced_symbol.602739b6af
[0m15:34:51.241029 [debug] [Thread-1 (]: Began running node test.quant_features.accepted_values_alpha_factors_final_volatility_regime__HIGH_VOL__LOW_VOL__NORMAL_VOL.18aaa3ebde
[0m15:34:51.242071 [debug] [Thread-4 (]: Writing injected SQL for node "test.quant_features.accepted_values_alpha_factors_final_market_regime__TRENDING__MEAN_REVERTING__SIDEWAYS.4b9f12aab9"
[0m15:34:51.242616 [debug] [Thread-2 (]: Began running node test.quant_features.dbt_utils_accepted_range_alpha_factors_final_momentum_composite__2___2.17007d2186
[0m15:34:51.243020 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_factors_advanced_momentum_reversal_norm__3___3.54e286d921, now test.quant_features.accepted_values_alpha_factors_final_volatility_regime__HIGH_VOL__LOW_VOL__NORMAL_VOL.18aaa3ebde)
[0m15:34:51.243389 [debug] [Thread-3 (]: Began executing node test.quant_features.not_null_alpha_factors_advanced_timestamp.75848dd75a
[0m15:34:51.243773 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.quant_features.not_null_alpha_factors_advanced_symbol.602739b6af, now test.quant_features.dbt_utils_accepted_range_alpha_factors_final_momentum_composite__2___2.17007d2186)
[0m15:34:51.244075 [debug] [Thread-1 (]: Began compiling node test.quant_features.accepted_values_alpha_factors_final_volatility_regime__HIGH_VOL__LOW_VOL__NORMAL_VOL.18aaa3ebde
[0m15:34:51.244614 [debug] [Thread-3 (]: Finished running node test.quant_features.not_null_alpha_factors_advanced_timestamp.75848dd75a
[0m15:34:51.245053 [debug] [Thread-4 (]: Began executing node test.quant_features.accepted_values_alpha_factors_final_market_regime__TRENDING__MEAN_REVERTING__SIDEWAYS.4b9f12aab9
[0m15:34:51.245613 [debug] [Thread-2 (]: Began compiling node test.quant_features.dbt_utils_accepted_range_alpha_factors_final_momentum_composite__2___2.17007d2186
[0m15:34:51.249617 [debug] [Thread-1 (]: Writing injected SQL for node "test.quant_features.accepted_values_alpha_factors_final_volatility_regime__HIGH_VOL__LOW_VOL__NORMAL_VOL.18aaa3ebde"
[0m15:34:51.250054 [debug] [Thread-3 (]: Began running node test.quant_features.not_null_alpha_factors_final_entity_id.489bd8c6db
[0m15:34:51.250854 [debug] [Thread-4 (]: Finished running node test.quant_features.accepted_values_alpha_factors_final_market_regime__TRENDING__MEAN_REVERTING__SIDEWAYS.4b9f12aab9
[0m15:34:51.253695 [debug] [Thread-2 (]: Writing injected SQL for node "test.quant_features.dbt_utils_accepted_range_alpha_factors_final_momentum_composite__2___2.17007d2186"
[0m15:34:51.254393 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.quant_features.not_null_alpha_factors_advanced_timestamp.75848dd75a, now test.quant_features.not_null_alpha_factors_final_entity_id.489bd8c6db)
[0m15:34:51.254734 [debug] [Thread-1 (]: Began executing node test.quant_features.accepted_values_alpha_factors_final_volatility_regime__HIGH_VOL__LOW_VOL__NORMAL_VOL.18aaa3ebde
[0m15:34:51.255333 [debug] [Thread-4 (]: Began running node test.quant_features.not_null_alpha_factors_final_event_timestamp.f53aa99c72
[0m15:34:51.255782 [debug] [Thread-3 (]: Began compiling node test.quant_features.not_null_alpha_factors_final_entity_id.489bd8c6db
[0m15:34:51.256325 [debug] [Thread-1 (]: Finished running node test.quant_features.accepted_values_alpha_factors_final_volatility_regime__HIGH_VOL__LOW_VOL__NORMAL_VOL.18aaa3ebde
[0m15:34:51.256870 [debug] [Thread-2 (]: Began executing node test.quant_features.dbt_utils_accepted_range_alpha_factors_final_momentum_composite__2___2.17007d2186
[0m15:34:51.257289 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.quant_features.accepted_values_alpha_factors_final_market_regime__TRENDING__MEAN_REVERTING__SIDEWAYS.4b9f12aab9, now test.quant_features.not_null_alpha_factors_final_event_timestamp.f53aa99c72)
[0m15:34:51.260196 [debug] [Thread-3 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_factors_final_entity_id.489bd8c6db"
[0m15:34:51.260883 [debug] [Thread-1 (]: Began running node test.quant_features.not_null_alpha_factors_final_symbol.e93e1ecd5c
[0m15:34:51.261618 [debug] [Thread-2 (]: Finished running node test.quant_features.dbt_utils_accepted_range_alpha_factors_final_momentum_composite__2___2.17007d2186
[0m15:34:51.262087 [debug] [Thread-4 (]: Began compiling node test.quant_features.not_null_alpha_factors_final_event_timestamp.f53aa99c72
[0m15:34:51.262545 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.quant_features.accepted_values_alpha_factors_final_volatility_regime__HIGH_VOL__LOW_VOL__NORMAL_VOL.18aaa3ebde, now test.quant_features.not_null_alpha_factors_final_symbol.e93e1ecd5c)
[0m15:34:51.262938 [debug] [Thread-2 (]: Began running node test.quant_features.not_null_alpha_factors_final_timestamp.332440bd6e
[0m15:34:51.263282 [debug] [Thread-3 (]: Began executing node test.quant_features.not_null_alpha_factors_final_entity_id.489bd8c6db
[0m15:34:51.265820 [debug] [Thread-4 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_factors_final_event_timestamp.f53aa99c72"
[0m15:34:51.266319 [debug] [Thread-1 (]: Began compiling node test.quant_features.not_null_alpha_factors_final_symbol.e93e1ecd5c
[0m15:34:51.266769 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.quant_features.dbt_utils_accepted_range_alpha_factors_final_momentum_composite__2___2.17007d2186, now test.quant_features.not_null_alpha_factors_final_timestamp.332440bd6e)
[0m15:34:51.267339 [debug] [Thread-3 (]: Finished running node test.quant_features.not_null_alpha_factors_final_entity_id.489bd8c6db
[0m15:34:51.270155 [debug] [Thread-1 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_factors_final_symbol.e93e1ecd5c"
[0m15:34:51.270660 [debug] [Thread-2 (]: Began compiling node test.quant_features.not_null_alpha_factors_final_timestamp.332440bd6e
[0m15:34:51.271067 [debug] [Thread-3 (]: Began running node test.quant_features.unique_alpha_factors_final_entity_id.81fbcb3964
[0m15:34:51.271402 [debug] [Thread-4 (]: Began executing node test.quant_features.not_null_alpha_factors_final_event_timestamp.f53aa99c72
[0m15:34:51.273993 [debug] [Thread-2 (]: Writing injected SQL for node "test.quant_features.not_null_alpha_factors_final_timestamp.332440bd6e"
[0m15:34:51.274426 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.quant_features.not_null_alpha_factors_final_entity_id.489bd8c6db, now test.quant_features.unique_alpha_factors_final_entity_id.81fbcb3964)
[0m15:34:51.275426 [debug] [Thread-3 (]: Began compiling node test.quant_features.unique_alpha_factors_final_entity_id.81fbcb3964
[0m15:34:51.275026 [debug] [Thread-4 (]: Finished running node test.quant_features.not_null_alpha_factors_final_event_timestamp.f53aa99c72
[0m15:34:51.275736 [debug] [Thread-1 (]: Began executing node test.quant_features.not_null_alpha_factors_final_symbol.e93e1ecd5c
[0m15:34:51.281503 [debug] [Thread-2 (]: Began executing node test.quant_features.not_null_alpha_factors_final_timestamp.332440bd6e
[0m15:34:51.282006 [debug] [Thread-3 (]: Writing injected SQL for node "test.quant_features.unique_alpha_factors_final_entity_id.81fbcb3964"
[0m15:34:51.283045 [debug] [Thread-1 (]: Finished running node test.quant_features.not_null_alpha_factors_final_symbol.e93e1ecd5c
[0m15:34:51.283836 [debug] [Thread-2 (]: Finished running node test.quant_features.not_null_alpha_factors_final_timestamp.332440bd6e
[0m15:34:51.284519 [debug] [Thread-3 (]: Began executing node test.quant_features.unique_alpha_factors_final_entity_id.81fbcb3964
[0m15:34:51.285870 [debug] [Thread-3 (]: Finished running node test.quant_features.unique_alpha_factors_final_entity_id.81fbcb3964
[0m15:34:51.287110 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:34:51.287366 [debug] [MainThread]: Connection 'test.quant_features.not_null_alpha_factors_final_symbol.e93e1ecd5c' was properly closed.
[0m15:34:51.287526 [debug] [MainThread]: Connection 'test.quant_features.unique_alpha_factors_final_entity_id.81fbcb3964' was properly closed.
[0m15:34:51.287670 [debug] [MainThread]: Connection 'test.quant_features.not_null_alpha_factors_final_event_timestamp.f53aa99c72' was properly closed.
[0m15:34:51.287807 [debug] [MainThread]: Connection 'test.quant_features.not_null_alpha_factors_final_timestamp.332440bd6e' was properly closed.
[0m15:34:51.290272 [debug] [MainThread]: Command end result
[0m15:34:51.314169 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:34:51.315314 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:34:51.321054 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:34:51.321316 [debug] [MainThread]: Excluded node 'alpha_base_data' from results
[0m15:34:51.321476 [debug] [MainThread]: Excluded node 'alpha_factors_001_020' from results
[0m15:34:51.321615 [debug] [MainThread]: Excluded node 'alpha_factors_076_101' from results
[0m15:34:51.321759 [debug] [MainThread]: Excluded node 'alpha_factors_021_050' from results
[0m15:34:51.321889 [debug] [MainThread]: Excluded node 'alpha_factors_051_075' from results
[0m15:34:51.322012 [debug] [MainThread]: Excluded node 'alpha_factors_simple' from results
[0m15:34:51.322136 [debug] [MainThread]: Excluded node 'alpha_factors_advanced' from results
[0m15:34:51.322263 [debug] [MainThread]: Excluded node 'dbt_utils_accepted_range_alpha_base_data_returns__10___1' from results
[0m15:34:51.322382 [debug] [MainThread]: Excluded node 'dbt_utils_accepted_range_alpha_base_data_close__0' from results
[0m15:34:51.322499 [debug] [MainThread]: Excluded node 'dbt_utils_accepted_range_alpha_base_data_vwap__0' from results
[0m15:34:51.322615 [debug] [MainThread]: Excluded node 'dbt_utils_accepted_range_alpha_base_data_volume__0' from results
[0m15:34:51.322731 [debug] [MainThread]: Excluded node 'not_null_alpha_base_data_close' from results
[0m15:34:51.322848 [debug] [MainThread]: Excluded node 'not_null_alpha_base_data_symbol' from results
[0m15:34:51.322962 [debug] [MainThread]: Excluded node 'not_null_alpha_base_data_volume' from results
[0m15:34:51.323077 [debug] [MainThread]: Excluded node 'not_null_alpha_base_data_timestamp' from results
[0m15:34:51.323198 [debug] [MainThread]: Excluded node 'dbt_utils_accepted_range_alpha_factors_001_020_alpha001__5___5' from results
[0m15:34:51.323317 [debug] [MainThread]: Excluded node 'not_null_alpha_base_data_vwap' from results
[0m15:34:51.323433 [debug] [MainThread]: Excluded node 'dbt_utils_accepted_range_alpha_factors_001_020_alpha003__2___2' from results
[0m15:34:51.323550 [debug] [MainThread]: Excluded node 'dbt_utils_accepted_range_alpha_factors_001_020_alpha006__2___2' from results
[0m15:34:51.323665 [debug] [MainThread]: Excluded node 'not_null_alpha_factors_001_020_symbol' from results
[0m15:34:51.323780 [debug] [MainThread]: Excluded node 'not_null_alpha_factors_001_020_timestamp' from results
[0m15:34:51.323895 [debug] [MainThread]: Excluded node 'not_null_alpha_factors_021_050_symbol' from results
[0m15:34:51.324011 [debug] [MainThread]: Excluded node 'not_null_alpha_factors_021_050_timestamp' from results
[0m15:34:51.324127 [debug] [MainThread]: Excluded node 'alpha_factors_final' from results
[0m15:34:51.324250 [debug] [MainThread]: Excluded node 'dbt_utils_accepted_range_alpha_factors_advanced_momentum_reversal_norm__3___3' from results
[0m15:34:51.324382 [debug] [MainThread]: Excluded node 'not_null_alpha_factors_advanced_symbol' from results
[0m15:34:51.324501 [debug] [MainThread]: Excluded node 'not_null_alpha_factors_advanced_timestamp' from results
[0m15:34:51.324620 [debug] [MainThread]: Excluded node 'accepted_values_alpha_factors_final_market_regime__TRENDING__MEAN_REVERTING__SIDEWAYS' from results
[0m15:34:51.324742 [debug] [MainThread]: Excluded node 'accepted_values_alpha_factors_final_volatility_regime__HIGH_VOL__LOW_VOL__NORMAL_VOL' from results
[0m15:34:51.324857 [debug] [MainThread]: Excluded node 'dbt_utils_accepted_range_alpha_factors_final_momentum_composite__2___2' from results
[0m15:34:51.324973 [debug] [MainThread]: Excluded node 'not_null_alpha_factors_final_entity_id' from results
[0m15:34:51.325096 [debug] [MainThread]: Excluded node 'not_null_alpha_factors_final_event_timestamp' from results
[0m15:34:51.325223 [debug] [MainThread]: Excluded node 'not_null_alpha_factors_final_symbol' from results
[0m15:34:51.325341 [debug] [MainThread]: Excluded node 'not_null_alpha_factors_final_timestamp' from results
[0m15:34:51.325459 [debug] [MainThread]: Excluded node 'unique_alpha_factors_final_entity_id' from results
[0m15:34:51.326120 [info ] [MainThread]: Compiled node 'alpha101_complete' is:


-- Alpha 101 完整因子汇总
-- 整合所有101个Alpha因子并进行最终处理

WITH factors_001_020 AS (
    SELECT * FROM "quant_features"."main"."alpha_factors_001_020"
),

factors_021_050 AS (
    SELECT * FROM "quant_features"."main"."alpha_factors_021_050"
),

factors_051_075 AS (
    SELECT * FROM "quant_features"."main"."alpha_factors_051_075"
),

factors_076_101 AS (
    SELECT * FROM "quant_features"."main"."alpha_factors_076_101"
),

-- 合并所有Alpha因子
all_alpha_factors AS (
    SELECT 
        a.symbol,
        a.timestamp,
        
        -- 基础市场数据
        a.open, a.high, a.low, a.close, a.volume, a.vwap, a.returns,
        
        -- Alpha 001-020
        a.alpha001, a.alpha002, a.alpha003, a.alpha004, a.alpha005,
        a.alpha006, a.alpha007, a.alpha008, a.alpha009, a.alpha010,
        a.alpha011, a.alpha012, a.alpha013, a.alpha014, a.alpha015,
        a.alpha016, a.alpha017, a.alpha018, a.alpha019, a.alpha020,
        
        -- Alpha 021-050
        b.alpha021, b.alpha022, b.alpha023, b.alpha024, b.alpha025,
        b.alpha026, b.alpha027, b.alpha028, b.alpha029, b.alpha030,
        b.alpha031, b.alpha032, b.alpha033, b.alpha034, b.alpha035,
        b.alpha036, b.alpha037, b.alpha038, b.alpha039, b.alpha040,
        b.alpha041, b.alpha042, b.alpha043, b.alpha044, b.alpha045,
        b.alpha046, b.alpha047, b.alpha048, b.alpha049, b.alpha050,
        
        -- Alpha 051-075
        c.alpha051, c.alpha052, c.alpha053, c.alpha054, c.alpha055,
        c.alpha056, c.alpha057, c.alpha058, c.alpha059, c.alpha060,
        c.alpha061, c.alpha062, c.alpha063, c.alpha064, c.alpha065,
        c.alpha066, c.alpha067, c.alpha068, c.alpha069, c.alpha070,
        c.alpha071, c.alpha072, c.alpha073, c.alpha074, c.alpha075,
        
        -- Alpha 076-101
        d.alpha076, d.alpha077, d.alpha078, d.alpha079, d.alpha080,
        d.alpha081, d.alpha082, d.alpha083, d.alpha084, d.alpha085,
        d.alpha086, d.alpha087, d.alpha088, d.alpha089, d.alpha090,
        d.alpha091, d.alpha092, d.alpha093, d.alpha094, d.alpha095,
        d.alpha096, d.alpha097, d.alpha098, d.alpha099, d.alpha100,
        d.alpha101,
        
        -- 有效性统计
        d.valid_factors_count
        
    FROM factors_001_020 a
    LEFT JOIN factors_021_050 b ON a.symbol = b.symbol AND a.timestamp = b.timestamp
    LEFT JOIN factors_051_075 c ON a.symbol = c.symbol AND a.timestamp = c.timestamp
    LEFT JOIN factors_076_101 d ON a.symbol = d.symbol AND a.timestamp = d.timestamp
),

-- 因子分类和组合
factor_categories AS (
    SELECT 
        *,
        
        -- ========================================
        -- 因子分类组合
        -- ========================================
        
        -- 动量类因子组合 (基于因子特性分类)
        (COALESCE(alpha001, 0) + COALESCE(alpha012, 0) + COALESCE(alpha019, 0) + 
         COALESCE(alpha037, 0) + COALESCE(alpha065, 0)) / 5 AS momentum_alpha_composite,
        
        -- 反转类因子组合
        (COALESCE(alpha003, 0) + COALESCE(alpha004, 0) + COALESCE(alpha009, 0) + 
         COALESCE(alpha023, 0) + COALESCE(alpha051, 0)) / 5 AS reversal_alpha_composite,
        
        -- 成交量类因子组合
        (COALESCE(alpha006, 0) + COALESCE(alpha013, 0) + COALESCE(alpha025, 0) + 
         COALESCE(alpha044, 0) + COALESCE(alpha075, 0)) / 5 AS volume_alpha_composite,
        
        -- 波动率类因子组合
        (COALESCE(alpha022, 0) + COALESCE(alpha040, 0) + COALESCE(alpha053, 0) + 
         COALESCE(alpha070, 0) + COALESCE(alpha084, 0)) / 5 AS volatility_alpha_composite,
        
        -- 趋势类因子组合
        (COALESCE(alpha005, 0) + COALESCE(alpha028, 0) + COALESCE(alpha032, 0) + 
         COALESCE(alpha046, 0) + COALESCE(alpha089, 0)) / 5 AS trend_alpha_composite,
        
        -- 价格形态类因子组合
        (COALESCE(alpha041, 0) + COALESCE(alpha054, 0) + COALESCE(alpha060, 0) + 
         COALESCE(alpha083, 0) + COALESCE(alpha101, 0)) / 5 AS pattern_alpha_composite,
        
        -- ========================================
        -- 因子质量指标
        -- ========================================
        
        -- 计算非空因子数量
        (
            CASE WHEN alpha001 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha002 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha003 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha004 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha005 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha006 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha007 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha008 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha009 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha010 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha011 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha012 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha013 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha014 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha015 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha016 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha017 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha018 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha019 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha020 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha021 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha022 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha023 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha024 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha025 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha026 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha027 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha028 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha029 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha030 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha031 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha032 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha033 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha034 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha035 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha036 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha037 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha038 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha039 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha040 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha041 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha042 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha043 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha044 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha045 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha046 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha047 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha048 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha049 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha050 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha051 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha052 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha053 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha054 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha055 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha056 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha057 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha058 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha059 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha060 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha061 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha062 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha063 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha064 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha065 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha066 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha067 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha068 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha069 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha070 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha071 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha072 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha073 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha074 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha075 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha076 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha077 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha078 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha079 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha080 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha081 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha082 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha083 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha084 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha085 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha086 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha087 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha088 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha089 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha090 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha091 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha092 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha093 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha094 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha095 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha096 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha097 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha098 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha099 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha100 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha101 IS NOT NULL THEN 1 ELSE 0 END
        ) AS total_valid_factors,
        
        -- Alpha 021-050
        b.alpha021, b.alpha022, b.alpha023, b.alpha024, b.alpha025,
        b.alpha026, b.alpha027, b.alpha028, b.alpha029, b.alpha030,
        b.alpha031, b.alpha032, b.alpha033, b.alpha034, b.alpha035,
        b.alpha036, b.alpha037, b.alpha038, b.alpha039, b.alpha040,
        b.alpha041, b.alpha042, b.alpha043, b.alpha044, b.alpha045,
        b.alpha046, b.alpha047, b.alpha048, b.alpha049, b.alpha050,
        
        -- Alpha 051-075
        c.alpha051, c.alpha052, c.alpha053, c.alpha054, c.alpha055,
        c.alpha056, c.alpha057, c.alpha058, c.alpha059, c.alpha060,
        c.alpha061, c.alpha062, c.alpha063, c.alpha064, c.alpha065,
        c.alpha066, c.alpha067, c.alpha068, c.alpha069, c.alpha070,
        c.alpha071, c.alpha072, c.alpha073, c.alpha074, c.alpha075,
        
        -- Alpha 076-101
        d.alpha076, d.alpha077, d.alpha078, d.alpha079, d.alpha080,
        d.alpha081, d.alpha082, d.alpha083, d.alpha084, d.alpha085,
        d.alpha086, d.alpha087, d.alpha088, d.alpha089, d.alpha090,
        d.alpha091, d.alpha092, d.alpha093, d.alpha094, d.alpha095,
        d.alpha096, d.alpha097, d.alpha098, d.alpha099, d.alpha100,
        d.alpha101
        
    FROM factors_001_020 a
    LEFT JOIN factors_021_050 b ON a.symbol = b.symbol AND a.timestamp = b.timestamp
    LEFT JOIN factors_051_075 c ON a.symbol = c.symbol AND a.timestamp = c.timestamp
    LEFT JOIN factors_076_101 d ON a.symbol = d.symbol AND a.timestamp = d.timestamp
),

-- 因子组合和元因子
factor_combinations AS (
    SELECT 
        *,
        
        -- ========================================
        -- 主要因子组合 (基于研究文献中的有效因子)
        -- ========================================
        
        -- 动量类因子组合 (经过验证的有效因子)
        (COALESCE(alpha001, 0) + COALESCE(alpha012, 0) + COALESCE(alpha019, 0) + 
         COALESCE(alpha037, 0) + COALESCE(alpha065, 0) + COALESCE(alpha089, 0)) / 6 AS momentum_alpha_composite,
        
        -- 反转类因子组合
        (COALESCE(alpha003, 0) + COALESCE(alpha004, 0) + COALESCE(alpha009, 0) + 
         COALESCE(alpha023, 0) + COALESCE(alpha051, 0) + COALESCE(alpha099, 0)) / 6 AS reversal_alpha_composite,
        
        -- 成交量类因子组合
        (COALESCE(alpha006, 0) + COALESCE(alpha013, 0) + COALESCE(alpha025, 0) + 
         COALESCE(alpha044, 0) + COALESCE(alpha075, 0) + COALESCE(alpha078, 0)) / 6 AS volume_alpha_composite,
        
        -- 波动率类因子组合
        (COALESCE(alpha022, 0) + COALESCE(alpha040, 0) + COALESCE(alpha053, 0) + 
         COALESCE(alpha070, 0) + COALESCE(alpha084, 0) + COALESCE(alpha094, 0)) / 6 AS volatility_alpha_composite,
        
        -- 趋势类因子组合
        (COALESCE(alpha005, 0) + COALESCE(alpha028, 0) + COALESCE(alpha032, 0) + 
         COALESCE(alpha046, 0) + COALESCE(alpha089, 0) + COALESCE(alpha097, 0)) / 6 AS trend_alpha_composite,
        
        -- 价格形态类因子组合
        (COALESCE(alpha041, 0) + COALESCE(alpha054, 0) + COALESCE(alpha060, 0) + 
         COALESCE(alpha083, 0) + COALESCE(alpha101, 0) + COALESCE(alpha088, 0)) / 6 AS pattern_alpha_composite,
        
        -- ========================================
        -- 高级组合因子
        -- ========================================
        
        -- 多空组合 (Long-Short Portfolio)
        -- 多头因子：选择正向预测的因子
        (COALESCE(alpha001, 0) + COALESCE(alpha005, 0) + COALESCE(alpha012, 0) + 
         COALESCE(alpha028, 0) + COALESCE(alpha032, 0) + COALESCE(alpha041, 0) + 
         COALESCE(alpha101, 0)) / 7 AS long_alpha_composite,
        
        -- 空头因子：选择负向预测的因子
        (COALESCE(alpha003, 0) + COALESCE(alpha006, 0) + COALESCE(alpha013, 0) + 
         COALESCE(alpha022, 0) + COALESCE(alpha040, 0) + COALESCE(alpha044, 0) + 
         COALESCE(alpha050, 0)) / 7 AS short_alpha_composite,
        
        -- 市场中性组合
        ((COALESCE(alpha001, 0) + COALESCE(alpha012, 0) + COALESCE(alpha028, 0)) / 3) - 
        ((COALESCE(alpha003, 0) + COALESCE(alpha006, 0) + COALESCE(alpha013, 0)) / 3) AS market_neutral_alpha,
        
        -- ========================================
        -- 因子稳健性指标
        -- ========================================
        
        -- 因子一致性 (同类因子的方向一致性)
        CASE 
            WHEN (COALESCE(alpha001, 0) > 0 AND COALESCE(alpha012, 0) > 0 AND COALESCE(alpha019, 0) > 0) OR
                 (COALESCE(alpha001, 0) < 0 AND COALESCE(alpha012, 0) < 0 AND COALESCE(alpha019, 0) < 0)
            THEN 1 ELSE 0 
        END AS momentum_consistency,
        
        CASE 
            WHEN (COALESCE(alpha003, 0) > 0 AND COALESCE(alpha004, 0) > 0 AND COALESCE(alpha009, 0) > 0) OR
                 (COALESCE(alpha003, 0) < 0 AND COALESCE(alpha004, 0) < 0 AND COALESCE(alpha009, 0) < 0)
            THEN 1 ELSE 0 
        END AS reversal_consistency,
        
        -- 因子强度 (因子绝对值的平均)
        (ABS(COALESCE(alpha001, 0)) + ABS(COALESCE(alpha003, 0)) + ABS(COALESCE(alpha006, 0)) + 
         ABS(COALESCE(alpha012, 0)) + ABS(COALESCE(alpha028, 0))) / 5 AS factor_strength,
        
        -- ========================================
        -- 特殊用途因子
        -- ========================================
        
        -- 高频交易因子 (基于短期因子)
        (COALESCE(alpha012, 0) + COALESCE(alpha041, 0) + COALESCE(alpha101, 0)) / 3 AS hft_alpha_composite,
        
        -- 低频交易因子 (基于长期因子)
        (COALESCE(alpha019, 0) + COALESCE(alpha032, 0) + COALESCE(alpha048, 0)) / 3 AS low_freq_alpha_composite,
        
        -- 风险平价因子 (考虑波动率调整)
        CASE 
            WHEN COALESCE(volatility_alpha_composite, 1) != 0 
            THEN COALESCE(momentum_alpha_composite, 0) / ABS(COALESCE(volatility_alpha_composite, 1))
            ELSE 0 
        END AS risk_parity_alpha
        
    FROM factor_categories
),

-- 最终输出
final_output AS (
    SELECT 
        symbol,
        timestamp,
        
        -- 完整的101个Alpha因子
        alpha001, alpha002, alpha003, alpha004, alpha005, alpha006, alpha007, alpha008, alpha009, alpha010,
        alpha011, alpha012, alpha013, alpha014, alpha015, alpha016, alpha017, alpha018, alpha019, alpha020,
        alpha021, alpha022, alpha023, alpha024, alpha025, alpha026, alpha027, alpha028, alpha029, alpha030,
        alpha031, alpha032, alpha033, alpha034, alpha035, alpha036, alpha037, alpha038, alpha039, alpha040,
        alpha041, alpha042, alpha043, alpha044, alpha045, alpha046, alpha047, alpha048, alpha049, alpha050,
        alpha051, alpha052, alpha053, alpha054, alpha055, alpha056, alpha057, alpha058, alpha059, alpha060,
        alpha061, alpha062, alpha063, alpha064, alpha065, alpha066, alpha067, alpha068, alpha069, alpha070,
        alpha071, alpha072, alpha073, alpha074, alpha075, alpha076, alpha077, alpha078, alpha079, alpha080,
        alpha081, alpha082, alpha083, alpha084, alpha085, alpha086, alpha087, alpha088, alpha089, alpha090,
        alpha091, alpha092, alpha093, alpha094, alpha095, alpha096, alpha097, alpha098, alpha099, alpha100,
        alpha101,
        
        -- 因子组合
        momentum_alpha_composite,
        reversal_alpha_composite,
        volume_alpha_composite,
        volatility_alpha_composite,
        trend_alpha_composite,
        pattern_alpha_composite,
        
        -- 高级组合
        long_alpha_composite,
        short_alpha_composite,
        market_neutral_alpha,
        
        -- 特殊用途因子
        hft_alpha_composite,
        low_freq_alpha_composite,
        risk_parity_alpha,
        
        -- 质量指标
        total_valid_factors,
        momentum_consistency,
        reversal_consistency,
        factor_strength,
        
        -- Feast集成字段
        CONCAT(symbol, '_', DATE_TRUNC('day', timestamp)::STRING) AS entity_id,
        timestamp AS event_timestamp,
        CURRENT_TIMESTAMP AS created_at
        
    FROM factor_combinations
)

SELECT * FROM final_output
[0m15:34:51.327570 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 0.9198197, "process_in_blocks": "0", "process_kernel_time": 0.107895, "process_mem_max_rss": "158332", "process_out_blocks": "5544", "process_user_time": 1.662354}
[0m15:34:51.327860 [debug] [MainThread]: Command `dbt compile` succeeded at 15:34:51.327801 after 0.92 seconds
[0m15:34:51.328059 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f441efff110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f441a763ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f441a763d80>]}
[0m15:34:51.328288 [debug] [MainThread]: Flushing usage events
[0m15:34:51.392770 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:35:40.655836 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2368187770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f23697b5a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2366ffbd90>]}


============================== 15:35:40.658158 | bb81056a-9648-4c86-aff4-2bcb3199f45a ==============================
[0m15:35:40.658158 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:35:40.658907 [debug] [MainThread]: running dbt with arguments {'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'quiet': 'False', 'static_parser': 'True', 'profiles_dir': '/workspace/dbt_project', 'indirect_selection': 'eager', 'warn_error': 'None', 'invocation_command': 'dbt run --select alpha_factors_001_020', 'empty': 'False', 'no_print': 'None', 'target_path': 'None', 'cache_selected_only': 'False', 'debug': 'False', 'version_check': 'True', 'printer_width': '80', 'introspect': 'True', 'log_cache_events': 'False', 'use_experimental_parser': 'False', 'use_colors': 'True', 'write_json': 'True', 'fail_fast': 'False', 'log_format': 'default', 'log_path': '/workspace/dbt_project/logs'}
[0m15:35:40.805237 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bb81056a-9648-4c86-aff4-2bcb3199f45a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2367e77360>]}
[0m15:35:40.848158 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bb81056a-9648-4c86-aff4-2bcb3199f45a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f23670eebe0>]}
[0m15:35:40.849158 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:35:40.880723 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:35:40.970581 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:35:40.971717 [debug] [MainThread]: Partial parsing: updated file: quant_features://macros/alpha101/base_operators.sql
[0m15:35:41.009204 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two macros named "sign" in the project "quant_features".
   To fix this error, rename or remove one of the following macros:
      - macros/alpha101/base_operators.sql
      - macros/alpha101/base_operators.sql
[0m15:35:41.009825 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.3900978, "process_in_blocks": "0", "process_kernel_time": 0.139762, "process_mem_max_rss": "122888", "process_out_blocks": "16", "process_user_time": 1.130063}
[0m15:35:41.010114 [debug] [MainThread]: Command `dbt run` failed at 15:35:41.010052 after 0.39 seconds
[0m15:35:41.010356 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2362b88950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2362b88250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2367700aa0>]}
[0m15:35:41.010592 [debug] [MainThread]: Flushing usage events
[0m15:35:41.066636 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:36:39.979909 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe444e1b770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe446465a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe443c8bd90>]}


============================== 15:36:39.982249 | b08c545b-77fa-4a2f-9884-ae56d4328e71 ==============================
[0m15:36:39.982249 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:36:39.982567 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'use_experimental_parser': 'False', 'version_check': 'True', 'write_json': 'True', 'log_cache_events': 'False', 'printer_width': '80', 'log_path': '/workspace/dbt_project/logs', 'fail_fast': 'False', 'debug': 'False', 'indirect_selection': 'eager', 'target_path': 'None', 'no_print': 'None', 'invocation_command': 'dbt run --select alpha_factors_001_020', 'profiles_dir': '/workspace/dbt_project', 'use_colors': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'cache_selected_only': 'False', 'partial_parse': 'True', 'empty': 'False', 'warn_error': 'None', 'quiet': 'False', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:36:40.126899 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b08c545b-77fa-4a2f-9884-ae56d4328e71', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe444b07360>]}
[0m15:36:40.169076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b08c545b-77fa-4a2f-9884-ae56d4328e71', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe443d7ebe0>]}
[0m15:36:40.170064 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:36:40.202006 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:36:40.292413 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m15:36:40.292850 [debug] [MainThread]: Partial parsing: added file: quant_features://macros/alpha101/base_operators_old.sql
[0m15:36:40.293823 [debug] [MainThread]: Partial parsing: updated file: quant_features://macros/alpha101/base_operators.sql
[0m15:36:40.299358 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two macros named "sign" in the project "quant_features".
   To fix this error, rename or remove one of the following macros:
      - macros/alpha101/base_operators_old.sql
      - macros/alpha101/base_operators_old.sql
[0m15:36:40.299987 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.35576245, "process_in_blocks": "0", "process_kernel_time": 0.140073, "process_mem_max_rss": "122592", "process_out_blocks": "16", "process_user_time": 1.088564}
[0m15:36:40.300290 [debug] [MainThread]: Command `dbt run` failed at 15:36:40.300228 after 0.36 seconds
[0m15:36:40.300512 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe43fd24650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe43fd26750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe444390aa0>]}
[0m15:36:40.300710 [debug] [MainThread]: Flushing usage events
[0m15:36:40.325783 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:36:50.001981 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f760076f770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7601dc1a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f75ff5a3d90>]}


============================== 15:36:50.004240 | 3340141c-24bb-4981-818c-bd41409f017a ==============================
[0m15:36:50.004240 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:36:50.004581 [debug] [MainThread]: running dbt with arguments {'profiles_dir': '/workspace/dbt_project', 'log_path': '/workspace/dbt_project/logs', 'no_print': 'None', 'use_experimental_parser': 'False', 'warn_error': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'printer_width': '80', 'write_json': 'True', 'version_check': 'True', 'partial_parse': 'True', 'static_parser': 'True', 'cache_selected_only': 'False', 'indirect_selection': 'eager', 'empty': 'False', 'quiet': 'False', 'use_colors': 'True', 'invocation_command': 'dbt run --select alpha_factors_001_020', 'log_cache_events': 'False', 'debug': 'False', 'fail_fast': 'False', 'target_path': 'None'}
[0m15:36:50.147656 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3340141c-24bb-4981-818c-bd41409f017a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f760045b360>]}
[0m15:36:50.190223 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3340141c-24bb-4981-818c-bd41409f017a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f75ff69abe0>]}
[0m15:36:50.191220 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:36:50.222601 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:36:50.312755 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:36:50.313905 [debug] [MainThread]: Partial parsing: updated file: quant_features://macros/alpha101/base_operators.sql
[0m15:36:50.352628 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two macros named "signed_power" in the project "quant_features".
   To fix this error, rename or remove one of the following macros:
      - macros/alpha101/alpha_factors.sql
      - macros/alpha101/base_operators.sql
[0m15:36:50.353253 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.38757345, "process_in_blocks": "0", "process_kernel_time": 0.119609, "process_mem_max_rss": "122732", "process_out_blocks": "16", "process_user_time": 1.132364}
[0m15:36:50.353551 [debug] [MainThread]: Command `dbt run` failed at 15:36:50.353490 after 0.39 seconds
[0m15:36:50.353766 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f75fb144950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f75fb144350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f75ffcc4aa0>]}
[0m15:36:50.353979 [debug] [MainThread]: Flushing usage events
[0m15:36:50.405201 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:37:11.422776 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbfbd923770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbfbef3da90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbfbc797d90>]}


============================== 15:37:11.425140 | e388e173-90ae-45b8-8e74-1506eea81d7f ==============================
[0m15:37:11.425140 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:37:11.425470 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'log_cache_events': 'False', 'no_print': 'None', 'target_path': 'None', 'static_parser': 'True', 'version_check': 'True', 'log_path': '/workspace/dbt_project/logs', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'profiles_dir': '/workspace/dbt_project', 'write_json': 'True', 'cache_selected_only': 'False', 'use_colors': 'True', 'empty': 'False', 'invocation_command': 'dbt run --select alpha_factors_001_020', 'printer_width': '80', 'warn_error': 'None', 'debug': 'False', 'partial_parse': 'True', 'fail_fast': 'False', 'introspect': 'True', 'log_format': 'default'}
[0m15:37:11.570235 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e388e173-90ae-45b8-8e74-1506eea81d7f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbfbd613360>]}
[0m15:37:11.612555 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e388e173-90ae-45b8-8e74-1506eea81d7f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbfbc88abe0>]}
[0m15:37:11.613586 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:37:11.645026 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:37:11.735713 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m15:37:11.736904 [debug] [MainThread]: Partial parsing: updated file: quant_features://macros/alpha101/base_operators.sql
[0m15:37:11.737151 [debug] [MainThread]: Partial parsing: updated file: quant_features://macros/alpha101/alpha_factors.sql
[0m15:37:11.777083 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two macros named "ts_rank" in the project "quant_features".
   To fix this error, rename or remove one of the following macros:
      - macros/alpha101/alpha_factors.sql
      - macros/alpha101/base_operators.sql
[0m15:37:11.777740 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.3909641, "process_in_blocks": "0", "process_kernel_time": 0.111822, "process_mem_max_rss": "122776", "process_out_blocks": "8", "process_user_time": 1.154164}
[0m15:37:11.778038 [debug] [MainThread]: Command `dbt run` failed at 15:37:11.777979 after 0.39 seconds
[0m15:37:11.778265 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbfb8388950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbfb8388350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbfbce98aa0>]}
[0m15:37:11.778484 [debug] [MainThread]: Flushing usage events
[0m15:37:11.826894 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:37:34.528749 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a94cdf770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a9632da90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a93b17d90>]}


============================== 15:37:34.531007 | bd3f9a04-ffa1-4928-8666-89f23c1dfc50 ==============================
[0m15:37:34.531007 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:37:34.531343 [debug] [MainThread]: running dbt with arguments {'log_path': '/workspace/dbt_project/logs', 'quiet': 'False', 'static_parser': 'True', 'use_experimental_parser': 'False', 'version_check': 'True', 'introspect': 'True', 'profiles_dir': '/workspace/dbt_project', 'partial_parse': 'True', 'fail_fast': 'False', 'invocation_command': 'dbt run --select alpha_factors_001_020', 'no_print': 'None', 'cache_selected_only': 'False', 'printer_width': '80', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'write_json': 'True', 'indirect_selection': 'eager', 'empty': 'False', 'log_cache_events': 'False', 'target_path': 'None'}
[0m15:37:34.670990 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bd3f9a04-ffa1-4928-8666-89f23c1dfc50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a949cf360>]}
[0m15:37:34.713501 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bd3f9a04-ffa1-4928-8666-89f23c1dfc50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a93c0ebe0>]}
[0m15:37:34.714506 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:37:34.746058 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:37:34.835722 [debug] [MainThread]: Partial parsing enabled: 4 files deleted, 0 files added, 1 files changed.
[0m15:37:34.836109 [debug] [MainThread]: Partial parsing: deleted file: quant_features://macros/alpha101/alpha_factors_21_50.sql
[0m15:37:34.836328 [debug] [MainThread]: Partial parsing: deleted file: quant_features://macros/alpha101/alpha_factors_76_101.sql
[0m15:37:34.836528 [debug] [MainThread]: Partial parsing: deleted file: quant_features://macros/alpha101/alpha_factors_51_75.sql
[0m15:37:34.837271 [debug] [MainThread]: Partial parsing: deleted file: quant_features://macros/alpha101/alpha_factors.sql
[0m15:37:34.837668 [debug] [MainThread]: Partial parsing: updated file: quant_features://macros/alpha101/base_operators.sql
[0m15:37:35.238710 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `dbt_utils.accepted_range`. Arguments to
generic tests should be nested under the `arguments` property.`
[0m15:37:35.239021 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'bd3f9a04-ffa1-4928-8666-89f23c1dfc50', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a93bdd950>]}
[0m15:37:35.349771 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bd3f9a04-ffa1-4928-8666-89f23c1dfc50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a8f877020>]}
[0m15:37:35.417719 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:37:35.418969 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:37:35.430467 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bd3f9a04-ffa1-4928-8666-89f23c1dfc50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a8fbfd1d0>]}
[0m15:37:35.430811 [info ] [MainThread]: Found 17 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:37:35.431030 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bd3f9a04-ffa1-4928-8666-89f23c1dfc50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a8edd5980>]}
[0m15:37:35.432281 [info ] [MainThread]: 
[0m15:37:35.432564 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:37:35.432741 [info ] [MainThread]: 
[0m15:37:35.433035 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:37:35.433953 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features'
[0m15:37:35.448614 [debug] [ThreadPool]: Using duckdb connection "list_quant_features"
[0m15:37:35.448879 [debug] [ThreadPool]: On list_quant_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"quant_features"'
    
  
  
[0m15:37:35.449065 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:37:35.461589 [debug] [ThreadPool]: SQL status: OK in 0.012 seconds
[0m15:37:35.462455 [debug] [ThreadPool]: On list_quant_features: Close
[0m15:37:35.463121 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_quant_features, now create_quant_features_main)
[0m15:37:35.463452 [debug] [ThreadPool]: Creating schema "database: "quant_features"
schema: "main"
"
[0m15:37:35.467573 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:37:35.467839 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='quant_features'
        and type='sqlite'
    
  
[0m15:37:35.468014 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:37:35.468739 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:37:35.469609 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:37:35.469819 [debug] [ThreadPool]: On create_quant_features_main: BEGIN
[0m15:37:35.470115 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:37:35.470293 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:37:35.470447 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
    
        create schema if not exists "quant_features"."main"
    
[0m15:37:35.470769 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:37:35.471302 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:37:35.471505 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:37:35.471664 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:37:35.471955 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:37:35.472139 [debug] [ThreadPool]: On create_quant_features_main: Close
[0m15:37:35.476713 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_quant_features_main, now list_quant_features_main)
[0m15:37:35.480405 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:37:35.481026 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:37:35.481313 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:37:35.481778 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:37:35.481978 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:37:35.482138 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:37:35.488609 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m15:37:35.489565 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:37:35.490163 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:37:35.490384 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:37:35.492105 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bd3f9a04-ffa1-4928-8666-89f23c1dfc50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a8e214650>]}
[0m15:37:35.492491 [debug] [MainThread]: Using duckdb connection "master"
[0m15:37:35.492673 [debug] [MainThread]: On master: BEGIN
[0m15:37:35.492826 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:37:35.493269 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:37:35.493489 [debug] [MainThread]: On master: COMMIT
[0m15:37:35.493640 [debug] [MainThread]: Using duckdb connection "master"
[0m15:37:35.493786 [debug] [MainThread]: On master: COMMIT
[0m15:37:35.494073 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:37:35.494255 [debug] [MainThread]: On master: Close
[0m15:37:35.496302 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_factors_001_020
[0m15:37:35.496702 [info ] [Thread-1 (]: 1 of 1 START sql table model main.alpha_factors_001_020 ........................ [RUN]
[0m15:37:35.496984 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.alpha_factors_001_020)
[0m15:37:35.497198 [debug] [Thread-1 (]: Began compiling node model.quant_features.alpha_factors_001_020
[0m15:37:35.507581 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.alpha_factors_001_020"
[0m15:37:35.508039 [debug] [Thread-1 (]: Began executing node model.quant_features.alpha_factors_001_020
[0m15:37:35.527990 [debug] [Thread-1 (]: Writing runtime sql for node "model.quant_features.alpha_factors_001_020"
[0m15:37:35.528517 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_001_020"
[0m15:37:35.528774 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_001_020: BEGIN
[0m15:37:35.528968 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:37:35.529540 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:37:35.529774 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_001_020"
[0m15:37:35.530230 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_001_020: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_factors_001_020"} */

  
    
    

    create  table
      "quant_features"."main"."alpha_factors_001_020__dbt_tmp"
  
    as (
      

-- Alpha 101 因子计算 (001-020)
-- 基于预处理的基础数据计算前20个Alpha因子

WITH base_data AS (
    SELECT * FROM "quant_features"."main"."alpha_base_data"
),

-- 预计算一些复杂的中间变量
intermediate_calcs AS (
    SELECT 
        *,
        -- Alpha001 相关计算
        
    CASE 
        WHEN CASE WHEN returns < 0 THEN returns_std20 ELSE close END = MAX(CASE WHEN returns < 0 THEN returns_std20 ELSE close END) OVER (
            PARTITION BY symbol 
            ORDER BY timestamp
            ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
        ) 
        THEN 4
        ELSE 0
    END
 AS alpha001_argmax,
        
        -- Alpha002 相关计算
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY 
    
    CASE 
        WHEN volume > 0 THEN LN(volume)
        ELSE NULL
    END
 - 
    LAG(
    CASE 
        WHEN volume > 0 THEN LN(volume)
        ELSE NULL
    END
, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )


    )
 AS alpha002_rank_delta_log_vol,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY 
    CASE 
        WHEN open != 0 THEN (close - open) / (open)
        ELSE NULL
    END

    )
 AS alpha002_rank_ret,
        
        -- Alpha005 相关计算
        close_ma10 AS alpha005_mean_vwap,
        
        -- Alpha007 相关计算
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY 
    ABS(close_delta7)

        ROWS BETWEEN 59 PRECEDING AND CURRENT ROW
    )
 AS alpha007_ts_rank,
        
    CASE 
        WHEN close_delta7 > 0 THEN 1
        WHEN close_delta7 < 0 THEN -1
        ELSE 0
    END
 AS alpha007_sign,
        
        -- Alpha008 相关计算
        (open * 5 + returns_sum250 / 50) AS alpha008_sum_open_returns,  -- 简化计算
        
    LAG((open * 5 + returns_sum250 / 50), 10) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS alpha008_delay_sum,
        
        -- Alpha009-010 逻辑
        CASE 
            WHEN 
    MIN(close_delta1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 > 0 THEN close_delta1
            WHEN 
    MAX(close_delta1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 < 0 THEN close_delta1
            ELSE -1 * close_delta1
        END AS alpha009_logic,
        
        -- Alpha011 相关计算
        
    MAX(vwap - close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha011_max_vwap_close,
        
    MIN(vwap - close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha011_min_vwap_close,
        
        -- Alpha014 相关计算
        
    returns - 
    LAG(returns, 3) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS alpha014_delta_returns,
        
        -- Alpha015 相关计算
        
    CORR(high_rank, volume_rank) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha015_corr_high_vol,
        
        -- Alpha017 相关计算
        
    close_delta1 - 
    LAG(close_delta1, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS alpha017_delta_delta_close,
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY 
    CASE 
        WHEN adv20 != 0 THEN (volume) / (adv20)
        ELSE NULL
    END

        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS alpha017_ts_rank_vol_adv,
        
        -- Alpha018 相关计算
        
    STDDEV(
    ABS(close - open)
) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS alpha018_stddev,
        close - open AS alpha018_close_open_diff,
        
    CORR(close, open) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS alpha018_corr_close_open,
        
        -- Alpha019 相关计算
        (close - close_lag7) + close_delta7 AS alpha019_close_diff_plus_delta,
        1 + returns_sum250 AS alpha019_sum_returns,
        
        -- Alpha020 相关计算
        open - high_lag1 AS alpha020_open_delay_high,
        open - close_lag1 AS alpha020_open_delay_close,
        open - low_lag1 AS alpha020_open_delay_low
        
    FROM base_data
),

-- 计算Alpha因子
alpha_factors AS (
    SELECT 
        symbol,
        timestamp,
        
        -- Alpha 001: RANK(Ts_ArgMax(SignedPower(((returns < 0) ? stddev(returns, 20) : close), 2.), 5)) - 0.5
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha001_argmax
    )
 - 0.5 AS alpha001,
        
        -- Alpha 002: (-1 * correlation(rank(delta(log(volume), 2)), rank(((close - open) / open)), 6))
        -1 * 
    CORR(alpha002_rank_delta_log_vol, alpha002_rank_ret) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 5 PRECEDING AND CURRENT ROW
    )
 AS alpha002,
        
        -- Alpha 003: (-1 * correlation(rank(open), rank(volume), 10))
        -1 * corr_open_volume_10 AS alpha003,
        
        -- Alpha 004: (-1 * Ts_Rank(rank(low), 9))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY low_rank
        ROWS BETWEEN 8 PRECEDING AND CURRENT ROW
    )
 AS alpha004,
        
        -- Alpha 005: (rank((open - (sum(vwap, 10) / 10))) * (-1 * abs(rank((close - vwap)))))
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY open - alpha005_mean_vwap
    )
 * (-1 * 
    ABS(
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY close - vwap
    )
)
) AS alpha005,
        
        -- Alpha 006: (-1 * correlation(open, volume, 10))
        -1 * corr_open_volume_10 AS alpha006,
        
        -- Alpha 007: ((adv20 < volume) ? ((-1 * ts_rank(abs(delta(close, 7)), 60)) * sign(delta(close, 7))) : (-1))
        CASE 
            WHEN adv20 < volume THEN (-1 * alpha007_ts_rank) * alpha007_sign
            ELSE -1
        END AS alpha007,
        
        -- Alpha 008: (-1 * rank(((sum(open, 5) * sum(returns, 5)) - delay((sum(open, 5) * sum(returns, 5)), 10))))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha008_sum_open_returns - alpha008_delay_sum
    )
 AS alpha008,
        
        -- Alpha 009: ((0 < ts_min(delta(close, 1), 5)) ? delta(close, 1) : ((ts_max(delta(close, 1), 5) < 0) ? delta(close, 1) : (-1 * delta(close, 1))))
        alpha009_logic AS alpha009,
        
        -- Alpha 010: rank(((0 < ts_min(delta(close, 1), 4)) ? delta(close, 1) : ((ts_max(delta(close, 1), 4) < 0) ? delta(close, 1) : (-1 * delta(close, 1)))))
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha009_logic
    )
 AS alpha010,
        
        -- Alpha 011: ((rank(ts_max((vwap - close), 3)) + rank(ts_min((vwap - close), 3))) * rank(delta(volume, 3)))
        (
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha011_max_vwap_close
    )
 + 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha011_min_vwap_close
    )
) * 
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY volume_delta3
    )
 AS alpha011,
        
        -- Alpha 012: (sign(delta(volume, 1)) * (-1 * delta(close, 1)))
        
    CASE 
        WHEN volume_delta1 > 0 THEN 1
        WHEN volume_delta1 < 0 THEN -1
        ELSE 0
    END
 * (-1 * close_delta1) AS alpha012,
        
        -- Alpha 013: (-1 * rank(covariance(rank(close), rank(volume), 5)))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY cov_close_volume_5
    )
 AS alpha013,
        
        -- Alpha 014: ((-1 * rank(delta(returns, 3))) * correlation(open, volume, 10))
        (-1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha014_delta_returns
    )
) * corr_open_volume_10 AS alpha014,
        
        -- Alpha 015: (-1 * sum(rank(correlation(rank(high), rank(volume), 3)), 3))
        -1 * 
    SUM(
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha015_corr_high_vol
    )
) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha015,
        
        -- Alpha 016: (-1 * rank(covariance(rank(high), rank(volume), 5)))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY cov_high_volume_5
    )
 AS alpha016,
        
        -- Alpha 017: (((-1 * rank(ts_rank(close, 10))) * rank(delta(delta(close, 1), 1))) * rank(ts_rank((volume / adv20), 5)))
        ((-1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY close_ts_rank10
    )
) * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha017_delta_delta_close
    )
) * 
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha017_ts_rank_vol_adv
    )
 AS alpha017,
        
        -- Alpha 018: (-1 * rank(((stddev(abs((close - open)), 5) + (close - open)) + correlation(close, open, 10))))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha018_stddev + alpha018_close_open_diff + alpha018_corr_close_open
    )
 AS alpha018,
        
        -- Alpha 019: ((-1 * sign(((close - delay(close, 7)) + delta(close, 7)))) * (1 + rank((1 + sum(returns, 250)))))
        (-1 * 
    CASE 
        WHEN alpha019_close_diff_plus_delta > 0 THEN 1
        WHEN alpha019_close_diff_plus_delta < 0 THEN -1
        ELSE 0
    END
) * 
        (1 + 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha019_sum_returns
    )
) AS alpha019,
        
        -- Alpha 020: (((-1 * rank((open - delay(high, 1)))) * rank((open - delay(close, 1)))) * rank((open - delay(low, 1))))
        ((-1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha020_open_delay_high
    )
) * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha020_open_delay_close
    )
) * 
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha020_open_delay_low
    )
 AS alpha020
        
    FROM intermediate_calcs
)

SELECT * FROM alpha_factors
    );
  
  
[0m15:37:35.531636 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_factors_001_020"} */

  
    
    

    create  table
      "quant_features"."main"."alpha_factors_001_020__dbt_tmp"
  
    as (
      

-- Alpha 101 因子计算 (001-020)
-- 基于预处理的基础数据计算前20个Alpha因子

WITH base_data AS (
    SELECT * FROM "quant_features"."main"."alpha_base_data"
),

-- 预计算一些复杂的中间变量
intermediate_calcs AS (
    SELECT 
        *,
        -- Alpha001 相关计算
        
    CASE 
        WHEN CASE WHEN returns < 0 THEN returns_std20 ELSE close END = MAX(CASE WHEN returns < 0 THEN returns_std20 ELSE close END) OVER (
            PARTITION BY symbol 
            ORDER BY timestamp
            ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
        ) 
        THEN 4
        ELSE 0
    END
 AS alpha001_argmax,
        
        -- Alpha002 相关计算
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY 
    
    CASE 
        WHEN volume > 0 THEN LN(volume)
        ELSE NULL
    END
 - 
    LAG(
    CASE 
        WHEN volume > 0 THEN LN(volume)
        ELSE NULL
    END
, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )


    )
 AS alpha002_rank_delta_log_vol,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY 
    CASE 
        WHEN open != 0 THEN (close - open) / (open)
        ELSE NULL
    END

    )
 AS alpha002_rank_ret,
        
        -- Alpha005 相关计算
        close_ma10 AS alpha005_mean_vwap,
        
        -- Alpha007 相关计算
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY 
    ABS(close_delta7)

        ROWS BETWEEN 59 PRECEDING AND CURRENT ROW
    )
 AS alpha007_ts_rank,
        
    CASE 
        WHEN close_delta7 > 0 THEN 1
        WHEN close_delta7 < 0 THEN -1
        ELSE 0
    END
 AS alpha007_sign,
        
        -- Alpha008 相关计算
        (open * 5 + returns_sum250 / 50) AS alpha008_sum_open_returns,  -- 简化计算
        
    LAG((open * 5 + returns_sum250 / 50), 10) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS alpha008_delay_sum,
        
        -- Alpha009-010 逻辑
        CASE 
            WHEN 
    MIN(close_delta1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 > 0 THEN close_delta1
            WHEN 
    MAX(close_delta1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 < 0 THEN close_delta1
            ELSE -1 * close_delta1
        END AS alpha009_logic,
        
        -- Alpha011 相关计算
        
    MAX(vwap - close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha011_max_vwap_close,
        
    MIN(vwap - close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha011_min_vwap_close,
        
        -- Alpha014 相关计算
        
    returns - 
    LAG(returns, 3) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS alpha014_delta_returns,
        
        -- Alpha015 相关计算
        
    CORR(high_rank, volume_rank) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha015_corr_high_vol,
        
        -- Alpha017 相关计算
        
    close_delta1 - 
    LAG(close_delta1, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS alpha017_delta_delta_close,
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY 
    CASE 
        WHEN adv20 != 0 THEN (volume) / (adv20)
        ELSE NULL
    END

        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS alpha017_ts_rank_vol_adv,
        
        -- Alpha018 相关计算
        
    STDDEV(
    ABS(close - open)
) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS alpha018_stddev,
        close - open AS alpha018_close_open_diff,
        
    CORR(close, open) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS alpha018_corr_close_open,
        
        -- Alpha019 相关计算
        (close - close_lag7) + close_delta7 AS alpha019_close_diff_plus_delta,
        1 + returns_sum250 AS alpha019_sum_returns,
        
        -- Alpha020 相关计算
        open - high_lag1 AS alpha020_open_delay_high,
        open - close_lag1 AS alpha020_open_delay_close,
        open - low_lag1 AS alpha020_open_delay_low
        
    FROM base_data
),

-- 计算Alpha因子
alpha_factors AS (
    SELECT 
        symbol,
        timestamp,
        
        -- Alpha 001: RANK(Ts_ArgMax(SignedPower(((returns < 0) ? stddev(returns, 20) : close), 2.), 5)) - 0.5
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha001_argmax
    )
 - 0.5 AS alpha001,
        
        -- Alpha 002: (-1 * correlation(rank(delta(log(volume), 2)), rank(((close - open) / open)), 6))
        -1 * 
    CORR(alpha002_rank_delta_log_vol, alpha002_rank_ret) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 5 PRECEDING AND CURRENT ROW
    )
 AS alpha002,
        
        -- Alpha 003: (-1 * correlation(rank(open), rank(volume), 10))
        -1 * corr_open_volume_10 AS alpha003,
        
        -- Alpha 004: (-1 * Ts_Rank(rank(low), 9))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY low_rank
        ROWS BETWEEN 8 PRECEDING AND CURRENT ROW
    )
 AS alpha004,
        
        -- Alpha 005: (rank((open - (sum(vwap, 10) / 10))) * (-1 * abs(rank((close - vwap)))))
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY open - alpha005_mean_vwap
    )
 * (-1 * 
    ABS(
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY close - vwap
    )
)
) AS alpha005,
        
        -- Alpha 006: (-1 * correlation(open, volume, 10))
        -1 * corr_open_volume_10 AS alpha006,
        
        -- Alpha 007: ((adv20 < volume) ? ((-1 * ts_rank(abs(delta(close, 7)), 60)) * sign(delta(close, 7))) : (-1))
        CASE 
            WHEN adv20 < volume THEN (-1 * alpha007_ts_rank) * alpha007_sign
            ELSE -1
        END AS alpha007,
        
        -- Alpha 008: (-1 * rank(((sum(open, 5) * sum(returns, 5)) - delay((sum(open, 5) * sum(returns, 5)), 10))))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha008_sum_open_returns - alpha008_delay_sum
    )
 AS alpha008,
        
        -- Alpha 009: ((0 < ts_min(delta(close, 1), 5)) ? delta(close, 1) : ((ts_max(delta(close, 1), 5) < 0) ? delta(close, 1) : (-1 * delta(close, 1))))
        alpha009_logic AS alpha009,
        
        -- Alpha 010: rank(((0 < ts_min(delta(close, 1), 4)) ? delta(close, 1) : ((ts_max(delta(close, 1), 4) < 0) ? delta(close, 1) : (-1 * delta(close, 1)))))
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha009_logic
    )
 AS alpha010,
        
        -- Alpha 011: ((rank(ts_max((vwap - close), 3)) + rank(ts_min((vwap - close), 3))) * rank(delta(volume, 3)))
        (
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha011_max_vwap_close
    )
 + 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha011_min_vwap_close
    )
) * 
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY volume_delta3
    )
 AS alpha011,
        
        -- Alpha 012: (sign(delta(volume, 1)) * (-1 * delta(close, 1)))
        
    CASE 
        WHEN volume_delta1 > 0 THEN 1
        WHEN volume_delta1 < 0 THEN -1
        ELSE 0
    END
 * (-1 * close_delta1) AS alpha012,
        
        -- Alpha 013: (-1 * rank(covariance(rank(close), rank(volume), 5)))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY cov_close_volume_5
    )
 AS alpha013,
        
        -- Alpha 014: ((-1 * rank(delta(returns, 3))) * correlation(open, volume, 10))
        (-1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha014_delta_returns
    )
) * corr_open_volume_10 AS alpha014,
        
        -- Alpha 015: (-1 * sum(rank(correlation(rank(high), rank(volume), 3)), 3))
        -1 * 
    SUM(
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha015_corr_high_vol
    )
) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha015,
        
        -- Alpha 016: (-1 * rank(covariance(rank(high), rank(volume), 5)))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY cov_high_volume_5
    )
 AS alpha016,
        
        -- Alpha 017: (((-1 * rank(ts_rank(close, 10))) * rank(delta(delta(close, 1), 1))) * rank(ts_rank((volume / adv20), 5)))
        ((-1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY close_ts_rank10
    )
) * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha017_delta_delta_close
    )
) * 
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha017_ts_rank_vol_adv
    )
 AS alpha017,
        
        -- Alpha 018: (-1 * rank(((stddev(abs((close - open)), 5) + (close - open)) + correlation(close, open, 10))))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha018_stddev + alpha018_close_open_diff + alpha018_corr_close_open
    )
 AS alpha018,
        
        -- Alpha 019: ((-1 * sign(((close - delay(close, 7)) + delta(close, 7)))) * (1 + rank((1 + sum(returns, 250)))))
        (-1 * 
    CASE 
        WHEN alpha019_close_diff_plus_delta > 0 THEN 1
        WHEN alpha019_close_diff_plus_delta < 0 THEN -1
        ELSE 0
    END
) * 
        (1 + 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha019_sum_returns
    )
) AS alpha019,
        
        -- Alpha 020: (((-1 * rank((open - delay(high, 1)))) * rank((open - delay(close, 1)))) * rank((open - delay(low, 1))))
        ((-1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha020_open_delay_high
    )
) * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha020_open_delay_close
    )
) * 
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha020_open_delay_low
    )
 AS alpha020
        
    FROM intermediate_calcs
)

SELECT * FROM alpha_factors
    );
  
  
[0m15:37:35.532198 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m15:37:35.532494 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_001_020: ROLLBACK
[0m15:37:35.535852 [debug] [Thread-1 (]: Failed to rollback 'model.quant_features.alpha_factors_001_020'
[0m15:37:35.536119 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_001_020: Close
[0m15:37:35.538507 [debug] [Thread-1 (]: Runtime Error in model alpha_factors_001_020 (models/alpha101/alpha_factors_001_020.sql)
  Parser Error: window functions are not allowed in window definitions
[0m15:37:35.539529 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bd3f9a04-ffa1-4928-8666-89f23c1dfc50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a8f9fb490>]}
[0m15:37:35.539985 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model main.alpha_factors_001_020 ............... [[31mERROR[0m in 0.04s]
[0m15:37:35.540351 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_factors_001_020
[0m15:37:35.541057 [debug] [Thread-7 (]: Marking all children of 'model.quant_features.alpha_factors_001_020' to be skipped because of status 'error'.  Reason: Runtime Error in model alpha_factors_001_020 (models/alpha101/alpha_factors_001_020.sql)
  Parser Error: window functions are not allowed in window definitions.
[0m15:37:35.542961 [debug] [MainThread]: Using duckdb connection "master"
[0m15:37:35.543227 [debug] [MainThread]: On master: BEGIN
[0m15:37:35.543400 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:37:35.543793 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:37:35.543985 [debug] [MainThread]: On master: COMMIT
[0m15:37:35.544169 [debug] [MainThread]: Using duckdb connection "master"
[0m15:37:35.544334 [debug] [MainThread]: On master: COMMIT
[0m15:37:35.544635 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:37:35.544829 [debug] [MainThread]: On master: Close
[0m15:37:35.545100 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:37:35.545280 [debug] [MainThread]: Connection 'model.quant_features.alpha_factors_001_020' was properly closed.
[0m15:37:35.545459 [info ] [MainThread]: 
[0m15:37:35.545649 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.11 seconds (0.11s).
[0m15:37:35.546020 [debug] [MainThread]: Command end result
[0m15:37:35.566648 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:37:35.567787 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:37:35.571392 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:37:35.571621 [info ] [MainThread]: 
[0m15:37:35.571833 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:37:35.572005 [info ] [MainThread]: 
[0m15:37:35.572217 [error] [MainThread]: [31mFailure in model alpha_factors_001_020 (models/alpha101/alpha_factors_001_020.sql)[0m
[0m15:37:35.572427 [error] [MainThread]:   Runtime Error in model alpha_factors_001_020 (models/alpha101/alpha_factors_001_020.sql)
  Parser Error: window functions are not allowed in window definitions
[0m15:37:35.572575 [info ] [MainThread]: 
[0m15:37:35.572758 [info ] [MainThread]:   compiled code at target/compiled/quant_features/models/alpha101/alpha_factors_001_020.sql
[0m15:37:35.572914 [info ] [MainThread]: 
[0m15:37:35.573083 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m15:37:35.573381 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 11 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m15:37:35.573927 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.0815535, "process_in_blocks": "0", "process_kernel_time": 0.111952, "process_mem_max_rss": "155844", "process_out_blocks": "5264", "process_user_time": 1.839219}
[0m15:37:35.574238 [debug] [MainThread]: Command `dbt run` failed at 15:37:35.574164 after 1.08 seconds
[0m15:37:35.574462 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a8fb53f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a8e2ef260>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a8ed8c350>]}
[0m15:37:35.574673 [debug] [MainThread]: Flushing usage events
[0m15:37:35.600699 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:37:38.019423 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63249af770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6326011a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f632382fd90>]}


============================== 15:37:38.021639 | 2a361f1d-3ef5-4ebb-a35e-9b6fa2592538 ==============================
[0m15:37:38.021639 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:37:38.021958 [debug] [MainThread]: running dbt with arguments {'profiles_dir': '/workspace/dbt_project', 'version_check': 'True', 'debug': 'False', 'warn_error': 'None', 'quiet': 'False', 'cache_selected_only': 'False', 'write_json': 'True', 'log_path': '/workspace/dbt_project/logs', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'invocation_command': 'dbt run --select alpha_factors_001_020', 'static_parser': 'True', 'printer_width': '80', 'introspect': 'True', 'use_colors': 'True', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'target_path': 'None', 'fail_fast': 'False', 'no_print': 'None', 'empty': 'False', 'send_anonymous_usage_stats': 'True'}
[0m15:37:38.160131 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2a361f1d-3ef5-4ebb-a35e-9b6fa2592538', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63246ab360>]}
[0m15:37:38.202139 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2a361f1d-3ef5-4ebb-a35e-9b6fa2592538', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6323922be0>]}
[0m15:37:38.203115 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:37:38.234639 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:37:38.322493 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:37:38.322731 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:37:38.359358 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2a361f1d-3ef5-4ebb-a35e-9b6fa2592538', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f631f925b50>]}
[0m15:37:38.428533 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:37:38.429843 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:37:38.440504 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2a361f1d-3ef5-4ebb-a35e-9b6fa2592538', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f631f954c80>]}
[0m15:37:38.440925 [info ] [MainThread]: Found 17 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:37:38.441169 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2a361f1d-3ef5-4ebb-a35e-9b6fa2592538', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f631f995470>]}
[0m15:37:38.442417 [info ] [MainThread]: 
[0m15:37:38.442668 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:37:38.442830 [info ] [MainThread]: 
[0m15:37:38.443124 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:37:38.444057 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features'
[0m15:37:38.491684 [debug] [ThreadPool]: Using duckdb connection "list_quant_features"
[0m15:37:38.491944 [debug] [ThreadPool]: On list_quant_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"quant_features"'
    
  
  
[0m15:37:38.492132 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:37:38.503788 [debug] [ThreadPool]: SQL status: OK in 0.012 seconds
[0m15:37:38.504665 [debug] [ThreadPool]: On list_quant_features: Close
[0m15:37:38.505600 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_quant_features, now create_quant_features_main)
[0m15:37:38.505971 [debug] [ThreadPool]: Creating schema "database: "quant_features"
schema: "main"
"
[0m15:37:38.510210 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:37:38.510470 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='quant_features'
        and type='sqlite'
    
  
[0m15:37:38.510647 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:37:38.511313 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:37:38.512145 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:37:38.512397 [debug] [ThreadPool]: On create_quant_features_main: BEGIN
[0m15:37:38.512705 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:37:38.512875 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:37:38.513026 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
    
        create schema if not exists "quant_features"."main"
    
[0m15:37:38.513331 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:37:38.513862 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:37:38.514068 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:37:38.514250 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:37:38.514541 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:37:38.514720 [debug] [ThreadPool]: On create_quant_features_main: Close
[0m15:37:38.519244 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_quant_features_main, now list_quant_features_main)
[0m15:37:38.522906 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:37:38.523148 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:37:38.523324 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:37:38.523772 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:37:38.523965 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:37:38.524127 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:37:38.530579 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m15:37:38.531519 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:37:38.532104 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:37:38.532323 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:37:38.534205 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2a361f1d-3ef5-4ebb-a35e-9b6fa2592538', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f631e916dd0>]}
[0m15:37:38.534573 [debug] [MainThread]: Using duckdb connection "master"
[0m15:37:38.534760 [debug] [MainThread]: On master: BEGIN
[0m15:37:38.534912 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:37:38.535288 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:37:38.535470 [debug] [MainThread]: On master: COMMIT
[0m15:37:38.535617 [debug] [MainThread]: Using duckdb connection "master"
[0m15:37:38.535754 [debug] [MainThread]: On master: COMMIT
[0m15:37:38.536013 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:37:38.536199 [debug] [MainThread]: On master: Close
[0m15:37:38.539436 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_factors_001_020
[0m15:37:38.539847 [info ] [Thread-1 (]: 1 of 1 START sql table model main.alpha_factors_001_020 ........................ [RUN]
[0m15:37:38.540149 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.alpha_factors_001_020)
[0m15:37:38.540388 [debug] [Thread-1 (]: Began compiling node model.quant_features.alpha_factors_001_020
[0m15:37:38.560760 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.alpha_factors_001_020"
[0m15:37:38.561221 [debug] [Thread-1 (]: Began executing node model.quant_features.alpha_factors_001_020
[0m15:37:38.581565 [debug] [Thread-1 (]: Writing runtime sql for node "model.quant_features.alpha_factors_001_020"
[0m15:37:38.582070 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_001_020"
[0m15:37:38.582320 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_001_020: BEGIN
[0m15:37:38.582514 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:37:38.582995 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:37:38.583231 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_001_020"
[0m15:37:38.583688 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_001_020: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_factors_001_020"} */

  
    
    

    create  table
      "quant_features"."main"."alpha_factors_001_020__dbt_tmp"
  
    as (
      

-- Alpha 101 因子计算 (001-020)
-- 基于预处理的基础数据计算前20个Alpha因子

WITH base_data AS (
    SELECT * FROM "quant_features"."main"."alpha_base_data"
),

-- 预计算一些复杂的中间变量
intermediate_calcs AS (
    SELECT 
        *,
        -- Alpha001 相关计算
        
    CASE 
        WHEN CASE WHEN returns < 0 THEN returns_std20 ELSE close END = MAX(CASE WHEN returns < 0 THEN returns_std20 ELSE close END) OVER (
            PARTITION BY symbol 
            ORDER BY timestamp
            ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
        ) 
        THEN 4
        ELSE 0
    END
 AS alpha001_argmax,
        
        -- Alpha002 相关计算
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY 
    
    CASE 
        WHEN volume > 0 THEN LN(volume)
        ELSE NULL
    END
 - 
    LAG(
    CASE 
        WHEN volume > 0 THEN LN(volume)
        ELSE NULL
    END
, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )


    )
 AS alpha002_rank_delta_log_vol,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY 
    CASE 
        WHEN open != 0 THEN (close - open) / (open)
        ELSE NULL
    END

    )
 AS alpha002_rank_ret,
        
        -- Alpha005 相关计算
        close_ma10 AS alpha005_mean_vwap,
        
        -- Alpha007 相关计算
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY 
    ABS(close_delta7)

        ROWS BETWEEN 59 PRECEDING AND CURRENT ROW
    )
 AS alpha007_ts_rank,
        
    CASE 
        WHEN close_delta7 > 0 THEN 1
        WHEN close_delta7 < 0 THEN -1
        ELSE 0
    END
 AS alpha007_sign,
        
        -- Alpha008 相关计算
        (open * 5 + returns_sum250 / 50) AS alpha008_sum_open_returns,  -- 简化计算
        
    LAG((open * 5 + returns_sum250 / 50), 10) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS alpha008_delay_sum,
        
        -- Alpha009-010 逻辑
        CASE 
            WHEN 
    MIN(close_delta1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 > 0 THEN close_delta1
            WHEN 
    MAX(close_delta1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 < 0 THEN close_delta1
            ELSE -1 * close_delta1
        END AS alpha009_logic,
        
        -- Alpha011 相关计算
        
    MAX(vwap - close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha011_max_vwap_close,
        
    MIN(vwap - close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha011_min_vwap_close,
        
        -- Alpha014 相关计算
        
    returns - 
    LAG(returns, 3) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS alpha014_delta_returns,
        
        -- Alpha015 相关计算
        
    CORR(high_rank, volume_rank) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha015_corr_high_vol,
        
        -- Alpha017 相关计算
        
    close_delta1 - 
    LAG(close_delta1, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS alpha017_delta_delta_close,
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY 
    CASE 
        WHEN adv20 != 0 THEN (volume) / (adv20)
        ELSE NULL
    END

        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS alpha017_ts_rank_vol_adv,
        
        -- Alpha018 相关计算
        
    STDDEV(
    ABS(close - open)
) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS alpha018_stddev,
        close - open AS alpha018_close_open_diff,
        
    CORR(close, open) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS alpha018_corr_close_open,
        
        -- Alpha019 相关计算
        (close - close_lag7) + close_delta7 AS alpha019_close_diff_plus_delta,
        1 + returns_sum250 AS alpha019_sum_returns,
        
        -- Alpha020 相关计算
        open - high_lag1 AS alpha020_open_delay_high,
        open - close_lag1 AS alpha020_open_delay_close,
        open - low_lag1 AS alpha020_open_delay_low
        
    FROM base_data
),

-- 计算Alpha因子
alpha_factors AS (
    SELECT 
        symbol,
        timestamp,
        
        -- Alpha 001: RANK(Ts_ArgMax(SignedPower(((returns < 0) ? stddev(returns, 20) : close), 2.), 5)) - 0.5
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha001_argmax
    )
 - 0.5 AS alpha001,
        
        -- Alpha 002: (-1 * correlation(rank(delta(log(volume), 2)), rank(((close - open) / open)), 6))
        -1 * 
    CORR(alpha002_rank_delta_log_vol, alpha002_rank_ret) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 5 PRECEDING AND CURRENT ROW
    )
 AS alpha002,
        
        -- Alpha 003: (-1 * correlation(rank(open), rank(volume), 10))
        -1 * corr_open_volume_10 AS alpha003,
        
        -- Alpha 004: (-1 * Ts_Rank(rank(low), 9))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY low_rank
        ROWS BETWEEN 8 PRECEDING AND CURRENT ROW
    )
 AS alpha004,
        
        -- Alpha 005: (rank((open - (sum(vwap, 10) / 10))) * (-1 * abs(rank((close - vwap)))))
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY open - alpha005_mean_vwap
    )
 * (-1 * 
    ABS(
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY close - vwap
    )
)
) AS alpha005,
        
        -- Alpha 006: (-1 * correlation(open, volume, 10))
        -1 * corr_open_volume_10 AS alpha006,
        
        -- Alpha 007: ((adv20 < volume) ? ((-1 * ts_rank(abs(delta(close, 7)), 60)) * sign(delta(close, 7))) : (-1))
        CASE 
            WHEN adv20 < volume THEN (-1 * alpha007_ts_rank) * alpha007_sign
            ELSE -1
        END AS alpha007,
        
        -- Alpha 008: (-1 * rank(((sum(open, 5) * sum(returns, 5)) - delay((sum(open, 5) * sum(returns, 5)), 10))))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha008_sum_open_returns - alpha008_delay_sum
    )
 AS alpha008,
        
        -- Alpha 009: ((0 < ts_min(delta(close, 1), 5)) ? delta(close, 1) : ((ts_max(delta(close, 1), 5) < 0) ? delta(close, 1) : (-1 * delta(close, 1))))
        alpha009_logic AS alpha009,
        
        -- Alpha 010: rank(((0 < ts_min(delta(close, 1), 4)) ? delta(close, 1) : ((ts_max(delta(close, 1), 4) < 0) ? delta(close, 1) : (-1 * delta(close, 1)))))
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha009_logic
    )
 AS alpha010,
        
        -- Alpha 011: ((rank(ts_max((vwap - close), 3)) + rank(ts_min((vwap - close), 3))) * rank(delta(volume, 3)))
        (
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha011_max_vwap_close
    )
 + 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha011_min_vwap_close
    )
) * 
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY volume_delta3
    )
 AS alpha011,
        
        -- Alpha 012: (sign(delta(volume, 1)) * (-1 * delta(close, 1)))
        
    CASE 
        WHEN volume_delta1 > 0 THEN 1
        WHEN volume_delta1 < 0 THEN -1
        ELSE 0
    END
 * (-1 * close_delta1) AS alpha012,
        
        -- Alpha 013: (-1 * rank(covariance(rank(close), rank(volume), 5)))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY cov_close_volume_5
    )
 AS alpha013,
        
        -- Alpha 014: ((-1 * rank(delta(returns, 3))) * correlation(open, volume, 10))
        (-1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha014_delta_returns
    )
) * corr_open_volume_10 AS alpha014,
        
        -- Alpha 015: (-1 * sum(rank(correlation(rank(high), rank(volume), 3)), 3))
        -1 * 
    SUM(
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha015_corr_high_vol
    )
) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha015,
        
        -- Alpha 016: (-1 * rank(covariance(rank(high), rank(volume), 5)))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY cov_high_volume_5
    )
 AS alpha016,
        
        -- Alpha 017: (((-1 * rank(ts_rank(close, 10))) * rank(delta(delta(close, 1), 1))) * rank(ts_rank((volume / adv20), 5)))
        ((-1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY close_ts_rank10
    )
) * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha017_delta_delta_close
    )
) * 
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha017_ts_rank_vol_adv
    )
 AS alpha017,
        
        -- Alpha 018: (-1 * rank(((stddev(abs((close - open)), 5) + (close - open)) + correlation(close, open, 10))))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha018_stddev + alpha018_close_open_diff + alpha018_corr_close_open
    )
 AS alpha018,
        
        -- Alpha 019: ((-1 * sign(((close - delay(close, 7)) + delta(close, 7)))) * (1 + rank((1 + sum(returns, 250)))))
        (-1 * 
    CASE 
        WHEN alpha019_close_diff_plus_delta > 0 THEN 1
        WHEN alpha019_close_diff_plus_delta < 0 THEN -1
        ELSE 0
    END
) * 
        (1 + 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha019_sum_returns
    )
) AS alpha019,
        
        -- Alpha 020: (((-1 * rank((open - delay(high, 1)))) * rank((open - delay(close, 1)))) * rank((open - delay(low, 1))))
        ((-1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha020_open_delay_high
    )
) * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha020_open_delay_close
    )
) * 
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha020_open_delay_low
    )
 AS alpha020
        
    FROM intermediate_calcs
)

SELECT * FROM alpha_factors
    );
  
  
[0m15:37:38.584889 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_factors_001_020"} */

  
    
    

    create  table
      "quant_features"."main"."alpha_factors_001_020__dbt_tmp"
  
    as (
      

-- Alpha 101 因子计算 (001-020)
-- 基于预处理的基础数据计算前20个Alpha因子

WITH base_data AS (
    SELECT * FROM "quant_features"."main"."alpha_base_data"
),

-- 预计算一些复杂的中间变量
intermediate_calcs AS (
    SELECT 
        *,
        -- Alpha001 相关计算
        
    CASE 
        WHEN CASE WHEN returns < 0 THEN returns_std20 ELSE close END = MAX(CASE WHEN returns < 0 THEN returns_std20 ELSE close END) OVER (
            PARTITION BY symbol 
            ORDER BY timestamp
            ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
        ) 
        THEN 4
        ELSE 0
    END
 AS alpha001_argmax,
        
        -- Alpha002 相关计算
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY 
    
    CASE 
        WHEN volume > 0 THEN LN(volume)
        ELSE NULL
    END
 - 
    LAG(
    CASE 
        WHEN volume > 0 THEN LN(volume)
        ELSE NULL
    END
, 2) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )


    )
 AS alpha002_rank_delta_log_vol,
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY 
    CASE 
        WHEN open != 0 THEN (close - open) / (open)
        ELSE NULL
    END

    )
 AS alpha002_rank_ret,
        
        -- Alpha005 相关计算
        close_ma10 AS alpha005_mean_vwap,
        
        -- Alpha007 相关计算
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY 
    ABS(close_delta7)

        ROWS BETWEEN 59 PRECEDING AND CURRENT ROW
    )
 AS alpha007_ts_rank,
        
    CASE 
        WHEN close_delta7 > 0 THEN 1
        WHEN close_delta7 < 0 THEN -1
        ELSE 0
    END
 AS alpha007_sign,
        
        -- Alpha008 相关计算
        (open * 5 + returns_sum250 / 50) AS alpha008_sum_open_returns,  -- 简化计算
        
    LAG((open * 5 + returns_sum250 / 50), 10) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )
 AS alpha008_delay_sum,
        
        -- Alpha009-010 逻辑
        CASE 
            WHEN 
    MIN(close_delta1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 > 0 THEN close_delta1
            WHEN 
    MAX(close_delta1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 < 0 THEN close_delta1
            ELSE -1 * close_delta1
        END AS alpha009_logic,
        
        -- Alpha011 相关计算
        
    MAX(vwap - close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha011_max_vwap_close,
        
    MIN(vwap - close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha011_min_vwap_close,
        
        -- Alpha014 相关计算
        
    returns - 
    LAG(returns, 3) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS alpha014_delta_returns,
        
        -- Alpha015 相关计算
        
    CORR(high_rank, volume_rank) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha015_corr_high_vol,
        
        -- Alpha017 相关计算
        
    close_delta1 - 
    LAG(close_delta1, 1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
    )

 AS alpha017_delta_delta_close,
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY 
    CASE 
        WHEN adv20 != 0 THEN (volume) / (adv20)
        ELSE NULL
    END

        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS alpha017_ts_rank_vol_adv,
        
        -- Alpha018 相关计算
        
    STDDEV(
    ABS(close - open)
) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS alpha018_stddev,
        close - open AS alpha018_close_open_diff,
        
    CORR(close, open) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS alpha018_corr_close_open,
        
        -- Alpha019 相关计算
        (close - close_lag7) + close_delta7 AS alpha019_close_diff_plus_delta,
        1 + returns_sum250 AS alpha019_sum_returns,
        
        -- Alpha020 相关计算
        open - high_lag1 AS alpha020_open_delay_high,
        open - close_lag1 AS alpha020_open_delay_close,
        open - low_lag1 AS alpha020_open_delay_low
        
    FROM base_data
),

-- 计算Alpha因子
alpha_factors AS (
    SELECT 
        symbol,
        timestamp,
        
        -- Alpha 001: RANK(Ts_ArgMax(SignedPower(((returns < 0) ? stddev(returns, 20) : close), 2.), 5)) - 0.5
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha001_argmax
    )
 - 0.5 AS alpha001,
        
        -- Alpha 002: (-1 * correlation(rank(delta(log(volume), 2)), rank(((close - open) / open)), 6))
        -1 * 
    CORR(alpha002_rank_delta_log_vol, alpha002_rank_ret) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 5 PRECEDING AND CURRENT ROW
    )
 AS alpha002,
        
        -- Alpha 003: (-1 * correlation(rank(open), rank(volume), 10))
        -1 * corr_open_volume_10 AS alpha003,
        
        -- Alpha 004: (-1 * Ts_Rank(rank(low), 9))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY low_rank
        ROWS BETWEEN 8 PRECEDING AND CURRENT ROW
    )
 AS alpha004,
        
        -- Alpha 005: (rank((open - (sum(vwap, 10) / 10))) * (-1 * abs(rank((close - vwap)))))
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY open - alpha005_mean_vwap
    )
 * (-1 * 
    ABS(
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY close - vwap
    )
)
) AS alpha005,
        
        -- Alpha 006: (-1 * correlation(open, volume, 10))
        -1 * corr_open_volume_10 AS alpha006,
        
        -- Alpha 007: ((adv20 < volume) ? ((-1 * ts_rank(abs(delta(close, 7)), 60)) * sign(delta(close, 7))) : (-1))
        CASE 
            WHEN adv20 < volume THEN (-1 * alpha007_ts_rank) * alpha007_sign
            ELSE -1
        END AS alpha007,
        
        -- Alpha 008: (-1 * rank(((sum(open, 5) * sum(returns, 5)) - delay((sum(open, 5) * sum(returns, 5)), 10))))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha008_sum_open_returns - alpha008_delay_sum
    )
 AS alpha008,
        
        -- Alpha 009: ((0 < ts_min(delta(close, 1), 5)) ? delta(close, 1) : ((ts_max(delta(close, 1), 5) < 0) ? delta(close, 1) : (-1 * delta(close, 1))))
        alpha009_logic AS alpha009,
        
        -- Alpha 010: rank(((0 < ts_min(delta(close, 1), 4)) ? delta(close, 1) : ((ts_max(delta(close, 1), 4) < 0) ? delta(close, 1) : (-1 * delta(close, 1)))))
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha009_logic
    )
 AS alpha010,
        
        -- Alpha 011: ((rank(ts_max((vwap - close), 3)) + rank(ts_min((vwap - close), 3))) * rank(delta(volume, 3)))
        (
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha011_max_vwap_close
    )
 + 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha011_min_vwap_close
    )
) * 
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY volume_delta3
    )
 AS alpha011,
        
        -- Alpha 012: (sign(delta(volume, 1)) * (-1 * delta(close, 1)))
        
    CASE 
        WHEN volume_delta1 > 0 THEN 1
        WHEN volume_delta1 < 0 THEN -1
        ELSE 0
    END
 * (-1 * close_delta1) AS alpha012,
        
        -- Alpha 013: (-1 * rank(covariance(rank(close), rank(volume), 5)))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY cov_close_volume_5
    )
 AS alpha013,
        
        -- Alpha 014: ((-1 * rank(delta(returns, 3))) * correlation(open, volume, 10))
        (-1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha014_delta_returns
    )
) * corr_open_volume_10 AS alpha014,
        
        -- Alpha 015: (-1 * sum(rank(correlation(rank(high), rank(volume), 3)), 3))
        -1 * 
    SUM(
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha015_corr_high_vol
    )
) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha015,
        
        -- Alpha 016: (-1 * rank(covariance(rank(high), rank(volume), 5)))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY cov_high_volume_5
    )
 AS alpha016,
        
        -- Alpha 017: (((-1 * rank(ts_rank(close, 10))) * rank(delta(delta(close, 1), 1))) * rank(ts_rank((volume / adv20), 5)))
        ((-1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY close_ts_rank10
    )
) * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha017_delta_delta_close
    )
) * 
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha017_ts_rank_vol_adv
    )
 AS alpha017,
        
        -- Alpha 018: (-1 * rank(((stddev(abs((close - open)), 5) + (close - open)) + correlation(close, open, 10))))
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha018_stddev + alpha018_close_open_diff + alpha018_corr_close_open
    )
 AS alpha018,
        
        -- Alpha 019: ((-1 * sign(((close - delay(close, 7)) + delta(close, 7)))) * (1 + rank((1 + sum(returns, 250)))))
        (-1 * 
    CASE 
        WHEN alpha019_close_diff_plus_delta > 0 THEN 1
        WHEN alpha019_close_diff_plus_delta < 0 THEN -1
        ELSE 0
    END
) * 
        (1 + 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha019_sum_returns
    )
) AS alpha019,
        
        -- Alpha 020: (((-1 * rank((open - delay(high, 1)))) * rank((open - delay(close, 1)))) * rank((open - delay(low, 1))))
        ((-1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha020_open_delay_high
    )
) * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha020_open_delay_close
    )
) * 
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY alpha020_open_delay_low
    )
 AS alpha020
        
    FROM intermediate_calcs
)

SELECT * FROM alpha_factors
    );
  
  
[0m15:37:38.585435 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m15:37:38.585700 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_001_020: ROLLBACK
[0m15:37:38.588816 [debug] [Thread-1 (]: Failed to rollback 'model.quant_features.alpha_factors_001_020'
[0m15:37:38.589087 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_001_020: Close
[0m15:37:38.591395 [debug] [Thread-1 (]: Runtime Error in model alpha_factors_001_020 (models/alpha101/alpha_factors_001_020.sql)
  Parser Error: window functions are not allowed in window definitions
[0m15:37:38.592438 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2a361f1d-3ef5-4ebb-a35e-9b6fa2592538', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f631e9c1850>]}
[0m15:37:38.592913 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model main.alpha_factors_001_020 ............... [[31mERROR[0m in 0.05s]
[0m15:37:38.593318 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_factors_001_020
[0m15:37:38.593891 [debug] [Thread-7 (]: Marking all children of 'model.quant_features.alpha_factors_001_020' to be skipped because of status 'error'.  Reason: Runtime Error in model alpha_factors_001_020 (models/alpha101/alpha_factors_001_020.sql)
  Parser Error: window functions are not allowed in window definitions.
[0m15:37:38.595750 [debug] [MainThread]: Using duckdb connection "master"
[0m15:37:38.595998 [debug] [MainThread]: On master: BEGIN
[0m15:37:38.596164 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:37:38.596578 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:37:38.596774 [debug] [MainThread]: On master: COMMIT
[0m15:37:38.596944 [debug] [MainThread]: Using duckdb connection "master"
[0m15:37:38.597097 [debug] [MainThread]: On master: COMMIT
[0m15:37:38.597386 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:37:38.597578 [debug] [MainThread]: On master: Close
[0m15:37:38.597851 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:37:38.598016 [debug] [MainThread]: Connection 'model.quant_features.alpha_factors_001_020' was properly closed.
[0m15:37:38.598196 [info ] [MainThread]: 
[0m15:37:38.598382 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m15:37:38.598751 [debug] [MainThread]: Command end result
[0m15:37:38.619223 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:37:38.620417 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:37:38.624160 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:37:38.624484 [info ] [MainThread]: 
[0m15:37:38.624713 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:37:38.624887 [info ] [MainThread]: 
[0m15:37:38.625086 [error] [MainThread]: [31mFailure in model alpha_factors_001_020 (models/alpha101/alpha_factors_001_020.sql)[0m
[0m15:37:38.625282 [error] [MainThread]:   Runtime Error in model alpha_factors_001_020 (models/alpha101/alpha_factors_001_020.sql)
  Parser Error: window functions are not allowed in window definitions
[0m15:37:38.625424 [info ] [MainThread]: 
[0m15:37:38.625595 [info ] [MainThread]:   compiled code at target/compiled/quant_features/models/alpha101/alpha_factors_001_020.sql
[0m15:37:38.625733 [info ] [MainThread]: 
[0m15:37:38.625896 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m15:37:38.626436 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.64256084, "process_in_blocks": "0", "process_kernel_time": 0.135423, "process_mem_max_rss": "151500", "process_out_blocks": "3736", "process_user_time": 1.354282}
[0m15:37:38.626704 [debug] [MainThread]: Command `dbt run` failed at 15:37:38.626650 after 0.64 seconds
[0m15:37:38.626903 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f631f35fa10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6323802670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6323802490>]}
[0m15:37:38.627111 [debug] [MainThread]: Flushing usage events
[0m15:37:38.671973 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:38:15.480029 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcd28c53770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcd2a29da90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcd27ac3d90>]}


============================== 15:38:15.482351 | 105ba326-3f3b-4365-b0b0-8d79f73cae5f ==============================
[0m15:38:15.482351 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:38:15.482686 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True', 'warn_error': 'None', 'log_format': 'default', 'invocation_command': 'dbt run --select alpha_factors_simple_001_020', 'partial_parse': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'cache_selected_only': 'False', 'no_print': 'None', 'use_colors': 'True', 'fail_fast': 'False', 'profiles_dir': '/workspace/dbt_project', 'write_json': 'True', 'debug': 'False', 'log_path': '/workspace/dbt_project/logs', 'version_check': 'True', 'use_experimental_parser': 'False', 'log_cache_events': 'False', 'printer_width': '80', 'empty': 'False', 'indirect_selection': 'eager', 'quiet': 'False'}
[0m15:38:15.627436 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '105ba326-3f3b-4365-b0b0-8d79f73cae5f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcd28943360>]}
[0m15:38:15.669834 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '105ba326-3f3b-4365-b0b0-8d79f73cae5f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcd27bb6be0>]}
[0m15:38:15.670828 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:38:15.702275 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:38:15.791798 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m15:38:15.792209 [debug] [MainThread]: Partial parsing: added file: quant_features://models/alpha101/alpha_factors_simple_001_020.sql
[0m15:38:15.991445 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '105ba326-3f3b-4365-b0b0-8d79f73cae5f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcd23b19a50>]}
[0m15:38:16.088974 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:38:16.090184 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:38:16.100072 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '105ba326-3f3b-4365-b0b0-8d79f73cae5f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcd23b48c80>]}
[0m15:38:16.100416 [info ] [MainThread]: Found 18 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:38:16.100625 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '105ba326-3f3b-4365-b0b0-8d79f73cae5f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcd23ad9b70>]}
[0m15:38:16.101849 [info ] [MainThread]: 
[0m15:38:16.102091 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:38:16.102264 [info ] [MainThread]: 
[0m15:38:16.102530 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:38:16.103418 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features'
[0m15:38:16.117836 [debug] [ThreadPool]: Using duckdb connection "list_quant_features"
[0m15:38:16.118093 [debug] [ThreadPool]: On list_quant_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"quant_features"'
    
  
  
[0m15:38:16.118284 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:38:16.130387 [debug] [ThreadPool]: SQL status: OK in 0.012 seconds
[0m15:38:16.131260 [debug] [ThreadPool]: On list_quant_features: Close
[0m15:38:16.131928 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_quant_features, now create_quant_features_main)
[0m15:38:16.132232 [debug] [ThreadPool]: Creating schema "database: "quant_features"
schema: "main"
"
[0m15:38:16.136282 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:38:16.136550 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='quant_features'
        and type='sqlite'
    
  
[0m15:38:16.136722 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:38:16.137370 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:38:16.138190 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:38:16.138398 [debug] [ThreadPool]: On create_quant_features_main: BEGIN
[0m15:38:16.138692 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:38:16.138852 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:38:16.138998 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
    
        create schema if not exists "quant_features"."main"
    
[0m15:38:16.139324 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:38:16.139862 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:38:16.140057 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:38:16.140229 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:38:16.140530 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:38:16.140712 [debug] [ThreadPool]: On create_quant_features_main: Close
[0m15:38:16.145229 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_quant_features_main, now list_quant_features_main)
[0m15:38:16.148855 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:38:16.149107 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:38:16.149281 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:38:16.149686 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:38:16.149871 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:38:16.150028 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:38:16.156412 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m15:38:16.157342 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:38:16.157926 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:38:16.158136 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:38:16.159957 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '105ba326-3f3b-4365-b0b0-8d79f73cae5f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcd2238fa00>]}
[0m15:38:16.160389 [debug] [MainThread]: Using duckdb connection "master"
[0m15:38:16.160587 [debug] [MainThread]: On master: BEGIN
[0m15:38:16.160739 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:38:16.161099 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:38:16.161296 [debug] [MainThread]: On master: COMMIT
[0m15:38:16.161447 [debug] [MainThread]: Using duckdb connection "master"
[0m15:38:16.161589 [debug] [MainThread]: On master: COMMIT
[0m15:38:16.161835 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:38:16.162009 [debug] [MainThread]: On master: Close
[0m15:38:16.164199 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_factors_simple_001_020
[0m15:38:16.164611 [info ] [Thread-1 (]: 1 of 1 START sql table model main.alpha_factors_simple_001_020 ................. [RUN]
[0m15:38:16.164900 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.alpha_factors_simple_001_020)
[0m15:38:16.165101 [debug] [Thread-1 (]: Began compiling node model.quant_features.alpha_factors_simple_001_020
[0m15:38:16.175106 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.alpha_factors_simple_001_020"
[0m15:38:16.175568 [debug] [Thread-1 (]: Began executing node model.quant_features.alpha_factors_simple_001_020
[0m15:38:16.195545 [debug] [Thread-1 (]: Writing runtime sql for node "model.quant_features.alpha_factors_simple_001_020"
[0m15:38:16.196015 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_simple_001_020"
[0m15:38:16.196250 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_simple_001_020: BEGIN
[0m15:38:16.196454 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:38:16.196942 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:38:16.197203 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_simple_001_020"
[0m15:38:16.197554 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_simple_001_020: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_factors_simple_001_020"} */

  
    
    

    create  table
      "quant_features"."main"."alpha_factors_simple_001_020__dbt_tmp"
  
    as (
      

-- 简化版 Alpha 101 因子计算 (001-020)
-- 避免复杂的嵌套窗口函数

WITH base_data AS (
    SELECT * FROM "quant_features"."main"."alpha_base_data"
),

alpha_factors AS (
    SELECT 
        symbol,
        timestamp,
        
        -- 基础数据
        open, high, low, close, volume, vwap, returns,
        
        -- Alpha 001 简化版: 基于收益率和波动率的排序
        PERCENT_RANK() OVER (
            PARTITION BY timestamp 
            ORDER BY CASE WHEN returns < 0 THEN returns_std20 ELSE close END
        ) - 0.5 AS alpha001,
        
        -- Alpha 002 简化版: 成交量变化和收益率的相关性
        
    CORR(volume_delta1, returns) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 5 PRECEDING AND CURRENT ROW
    )
 * -1 AS alpha002,
        
        -- Alpha 003 简化版: 收盘价相关性
        
    CORR(close, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 * -1 AS alpha003,
        
        -- Alpha 004 简化版: 成交量趋势
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY volume
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 * -1 AS alpha004,
        
        -- Alpha 005 简化版: VWAP相对强度
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY open - vwap
    )
 * (-1 * 
    ABS(
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY close - vwap
    )
)
) AS alpha005,
        
        -- Alpha 006 简化版: 开盘价相关性
        
    CORR(open, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 * -1 AS alpha006,
        
        -- Alpha 007 简化版: 价格变化排序
        CASE 
            WHEN adv20 < volume THEN (-1 * 
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY 
    ABS(close_delta7)

        ROWS BETWEEN 59 PRECEDING AND CURRENT ROW
    )
) * 
    CASE 
        WHEN close_delta7 > 0 THEN 1
        WHEN close_delta7 < 0 THEN -1
        ELSE 0
    END

            ELSE -1
        END AS alpha007,
        
        -- Alpha 008 简化版: 开盘价和收益率的组合
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY (open * 0.35 + returns * 0.65)
    )
 AS alpha008,
        
        -- Alpha 009 简化版: 价格变化逻辑
        CASE 
            WHEN 
    MIN(close_delta1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 > 0 THEN close_delta1
            WHEN 
    MAX(close_delta1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 < 0 THEN close_delta1
            ELSE -1 * close_delta1
        END AS alpha009,
        
        -- Alpha 010 简化版: 收盘价排序
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY close_delta1
    )
 AS alpha010,
        
        -- Alpha 011 简化版: VWAP和收盘价差异
        
    MAX(vwap - close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha011,
        
        -- Alpha 012 简化版: 成交量和价格变化
        
    CASE 
        WHEN volume_delta1 > 0 THEN 1
        WHEN volume_delta1 < 0 THEN -1
        ELSE 0
    END
 * (-1 * close_delta1) AS alpha012,
        
        -- Alpha 013 简化版: 收盘价和成交量相关性
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY 
    CORR(close, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )

    )
 AS alpha013,
        
        -- Alpha 014 简化版: 收益率变化
        (-1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY returns_delta3
    )
) * 
    CORR(open, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS alpha014,
        
        -- Alpha 015 简化版: 高价和成交量相关性
        -1 * 
    SUM(
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY 
    CORR(high_rank, volume_rank) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )

    )
) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha015,
        
        -- Alpha 016 简化版: 高价排序
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY high
    )
 AS alpha016,
        
        -- Alpha 017 简化版: 成交量相对强度
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY vwap - close
    )
 AS alpha017,
        
        -- Alpha 018 简化版: 收盘价开盘价差异标准差
        
    STDDEV(
    ABS(close - open)
) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 / close AS alpha018,
        
        -- Alpha 019 简化版: 收盘价变化和收益率
        (-1 * 
    CASE 
        WHEN close_delta7 + close_delta7 > 0 THEN 1
        WHEN close_delta7 + close_delta7 < 0 THEN -1
        ELSE 0
    END
) * 
        (1 + 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY 1 + returns_sum250
    )
) AS alpha019,
        
        -- Alpha 020 简化版: 开盘价相对强度
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY open - close_lag1
    )
 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY open - close
    )
 AS alpha020
        
    FROM base_data
    WHERE close_ma20 IS NOT NULL
      AND returns_std20 IS NOT NULL
      AND adv20 IS NOT NULL
)

SELECT * FROM alpha_factors
    );
  
  
[0m15:38:16.198511 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_factors_simple_001_020"} */

  
    
    

    create  table
      "quant_features"."main"."alpha_factors_simple_001_020__dbt_tmp"
  
    as (
      

-- 简化版 Alpha 101 因子计算 (001-020)
-- 避免复杂的嵌套窗口函数

WITH base_data AS (
    SELECT * FROM "quant_features"."main"."alpha_base_data"
),

alpha_factors AS (
    SELECT 
        symbol,
        timestamp,
        
        -- 基础数据
        open, high, low, close, volume, vwap, returns,
        
        -- Alpha 001 简化版: 基于收益率和波动率的排序
        PERCENT_RANK() OVER (
            PARTITION BY timestamp 
            ORDER BY CASE WHEN returns < 0 THEN returns_std20 ELSE close END
        ) - 0.5 AS alpha001,
        
        -- Alpha 002 简化版: 成交量变化和收益率的相关性
        
    CORR(volume_delta1, returns) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 5 PRECEDING AND CURRENT ROW
    )
 * -1 AS alpha002,
        
        -- Alpha 003 简化版: 收盘价相关性
        
    CORR(close, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 * -1 AS alpha003,
        
        -- Alpha 004 简化版: 成交量趋势
        
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY volume
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 * -1 AS alpha004,
        
        -- Alpha 005 简化版: VWAP相对强度
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY open - vwap
    )
 * (-1 * 
    ABS(
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY close - vwap
    )
)
) AS alpha005,
        
        -- Alpha 006 简化版: 开盘价相关性
        
    CORR(open, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 * -1 AS alpha006,
        
        -- Alpha 007 简化版: 价格变化排序
        CASE 
            WHEN adv20 < volume THEN (-1 * 
    PERCENT_RANK() OVER (
        PARTITION BY symbol
        ORDER BY 
    ABS(close_delta7)

        ROWS BETWEEN 59 PRECEDING AND CURRENT ROW
    )
) * 
    CASE 
        WHEN close_delta7 > 0 THEN 1
        WHEN close_delta7 < 0 THEN -1
        ELSE 0
    END

            ELSE -1
        END AS alpha007,
        
        -- Alpha 008 简化版: 开盘价和收益率的组合
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY (open * 0.35 + returns * 0.65)
    )
 AS alpha008,
        
        -- Alpha 009 简化版: 价格变化逻辑
        CASE 
            WHEN 
    MIN(close_delta1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 > 0 THEN close_delta1
            WHEN 
    MAX(close_delta1) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 < 0 THEN close_delta1
            ELSE -1 * close_delta1
        END AS alpha009,
        
        -- Alpha 010 简化版: 收盘价排序
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY close_delta1
    )
 AS alpha010,
        
        -- Alpha 011 简化版: VWAP和收盘价差异
        
    MAX(vwap - close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha011,
        
        -- Alpha 012 简化版: 成交量和价格变化
        
    CASE 
        WHEN volume_delta1 > 0 THEN 1
        WHEN volume_delta1 < 0 THEN -1
        ELSE 0
    END
 * (-1 * close_delta1) AS alpha012,
        
        -- Alpha 013 简化版: 收盘价和成交量相关性
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY 
    CORR(close, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )

    )
 AS alpha013,
        
        -- Alpha 014 简化版: 收益率变化
        (-1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY returns_delta3
    )
) * 
    CORR(open, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    )
 AS alpha014,
        
        -- Alpha 015 简化版: 高价和成交量相关性
        -1 * 
    SUM(
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY 
    CORR(high_rank, volume_rank) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )

    )
) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    )
 AS alpha015,
        
        -- Alpha 016 简化版: 高价排序
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY high
    )
 AS alpha016,
        
        -- Alpha 017 简化版: 成交量相对强度
        
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY vwap - close
    )
 AS alpha017,
        
        -- Alpha 018 简化版: 收盘价开盘价差异标准差
        
    STDDEV(
    ABS(close - open)
) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 / close AS alpha018,
        
        -- Alpha 019 简化版: 收盘价变化和收益率
        (-1 * 
    CASE 
        WHEN close_delta7 + close_delta7 > 0 THEN 1
        WHEN close_delta7 + close_delta7 < 0 THEN -1
        ELSE 0
    END
) * 
        (1 + 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY 1 + returns_sum250
    )
) AS alpha019,
        
        -- Alpha 020 简化版: 开盘价相对强度
        -1 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY open - close_lag1
    )
 * 
    PERCENT_RANK() OVER (
        PARTITION BY timestamp
        ORDER BY open - close
    )
 AS alpha020
        
    FROM base_data
    WHERE close_ma20 IS NOT NULL
      AND returns_std20 IS NOT NULL
      AND adv20 IS NOT NULL
)

SELECT * FROM alpha_factors
    );
  
  
[0m15:38:16.198958 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m15:38:16.199273 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_simple_001_020: ROLLBACK
[0m15:38:16.202353 [debug] [Thread-1 (]: Failed to rollback 'model.quant_features.alpha_factors_simple_001_020'
[0m15:38:16.202624 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_simple_001_020: Close
[0m15:38:16.204957 [debug] [Thread-1 (]: Runtime Error in model alpha_factors_simple_001_020 (models/alpha101/alpha_factors_simple_001_020.sql)
  Parser Error: window functions are not allowed in window definitions
[0m15:38:16.205970 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '105ba326-3f3b-4365-b0b0-8d79f73cae5f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcd223e2690>]}
[0m15:38:16.206476 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model main.alpha_factors_simple_001_020 ........ [[31mERROR[0m in 0.04s]
[0m15:38:16.206870 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_factors_simple_001_020
[0m15:38:16.207574 [debug] [Thread-7 (]: Marking all children of 'model.quant_features.alpha_factors_simple_001_020' to be skipped because of status 'error'.  Reason: Runtime Error in model alpha_factors_simple_001_020 (models/alpha101/alpha_factors_simple_001_020.sql)
  Parser Error: window functions are not allowed in window definitions.
[0m15:38:16.209773 [debug] [MainThread]: Using duckdb connection "master"
[0m15:38:16.210016 [debug] [MainThread]: On master: BEGIN
[0m15:38:16.210187 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:38:16.210567 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:38:16.210754 [debug] [MainThread]: On master: COMMIT
[0m15:38:16.210914 [debug] [MainThread]: Using duckdb connection "master"
[0m15:38:16.211061 [debug] [MainThread]: On master: COMMIT
[0m15:38:16.211328 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:38:16.211516 [debug] [MainThread]: On master: Close
[0m15:38:16.211778 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:38:16.211941 [debug] [MainThread]: Connection 'model.quant_features.alpha_factors_simple_001_020' was properly closed.
[0m15:38:16.212108 [info ] [MainThread]: 
[0m15:38:16.212301 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.11 seconds (0.11s).
[0m15:38:16.212705 [debug] [MainThread]: Command end result
[0m15:38:16.232101 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:38:16.233301 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:38:16.237034 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:38:16.237307 [info ] [MainThread]: 
[0m15:38:16.237524 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:38:16.237695 [info ] [MainThread]: 
[0m15:38:16.237901 [error] [MainThread]: [31mFailure in model alpha_factors_simple_001_020 (models/alpha101/alpha_factors_simple_001_020.sql)[0m
[0m15:38:16.238099 [error] [MainThread]:   Runtime Error in model alpha_factors_simple_001_020 (models/alpha101/alpha_factors_simple_001_020.sql)
  Parser Error: window functions are not allowed in window definitions
[0m15:38:16.238258 [info ] [MainThread]: 
[0m15:38:16.238456 [info ] [MainThread]:   compiled code at target/compiled/quant_features/models/alpha101/alpha_factors_simple_001_020.sql
[0m15:38:16.238610 [info ] [MainThread]: 
[0m15:38:16.238809 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m15:38:16.239384 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.79543877, "process_in_blocks": "0", "process_kernel_time": 0.120587, "process_mem_max_rss": "153740", "process_out_blocks": "5304", "process_user_time": 1.555638}
[0m15:38:16.239653 [debug] [MainThread]: Command `dbt run` failed at 15:38:16.239598 after 0.80 seconds
[0m15:38:16.239857 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcd2353fcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcd2a3b3930>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcd23519450>]}
[0m15:38:16.240072 [debug] [MainThread]: Flushing usage events
[0m15:38:16.288650 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:38:51.547361 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f4e403770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f4fa35a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f4d26fd90>]}


============================== 15:38:51.549973 | b32c8452-1d2e-40a5-8417-3e0354c1440a ==============================
[0m15:38:51.549973 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:38:51.550328 [debug] [MainThread]: running dbt with arguments {'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'log_path': '/workspace/dbt_project/logs', 'send_anonymous_usage_stats': 'True', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'warn_error': 'None', 'printer_width': '80', 'introspect': 'True', 'no_print': 'None', 'use_experimental_parser': 'False', 'fail_fast': 'False', 'target_path': 'None', 'cache_selected_only': 'False', 'write_json': 'True', 'empty': 'False', 'invocation_command': 'dbt run --select alpha_factors_basic', 'partial_parse': 'True', 'static_parser': 'True', 'use_colors': 'True', 'quiet': 'False', 'profiles_dir': '/workspace/dbt_project', 'log_format': 'default'}
[0m15:38:51.696942 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b32c8452-1d2e-40a5-8417-3e0354c1440a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f4e0eb360>]}
[0m15:38:51.739003 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b32c8452-1d2e-40a5-8417-3e0354c1440a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f4d362be0>]}
[0m15:38:51.740183 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:38:51.774302 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:38:51.865466 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m15:38:51.865928 [debug] [MainThread]: Partial parsing: added file: quant_features://models/alpha101/alpha_factors_basic.sql
[0m15:38:51.866721 [debug] [MainThread]: Partial parsing: updated file: quant_features://macros/alpha101/base_operators.sql
[0m15:38:52.299600 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `dbt_utils.accepted_range`. Arguments to
generic tests should be nested under the `arguments` property.`
[0m15:38:52.300059 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'b32c8452-1d2e-40a5-8417-3e0354c1440a', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f48e55950>]}
[0m15:38:52.412840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b32c8452-1d2e-40a5-8417-3e0354c1440a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f48f7b890>]}
[0m15:38:52.480495 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:38:52.481955 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:38:52.494623 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b32c8452-1d2e-40a5-8417-3e0354c1440a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f48442dd0>]}
[0m15:38:52.494988 [info ] [MainThread]: Found 19 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:38:52.495222 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b32c8452-1d2e-40a5-8417-3e0354c1440a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f48326f70>]}
[0m15:38:52.496608 [info ] [MainThread]: 
[0m15:38:52.496891 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:38:52.497075 [info ] [MainThread]: 
[0m15:38:52.504888 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:38:52.506418 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features'
[0m15:38:52.522811 [debug] [ThreadPool]: Using duckdb connection "list_quant_features"
[0m15:38:52.523092 [debug] [ThreadPool]: On list_quant_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"quant_features"'
    
  
  
[0m15:38:52.523295 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:38:52.537547 [debug] [ThreadPool]: SQL status: OK in 0.014 seconds
[0m15:38:52.538572 [debug] [ThreadPool]: On list_quant_features: Close
[0m15:38:52.539296 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_quant_features, now create_quant_features_main)
[0m15:38:52.539629 [debug] [ThreadPool]: Creating schema "database: "quant_features"
schema: "main"
"
[0m15:38:52.543794 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:38:52.544059 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='quant_features'
        and type='sqlite'
    
  
[0m15:38:52.544248 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:38:52.544961 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:38:52.545843 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:38:52.546079 [debug] [ThreadPool]: On create_quant_features_main: BEGIN
[0m15:38:52.546402 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:38:52.546578 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:38:52.546737 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
    
        create schema if not exists "quant_features"."main"
    
[0m15:38:52.547058 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:38:52.547620 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:38:52.547809 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:38:52.547962 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:38:52.548250 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:38:52.548462 [debug] [ThreadPool]: On create_quant_features_main: Close
[0m15:38:52.553204 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_quant_features_main, now list_quant_features_main)
[0m15:38:52.557155 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:38:52.557414 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:38:52.557576 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:38:52.557956 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:38:52.558142 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:38:52.558315 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:38:52.564885 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m15:38:52.565863 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:38:52.566687 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:38:52.566905 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:38:52.568949 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b32c8452-1d2e-40a5-8417-3e0354c1440a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f48148b90>]}
[0m15:38:52.569367 [debug] [MainThread]: Using duckdb connection "master"
[0m15:38:52.569564 [debug] [MainThread]: On master: BEGIN
[0m15:38:52.569718 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:38:52.570164 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:38:52.570369 [debug] [MainThread]: On master: COMMIT
[0m15:38:52.570527 [debug] [MainThread]: Using duckdb connection "master"
[0m15:38:52.570674 [debug] [MainThread]: On master: COMMIT
[0m15:38:52.570925 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:38:52.571111 [debug] [MainThread]: On master: Close
[0m15:38:52.575090 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_factors_basic
[0m15:38:52.575536 [info ] [Thread-1 (]: 1 of 1 START sql table model main.alpha_factors_basic .......................... [RUN]
[0m15:38:52.575868 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.alpha_factors_basic)
[0m15:38:52.576087 [debug] [Thread-1 (]: Began compiling node model.quant_features.alpha_factors_basic
[0m15:38:52.580991 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.alpha_factors_basic"
[0m15:38:52.581477 [debug] [Thread-1 (]: Began executing node model.quant_features.alpha_factors_basic
[0m15:38:52.602160 [debug] [Thread-1 (]: Writing runtime sql for node "model.quant_features.alpha_factors_basic"
[0m15:38:52.602639 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_basic"
[0m15:38:52.602864 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_basic: BEGIN
[0m15:38:52.603050 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:38:52.603623 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:38:52.603871 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_basic"
[0m15:38:52.604143 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_basic: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_factors_basic"} */

  
    
    

    create  table
      "quant_features"."main"."alpha_factors_basic__dbt_tmp"
  
    as (
      

-- 基础版 Alpha 101 因子计算
-- 避免复杂的嵌套窗口函数，使用最简单的实现

WITH base_data AS (
    SELECT * FROM "quant_features"."main"."alpha_base_data"
),

alpha_factors AS (
    SELECT 
        symbol,
        timestamp,
        
        -- 基础数据
        open, high, low, close, volume, vwap, returns,
        
        -- Alpha 001: 基于收益率的简单排序
        PERCENT_RANK() OVER (
            PARTITION BY timestamp 
            ORDER BY returns
        ) AS alpha001,
        
        -- Alpha 002: 成交量变化排序  
        PERCENT_RANK() OVER (
            PARTITION BY timestamp 
            ORDER BY volume_delta1
        ) AS alpha002,
        
        -- Alpha 003: 收盘价和成交量的简单比率
        close / NULLIF(volume, 0) AS alpha003,
        
        -- Alpha 004: 成交量相对强度
        volume / NULLIF(adv20, 0) AS alpha004,
        
        -- Alpha 005: 开盘价相对于VWAP的强度
        (open - vwap) / NULLIF(vwap, 0) AS alpha005,
        
        -- Alpha 006: 开盘价和成交量的关系
        open * volume AS alpha006,
        
        -- Alpha 007: 价格变化幅度
        ABS(close_delta7) AS alpha007,
        
        -- Alpha 008: 开盘价动量
        (open - close_lag1) / NULLIF(close_lag1, 0) AS alpha008,
        
        -- Alpha 009: 价格变化方向
        CASE 
            WHEN close_delta1 > 0 THEN 1
            WHEN close_delta1 < 0 THEN -1
            ELSE 0
        END AS alpha009,
        
        -- Alpha 010: 收盘价变化
        close_delta1 AS alpha010,
        
        -- Alpha 011: VWAP和收盘价差异
        vwap - close AS alpha011,
        
        -- Alpha 012: 成交量和价格变化的符号关系
        CASE 
            WHEN volume_delta1 > 0 AND close_delta1 > 0 THEN 1
            WHEN volume_delta1 < 0 AND close_delta1 < 0 THEN 1
            WHEN volume_delta1 > 0 AND close_delta1 < 0 THEN -1
            WHEN volume_delta1 < 0 AND close_delta1 > 0 THEN -1
            ELSE 0
        END AS alpha012,
        
        -- Alpha 013: 收盘价排序
        PERCENT_RANK() OVER (
            PARTITION BY timestamp 
            ORDER BY close
        ) AS alpha013,
        
        -- Alpha 014: 收益率相对强度
        returns / NULLIF(returns_std20, 0) AS alpha014,
        
        -- Alpha 015: 高价排序
        PERCENT_RANK() OVER (
            PARTITION BY timestamp 
            ORDER BY high
        ) AS alpha015,
        
        -- Alpha 016: 高价的负值
        -1 * high AS alpha016,
        
        -- Alpha 017: VWAP相对强度
        (vwap - close) / NULLIF(close, 0) AS alpha017,
        
        -- Alpha 018: 价格波动率
        ABS(close - open) / NULLIF(close, 0) AS alpha018,
        
        -- Alpha 019: 收盘价长期变化
        (close - close_lag5) / NULLIF(close_lag5, 0) AS alpha019,
        
        -- Alpha 020: 开盘价相对强度
        (open - close_lag1) / NULLIF(close_lag1, 0) * 
        (open - close) / NULLIF(close, 0) AS alpha020
        
    FROM base_data
    WHERE close_ma20 IS NOT NULL
      AND returns_std20 IS NOT NULL
      AND adv20 IS NOT NULL
)

SELECT * FROM alpha_factors
    );
  
  
[0m15:38:52.614258 [debug] [Thread-1 (]: SQL status: OK in 0.010 seconds
[0m15:38:52.618333 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_basic"
[0m15:38:52.618614 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_basic: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_factors_basic"} */
alter table "quant_features"."main"."alpha_factors_basic__dbt_tmp" rename to "alpha_factors_basic"
[0m15:38:52.619136 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:38:52.627645 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_basic: COMMIT
[0m15:38:52.627932 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_basic"
[0m15:38:52.628129 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_basic: COMMIT
[0m15:38:52.630866 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:38:52.634335 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_basic"
[0m15:38:52.634609 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_basic: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_factors_basic"} */

      drop table if exists "quant_features"."main"."alpha_factors_basic__dbt_backup" cascade
    
[0m15:38:52.635068 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:38:52.636691 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_basic: Close
[0m15:38:52.638421 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b32c8452-1d2e-40a5-8417-3e0354c1440a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f484c7070>]}
[0m15:38:52.638891 [info ] [Thread-1 (]: 1 of 1 OK created sql table model main.alpha_factors_basic ..................... [[32mOK[0m in 0.06s]
[0m15:38:52.639263 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_factors_basic
[0m15:38:52.640617 [debug] [MainThread]: Using duckdb connection "master"
[0m15:38:52.640930 [debug] [MainThread]: On master: BEGIN
[0m15:38:52.641098 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:38:52.641497 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:38:52.641692 [debug] [MainThread]: On master: COMMIT
[0m15:38:52.641863 [debug] [MainThread]: Using duckdb connection "master"
[0m15:38:52.642018 [debug] [MainThread]: On master: COMMIT
[0m15:38:52.642356 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:38:52.642529 [debug] [MainThread]: On master: Close
[0m15:38:52.642786 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:38:52.642941 [debug] [MainThread]: Connection 'model.quant_features.alpha_factors_basic' was properly closed.
[0m15:38:52.643119 [info ] [MainThread]: 
[0m15:38:52.643321 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.14 seconds (0.14s).
[0m15:38:52.643717 [debug] [MainThread]: Command end result
[0m15:38:52.667703 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:38:52.669075 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:38:52.672630 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:38:52.672853 [info ] [MainThread]: 
[0m15:38:52.673084 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:38:52.673256 [info ] [MainThread]: 
[0m15:38:52.673440 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m15:38:52.673780 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 11 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m15:38:52.674452 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 1.1635846, "process_in_blocks": "0", "process_kernel_time": 0.176069, "process_mem_max_rss": "160388", "process_out_blocks": "5272", "process_user_time": 1.868719}
[0m15:38:52.674803 [debug] [MainThread]: Command `dbt run` succeeded at 15:38:52.674738 after 1.16 seconds
[0m15:38:52.675053 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f48d4ee90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f48359d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2f49101d50>]}
[0m15:38:52.675282 [debug] [MainThread]: Flushing usage events
[0m15:38:52.704138 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:39:16.632079 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd615d17770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd61734da90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd614b87d90>]}


============================== 15:39:16.634601 | 5db364f2-c4ac-4826-94c9-e2c8d7595bad ==============================
[0m15:39:16.634601 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:39:16.634924 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'static_parser': 'True', 'empty': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'introspect': 'True', 'printer_width': '80', 'log_path': '/workspace/dbt_project/logs', 'debug': 'False', 'invocation_command': 'dbt run --select alpha_factors_021_040', 'log_cache_events': 'False', 'use_colors': 'True', 'cache_selected_only': 'False', 'version_check': 'True', 'warn_error': 'None', 'profiles_dir': '/workspace/dbt_project', 'indirect_selection': 'eager', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'log_format': 'default', 'target_path': 'None'}
[0m15:39:16.781870 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5db364f2-c4ac-4826-94c9-e2c8d7595bad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd615a03360>]}
[0m15:39:16.824341 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5db364f2-c4ac-4826-94c9-e2c8d7595bad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd614c76be0>]}
[0m15:39:16.825422 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:39:16.857275 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:39:16.947146 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m15:39:16.947572 [debug] [MainThread]: Partial parsing: added file: quant_features://models/alpha101/alpha_factors_021_040.sql
[0m15:39:17.133540 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5db364f2-c4ac-4826-94c9-e2c8d7595bad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd610755950>]}
[0m15:39:17.236429 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:39:17.237651 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:39:17.248262 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5db364f2-c4ac-4826-94c9-e2c8d7595bad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd6114c8c80>]}
[0m15:39:17.248610 [info ] [MainThread]: Found 20 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:39:17.248835 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5db364f2-c4ac-4826-94c9-e2c8d7595bad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd610bd9d30>]}
[0m15:39:17.250081 [info ] [MainThread]: 
[0m15:39:17.250351 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:39:17.250520 [info ] [MainThread]: 
[0m15:39:17.250794 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:39:17.251582 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features'
[0m15:39:17.266307 [debug] [ThreadPool]: Using duckdb connection "list_quant_features"
[0m15:39:17.266571 [debug] [ThreadPool]: On list_quant_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"quant_features"'
    
  
  
[0m15:39:17.266753 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:39:17.280054 [debug] [ThreadPool]: SQL status: OK in 0.013 seconds
[0m15:39:17.280956 [debug] [ThreadPool]: On list_quant_features: Close
[0m15:39:17.281715 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_quant_features, now create_quant_features_main)
[0m15:39:17.282041 [debug] [ThreadPool]: Creating schema "database: "quant_features"
schema: "main"
"
[0m15:39:17.286258 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:39:17.286550 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='quant_features'
        and type='sqlite'
    
  
[0m15:39:17.286745 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:39:17.287690 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:39:17.288570 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:39:17.288784 [debug] [ThreadPool]: On create_quant_features_main: BEGIN
[0m15:39:17.289105 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:39:17.289292 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:39:17.289450 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
    
        create schema if not exists "quant_features"."main"
    
[0m15:39:17.289771 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:39:17.290303 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:39:17.290502 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:39:17.290662 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:39:17.290974 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:39:17.291158 [debug] [ThreadPool]: On create_quant_features_main: Close
[0m15:39:17.295779 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_quant_features_main, now list_quant_features_main)
[0m15:39:17.299562 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:39:17.299815 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:39:17.299980 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:39:17.300462 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:39:17.300675 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:39:17.300841 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:39:17.307394 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m15:39:17.308380 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:39:17.308981 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:39:17.309197 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:39:17.311082 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5db364f2-c4ac-4826-94c9-e2c8d7595bad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd60f30a1a0>]}
[0m15:39:17.311489 [debug] [MainThread]: Using duckdb connection "master"
[0m15:39:17.311687 [debug] [MainThread]: On master: BEGIN
[0m15:39:17.311841 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:39:17.312257 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:39:17.312473 [debug] [MainThread]: On master: COMMIT
[0m15:39:17.312627 [debug] [MainThread]: Using duckdb connection "master"
[0m15:39:17.312775 [debug] [MainThread]: On master: COMMIT
[0m15:39:17.313037 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:39:17.313232 [debug] [MainThread]: On master: Close
[0m15:39:17.315569 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_factors_021_040
[0m15:39:17.315972 [info ] [Thread-1 (]: 1 of 1 START sql table model main.alpha_factors_021_040 ........................ [RUN]
[0m15:39:17.316275 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.alpha_factors_021_040)
[0m15:39:17.316495 [debug] [Thread-1 (]: Began compiling node model.quant_features.alpha_factors_021_040
[0m15:39:17.321247 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.alpha_factors_021_040"
[0m15:39:17.321659 [debug] [Thread-1 (]: Began executing node model.quant_features.alpha_factors_021_040
[0m15:39:17.342308 [debug] [Thread-1 (]: Writing runtime sql for node "model.quant_features.alpha_factors_021_040"
[0m15:39:17.342764 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_021_040"
[0m15:39:17.342987 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_021_040: BEGIN
[0m15:39:17.343183 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:39:17.343641 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:39:17.343860 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_021_040"
[0m15:39:17.344116 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_021_040: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_factors_021_040"} */

  
    
    

    create  table
      "quant_features"."main"."alpha_factors_021_040__dbt_tmp"
  
    as (
      

-- Alpha 101 因子计算 (021-040)
-- 基础版本，避免复杂嵌套

WITH base_data AS (
    SELECT * FROM "quant_features"."main"."alpha_base_data"
),

alpha_factors AS (
    SELECT 
        symbol,
        timestamp,
        
        -- 基础数据
        open, high, low, close, volume, vwap, returns,
        
        -- Alpha 021: 成交量相对强度
        volume / NULLIF(adv20, 0) AS alpha021,
        
        -- Alpha 022: 高价变化率
        (high - close_lag1) / NULLIF(close_lag1, 0) AS alpha022,
        
        -- Alpha 023: 低价相对强度  
        CASE 
            WHEN low < close_lag1 THEN -1 * (close - low)
            ELSE 0
        END AS alpha023,
        
        -- Alpha 024: 移动平均相对强度
        (close_ma5 - close_ma20) / NULLIF(close_ma20, 0) AS alpha024,
        
        -- Alpha 025: 成交量排序
        PERCENT_RANK() OVER (
            PARTITION BY timestamp 
            ORDER BY volume
        ) AS alpha025,
        
        -- Alpha 026: VWAP动量
        (vwap - close_lag5) / NULLIF(close_lag5, 0) AS alpha026,
        
        -- Alpha 027: 成交量变化率
        volume_delta1 / NULLIF(volume_lag1, 0) AS alpha027,
        
        -- Alpha 028: 价格趋势强度
        (close_ma5 - close_ma10) / NULLIF(close_ma10, 0) AS alpha028,
        
        -- Alpha 029: 收盘价相对位置
        (close - low) / NULLIF(high - low, 0) AS alpha029,
        
        -- Alpha 030: 多重信号组合
        CASE WHEN close > close_lag1 THEN 1 ELSE 0 END +
        CASE WHEN close_lag1 > close_lag2 THEN 1 ELSE 0 END +
        CASE WHEN volume > volume_lag1 THEN 1 ELSE 0 END AS alpha030,
        
        -- Alpha 031: 低价相关性
        (low - close) / NULLIF(close, 0) AS alpha031,
        
        -- Alpha 032: 价格动量
        (close - close_lag10) / NULLIF(close_lag10, 0) AS alpha032,
        
        -- Alpha 033: 开盘价相对强度
        1 - open / NULLIF(close, 0) AS alpha033,
        
        -- Alpha 034: 波动率比率
        returns_std20 / NULLIF(ABS(returns), 0.001) AS alpha034,
        
        -- Alpha 035: 成交量动量
        volume / NULLIF(volume_ma20, 0) AS alpha035,
        
        -- Alpha 036: VWAP和成交量关系
        ABS(vwap - close) * volume AS alpha036,
        
        -- Alpha 037: 收盘价相对强度
        (close - open) / NULLIF(open, 0) AS alpha037,
        
        -- Alpha 038: 开盘收盘比
        close / NULLIF(open, 0) AS alpha038,
        
        -- Alpha 039: 价格变化加权
        close_delta7 * (1 - volume / NULLIF(adv20, 0)) AS alpha039,
        
        -- Alpha 040: 成交量标准化
        (volume - volume_ma20) / NULLIF(volume_ma20, 0) AS alpha040
        
    FROM base_data
    WHERE close_ma20 IS NOT NULL
      AND returns_std20 IS NOT NULL
      AND adv20 IS NOT NULL
)

SELECT * FROM alpha_factors
    );
  
  
[0m15:39:17.350373 [debug] [Thread-1 (]: SQL status: OK in 0.006 seconds
[0m15:39:17.355924 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_021_040"
[0m15:39:17.356216 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_021_040: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_factors_021_040"} */
alter table "quant_features"."main"."alpha_factors_021_040__dbt_tmp" rename to "alpha_factors_021_040"
[0m15:39:17.356720 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:39:17.365266 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_021_040: COMMIT
[0m15:39:17.365572 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_021_040"
[0m15:39:17.365771 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_021_040: COMMIT
[0m15:39:17.368017 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:39:17.371400 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_021_040"
[0m15:39:17.371669 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_021_040: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_factors_021_040"} */

      drop table if exists "quant_features"."main"."alpha_factors_021_040__dbt_backup" cascade
    
[0m15:39:17.372090 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:39:17.373598 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_021_040: Close
[0m15:39:17.375240 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5db364f2-c4ac-4826-94c9-e2c8d7595bad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd60f4e9cd0>]}
[0m15:39:17.375695 [info ] [Thread-1 (]: 1 of 1 OK created sql table model main.alpha_factors_021_040 ................... [[32mOK[0m in 0.06s]
[0m15:39:17.376024 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_factors_021_040
[0m15:39:17.377947 [debug] [MainThread]: Using duckdb connection "master"
[0m15:39:17.378254 [debug] [MainThread]: On master: BEGIN
[0m15:39:17.378417 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:39:17.378778 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:39:17.378961 [debug] [MainThread]: On master: COMMIT
[0m15:39:17.379115 [debug] [MainThread]: Using duckdb connection "master"
[0m15:39:17.379271 [debug] [MainThread]: On master: COMMIT
[0m15:39:17.379523 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:39:17.379690 [debug] [MainThread]: On master: Close
[0m15:39:17.379943 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:39:17.380096 [debug] [MainThread]: Connection 'model.quant_features.alpha_factors_021_040' was properly closed.
[0m15:39:17.380269 [info ] [MainThread]: 
[0m15:39:17.380459 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.13 seconds (0.13s).
[0m15:39:17.380814 [debug] [MainThread]: Command end result
[0m15:39:17.402901 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:39:17.404052 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:39:17.407703 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:39:17.407933 [info ] [MainThread]: 
[0m15:39:17.408149 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:39:17.408329 [info ] [MainThread]: 
[0m15:39:17.408519 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m15:39:17.409035 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.81268287, "process_in_blocks": "0", "process_kernel_time": 0.146727, "process_mem_max_rss": "158248", "process_out_blocks": "5336", "process_user_time": 1.554503}
[0m15:39:17.409334 [debug] [MainThread]: Command `dbt run` succeeded at 15:39:17.409273 after 0.81 seconds
[0m15:39:17.409543 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd6109f7d80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd610b59770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd610b594f0>]}
[0m15:39:17.409769 [debug] [MainThread]: Flushing usage events
[0m15:39:17.465731 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:39:40.767005 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f06ad6e3770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f06aed45a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f06ac557d90>]}


============================== 15:39:40.769294 | 69dec98d-9350-4db9-b0bb-308f68f4dedf ==============================
[0m15:39:40.769294 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:39:40.770156 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'target_path': 'None', 'fail_fast': 'False', 'write_json': 'True', 'static_parser': 'True', 'log_cache_events': 'False', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'warn_error': 'None', 'printer_width': '80', 'partial_parse': 'True', 'debug': 'False', 'empty': 'False', 'invocation_command': 'dbt run --select alpha_factors_041_060', 'profiles_dir': '/workspace/dbt_project', 'log_path': '/workspace/dbt_project/logs', 'version_check': 'True', 'introspect': 'True', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'no_print': 'None', 'use_colors': 'True'}
[0m15:39:40.923260 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '69dec98d-9350-4db9-b0bb-308f68f4dedf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f06ad3d3360>]}
[0m15:39:40.965613 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '69dec98d-9350-4db9-b0bb-308f68f4dedf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f06ac64abe0>]}
[0m15:39:40.966646 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:39:40.999104 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:39:41.087930 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m15:39:41.088334 [debug] [MainThread]: Partial parsing: added file: quant_features://models/alpha101/alpha_factors_041_060.sql
[0m15:39:41.281960 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '69dec98d-9350-4db9-b0bb-308f68f4dedf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f06a8175750>]}
[0m15:39:41.380151 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:39:41.381369 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:39:41.391683 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '69dec98d-9350-4db9-b0bb-308f68f4dedf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f06a8e94c80>]}
[0m15:39:41.392021 [info ] [MainThread]: Found 21 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:39:41.392253 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '69dec98d-9350-4db9-b0bb-308f68f4dedf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f06a85f1010>]}
[0m15:39:41.393488 [info ] [MainThread]: 
[0m15:39:41.393731 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:39:41.393890 [info ] [MainThread]: 
[0m15:39:41.394170 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:39:41.394917 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features'
[0m15:39:41.410556 [debug] [ThreadPool]: Using duckdb connection "list_quant_features"
[0m15:39:41.410822 [debug] [ThreadPool]: On list_quant_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"quant_features"'
    
  
  
[0m15:39:41.411012 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:39:41.424687 [debug] [ThreadPool]: SQL status: OK in 0.014 seconds
[0m15:39:41.425559 [debug] [ThreadPool]: On list_quant_features: Close
[0m15:39:41.426400 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_quant_features, now create_quant_features_main)
[0m15:39:41.426779 [debug] [ThreadPool]: Creating schema "database: "quant_features"
schema: "main"
"
[0m15:39:41.431000 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:39:41.431274 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='quant_features'
        and type='sqlite'
    
  
[0m15:39:41.431451 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:39:41.432101 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:39:41.432962 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:39:41.433177 [debug] [ThreadPool]: On create_quant_features_main: BEGIN
[0m15:39:41.433480 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:39:41.433644 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:39:41.433788 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
    
        create schema if not exists "quant_features"."main"
    
[0m15:39:41.434056 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:39:41.434577 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:39:41.434778 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:39:41.434940 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:39:41.435234 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:39:41.435413 [debug] [ThreadPool]: On create_quant_features_main: Close
[0m15:39:41.439979 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_quant_features_main, now list_quant_features_main)
[0m15:39:41.443729 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:39:41.443979 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:39:41.444144 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:39:41.444564 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:39:41.444766 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:39:41.444929 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:39:41.451437 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m15:39:41.452410 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:39:41.453018 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:39:41.453243 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:39:41.455139 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '69dec98d-9350-4db9-b0bb-308f68f4dedf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f06a6ef6750>]}
[0m15:39:41.455515 [debug] [MainThread]: Using duckdb connection "master"
[0m15:39:41.455703 [debug] [MainThread]: On master: BEGIN
[0m15:39:41.455859 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:39:41.456267 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:39:41.456474 [debug] [MainThread]: On master: COMMIT
[0m15:39:41.456627 [debug] [MainThread]: Using duckdb connection "master"
[0m15:39:41.456765 [debug] [MainThread]: On master: COMMIT
[0m15:39:41.457016 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:39:41.457207 [debug] [MainThread]: On master: Close
[0m15:39:41.459529 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_factors_041_060
[0m15:39:41.459950 [info ] [Thread-1 (]: 1 of 1 START sql table model main.alpha_factors_041_060 ........................ [RUN]
[0m15:39:41.460255 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.alpha_factors_041_060)
[0m15:39:41.460481 [debug] [Thread-1 (]: Began compiling node model.quant_features.alpha_factors_041_060
[0m15:39:41.465619 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.alpha_factors_041_060"
[0m15:39:41.466035 [debug] [Thread-1 (]: Began executing node model.quant_features.alpha_factors_041_060
[0m15:39:41.487656 [debug] [Thread-1 (]: Writing runtime sql for node "model.quant_features.alpha_factors_041_060"
[0m15:39:41.488130 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_041_060"
[0m15:39:41.488392 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_041_060: BEGIN
[0m15:39:41.488591 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:39:41.489067 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:39:41.489297 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_041_060"
[0m15:39:41.489567 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_041_060: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_factors_041_060"} */

  
    
    

    create  table
      "quant_features"."main"."alpha_factors_041_060__dbt_tmp"
  
    as (
      

-- Alpha 101 因子计算 (041-060)

WITH base_data AS (
    SELECT * FROM "quant_features"."main"."alpha_base_data"
),

alpha_factors AS (
    SELECT 
        symbol,
        timestamp,
        
        -- 基础数据
        open, high, low, close, volume, vwap, returns,
        
        -- Alpha 041: VWAP相对强度
        (vwap - close) / NULLIF(close, 0) AS alpha041,
        
        -- Alpha 042: VWAP排序比率
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY vwap - close) /
        NULLIF(PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY vwap + close), 0) AS alpha042,
        
        -- Alpha 043: 成交量时间排序
        CASE 
            WHEN volume > volume_lag1 THEN volume / NULLIF(adv20, 0)
            ELSE 0
        END AS alpha043,
        
        -- Alpha 044: 低价和成交量关系
        low * volume AS alpha044,
        
        -- Alpha 045: 成交量加权价格
        (close * volume + open * volume) / NULLIF(2 * volume, 0) AS alpha045,
        
        -- Alpha 046: 价格动量
        (close_ma10 - close_ma20) / NULLIF(close_ma20, 0) AS alpha046,
        
        -- Alpha 047: 复杂价格成交量关系
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY 1 / NULLIF(close, 0)) * 
        volume / NULLIF(adv20, 0) * high * 
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY high - close) AS alpha047,
        
        -- Alpha 048: 收益率标准化
        returns / NULLIF(returns_std20, 0) AS alpha048,
        
        -- Alpha 049: 价格变化比率
        CASE 
            WHEN close_lag1 != 0 AND close_lag10 != 0 
            THEN (close - close_lag1) / close_lag1 - (close_lag1 - close_lag10) / close_lag10
            ELSE 0
        END AS alpha049,
        
        -- Alpha 050: 成交量相对强度
        volume / NULLIF(volume_ma20, 0) AS alpha050,
        
        -- Alpha 051: 价格变化逻辑
        CASE 
            WHEN close_delta1 < 0 THEN close_delta1
            ELSE -1 * close_delta1
        END AS alpha051,
        
        -- Alpha 052: 价格动量
        (close - close_lag5) / NULLIF(close_lag5, 0) AS alpha052,
        
        -- Alpha 053: 价格位置
        (close - low) / NULLIF(high - low, 0) AS alpha053,
        
        -- Alpha 054: 开盘价相对强度
        -1 * (low - close) * POWER(open, 5) / 
        NULLIF((low - high) * POWER(close, 5), 0) AS alpha054,
        
        -- Alpha 055: 随机指标样式
        (close - 
    MIN(low) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 11 PRECEDING AND CURRENT ROW
    )
) / 
        NULLIF(
    MAX(high) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 11 PRECEDING AND CURRENT ROW
    )
 - 
    MIN(low) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 11 PRECEDING AND CURRENT ROW
    )
, 0) AS alpha055,
        
        -- Alpha 056: 收益率比率
        returns / NULLIF(ABS(returns), 0.001) AS alpha056,
        
        -- Alpha 057: VWAP相对强度
        -1 * (close - vwap) / NULLIF(close_ma5 - close_ma20, 0) AS alpha057,
        
        -- Alpha 058: 价格相对强度
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY close) AS alpha058,
        
        -- Alpha 059: 成交量排序
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY volume) AS alpha059,
        
        -- Alpha 060: 价格成交量乘积
        (close - low - (high - close)) / NULLIF(high - low, 0) * volume AS alpha060
        
    FROM base_data
    WHERE close_ma20 IS NOT NULL
      AND returns_std20 IS NOT NULL
      AND adv20 IS NOT NULL
)

SELECT * FROM alpha_factors
    );
  
  
[0m15:39:41.502156 [debug] [Thread-1 (]: SQL status: OK in 0.012 seconds
[0m15:39:41.506327 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_041_060"
[0m15:39:41.506594 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_041_060: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_factors_041_060"} */
alter table "quant_features"."main"."alpha_factors_041_060__dbt_tmp" rename to "alpha_factors_041_060"
[0m15:39:41.507071 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:39:41.515509 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_041_060: COMMIT
[0m15:39:41.515786 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_041_060"
[0m15:39:41.515985 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_041_060: COMMIT
[0m15:39:41.518811 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m15:39:41.522183 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_041_060"
[0m15:39:41.522457 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_041_060: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_factors_041_060"} */

      drop table if exists "quant_features"."main"."alpha_factors_041_060__dbt_backup" cascade
    
[0m15:39:41.522851 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:39:41.524333 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_041_060: Close
[0m15:39:41.526129 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '69dec98d-9350-4db9-b0bb-308f68f4dedf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f06a6cfda90>]}
[0m15:39:41.526582 [info ] [Thread-1 (]: 1 of 1 OK created sql table model main.alpha_factors_041_060 ................... [[32mOK[0m in 0.07s]
[0m15:39:41.526916 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_factors_041_060
[0m15:39:41.528345 [debug] [MainThread]: Using duckdb connection "master"
[0m15:39:41.528602 [debug] [MainThread]: On master: BEGIN
[0m15:39:41.528768 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:39:41.529132 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:39:41.529332 [debug] [MainThread]: On master: COMMIT
[0m15:39:41.529499 [debug] [MainThread]: Using duckdb connection "master"
[0m15:39:41.529650 [debug] [MainThread]: On master: COMMIT
[0m15:39:41.529905 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:39:41.530073 [debug] [MainThread]: On master: Close
[0m15:39:41.530346 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:39:41.530521 [debug] [MainThread]: Connection 'model.quant_features.alpha_factors_041_060' was properly closed.
[0m15:39:41.530701 [info ] [MainThread]: 
[0m15:39:41.530875 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.14 seconds (0.14s).
[0m15:39:41.531265 [debug] [MainThread]: Command end result
[0m15:39:41.552090 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:39:41.553291 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:39:41.556953 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:39:41.557222 [info ] [MainThread]: 
[0m15:39:41.557456 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:39:41.557630 [info ] [MainThread]: 
[0m15:39:41.557819 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m15:39:41.558364 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.827413, "process_in_blocks": "0", "process_kernel_time": 0.090708, "process_mem_max_rss": "163488", "process_out_blocks": "5392", "process_user_time": 1.613009}
[0m15:39:41.558641 [debug] [MainThread]: Command `dbt run` succeeded at 15:39:41.558584 after 0.83 seconds
[0m15:39:41.558834 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f06a81f7d80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f06a6c07d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f06a6c07cf0>]}
[0m15:39:41.559044 [debug] [MainThread]: Flushing usage events
[0m15:39:41.604613 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:40:01.480383 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba84e2f770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba8647da90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba83ca3d90>]}


============================== 15:40:01.482848 | a2740a36-c2df-4cdc-914b-c0b23dc30863 ==============================
[0m15:40:01.482848 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:40:01.483599 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'write_json': 'True', 'profiles_dir': '/workspace/dbt_project', 'empty': 'False', 'cache_selected_only': 'False', 'target_path': 'None', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'partial_parse': 'True', 'introspect': 'True', 'version_check': 'True', 'fail_fast': 'False', 'log_format': 'default', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'warn_error': 'None', 'quiet': 'False', 'use_colors': 'True', 'printer_width': '80', 'log_path': '/workspace/dbt_project/logs', 'invocation_command': 'dbt run --select alpha_factors_061_080', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False'}
[0m15:40:01.634349 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a2740a36-c2df-4cdc-914b-c0b23dc30863', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba84b23360>]}
[0m15:40:01.677391 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a2740a36-c2df-4cdc-914b-c0b23dc30863', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba83d96be0>]}
[0m15:40:01.678524 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:40:01.711756 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:40:01.810492 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m15:40:01.811003 [debug] [MainThread]: Partial parsing: added file: quant_features://models/alpha101/alpha_factors_061_080.sql
[0m15:40:02.011039 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a2740a36-c2df-4cdc-914b-c0b23dc30863', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba7fd29a50>]}
[0m15:40:02.119261 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:40:02.120543 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:40:02.131584 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a2740a36-c2df-4cdc-914b-c0b23dc30863', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba7fd60c80>]}
[0m15:40:02.131936 [info ] [MainThread]: Found 22 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:40:02.132156 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a2740a36-c2df-4cdc-914b-c0b23dc30863', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba7fcec590>]}
[0m15:40:02.133454 [info ] [MainThread]: 
[0m15:40:02.133728 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:40:02.133900 [info ] [MainThread]: 
[0m15:40:02.134200 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:40:02.135089 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features'
[0m15:40:02.150982 [debug] [ThreadPool]: Using duckdb connection "list_quant_features"
[0m15:40:02.151268 [debug] [ThreadPool]: On list_quant_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"quant_features"'
    
  
  
[0m15:40:02.151455 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:40:02.170143 [debug] [ThreadPool]: SQL status: OK in 0.019 seconds
[0m15:40:02.171480 [debug] [ThreadPool]: On list_quant_features: Close
[0m15:40:02.172270 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_quant_features, now create_quant_features_main)
[0m15:40:02.172605 [debug] [ThreadPool]: Creating schema "database: "quant_features"
schema: "main"
"
[0m15:40:02.177537 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:40:02.177785 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='quant_features'
        and type='sqlite'
    
  
[0m15:40:02.177959 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:40:02.178847 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:40:02.180013 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:40:02.180302 [debug] [ThreadPool]: On create_quant_features_main: BEGIN
[0m15:40:02.180706 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:40:02.180990 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:40:02.181225 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
    
        create schema if not exists "quant_features"."main"
    
[0m15:40:02.182018 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:40:02.182859 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:40:02.183202 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:40:02.183477 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:40:02.184029 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:40:02.184382 [debug] [ThreadPool]: On create_quant_features_main: Close
[0m15:40:02.189125 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_quant_features_main, now list_quant_features_main)
[0m15:40:02.193834 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:40:02.194081 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:40:02.194264 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:40:02.194634 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:40:02.194892 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:40:02.195072 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:40:02.202290 [debug] [ThreadPool]: SQL status: OK in 0.007 seconds
[0m15:40:02.203268 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:40:02.203882 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:40:02.204089 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:40:02.206123 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a2740a36-c2df-4cdc-914b-c0b23dc30863', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba7e5c6dd0>]}
[0m15:40:02.206520 [debug] [MainThread]: Using duckdb connection "master"
[0m15:40:02.206711 [debug] [MainThread]: On master: BEGIN
[0m15:40:02.206870 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:40:02.207280 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:40:02.207482 [debug] [MainThread]: On master: COMMIT
[0m15:40:02.207639 [debug] [MainThread]: Using duckdb connection "master"
[0m15:40:02.207786 [debug] [MainThread]: On master: COMMIT
[0m15:40:02.208042 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:40:02.208240 [debug] [MainThread]: On master: Close
[0m15:40:02.211237 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_factors_061_080
[0m15:40:02.211650 [info ] [Thread-1 (]: 1 of 1 START sql table model main.alpha_factors_061_080 ........................ [RUN]
[0m15:40:02.211946 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.alpha_factors_061_080)
[0m15:40:02.212150 [debug] [Thread-1 (]: Began compiling node model.quant_features.alpha_factors_061_080
[0m15:40:02.217827 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.alpha_factors_061_080"
[0m15:40:02.218270 [debug] [Thread-1 (]: Began executing node model.quant_features.alpha_factors_061_080
[0m15:40:02.240304 [debug] [Thread-1 (]: Writing runtime sql for node "model.quant_features.alpha_factors_061_080"
[0m15:40:02.240788 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_061_080"
[0m15:40:02.241010 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_061_080: BEGIN
[0m15:40:02.241205 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:40:02.241639 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:40:02.241865 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_061_080"
[0m15:40:02.242159 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_061_080: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_factors_061_080"} */

  
    
    

    create  table
      "quant_features"."main"."alpha_factors_061_080__dbt_tmp"
  
    as (
      

-- Alpha 101 因子计算 (061-080)

WITH base_data AS (
    SELECT * FROM "quant_features"."main"."alpha_base_data"
),

alpha_factors AS (
    SELECT 
        symbol,
        timestamp,
        
        -- 基础数据
        open, high, low, close, volume, vwap, returns,
        
        -- Alpha 061: 成交量排序
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY volume) AS alpha061,
        
        -- Alpha 062: 高价相对强度
        -1 * 
    CORR(high, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS alpha062,
        
        -- Alpha 063: 价格动量
        
    
    CASE 
        WHEN close / close_lag1 - 1 > 0 THEN 1
        WHEN close / close_lag1 - 1 < 0 THEN -1
        ELSE 0
    END
 * POWER(ABS(close / close_lag1 - 1), 1)
 AS alpha063,
        
        -- Alpha 064: 成交量相对强度
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY volume / NULLIF(adv20, 0)) AS alpha064,
        
        -- Alpha 065: 价格相对强度
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY close / NULLIF(close_lag1, 0)) AS alpha065,
        
        -- Alpha 066: 低价和VWAP差异
        (low - vwap) / NULLIF((open + high) / 2 - (high + low) / 2, 0) AS alpha066,
        
        -- Alpha 067: 价格排序
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY high) AS alpha067,
        
        -- Alpha 068: 高价相对强度
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY high) * volume AS alpha068,
        
        -- Alpha 069: 价格变化
        POWER(close_delta1 / NULLIF(close_lag1, 0), 2) AS alpha069,
        
        -- Alpha 070: 价格标准化
        
    STDDEV(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 AS alpha070,
        
        -- Alpha 071: 价格相对强度
        (close - close_ma20) / NULLIF(close_ma20, 0) AS alpha071,
        
        -- Alpha 072: 成交量相对强度
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY volume / NULLIF(adv20, 0)) AS alpha072,
        
        -- Alpha 073: 价格排序
        -1 * PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY close) AS alpha073,
        
        -- Alpha 074: 高价和低价关系
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY high) + 
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY low) AS alpha074,
        
        -- Alpha 075: 成交量相关性
        
    CORR(vwap, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 3 PRECEDING AND CURRENT ROW
    )
 AS alpha075,
        
        -- Alpha 076: 价格标准化
        
    (volume / adv20 - AVG(volume / adv20) OVER (PARTITION BY timestamp)) / 
    NULLIF(STDDEV(volume / adv20) OVER (PARTITION BY timestamp), 0)
 AS alpha076,
        
        -- Alpha 077: 高价相对强度
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY high) AS alpha077,
        
        -- Alpha 078: 低价相对强度
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY low) AS alpha078,
        
        -- Alpha 079: 价格变化
        
    CASE 
        WHEN close - close_lag1 > 0 THEN 1
        WHEN close - close_lag1 < 0 THEN -1
        ELSE 0
    END
 AS alpha079,
        
        -- Alpha 080: 成交量变化
        volume / NULLIF(volume_lag1, 0) - 1 AS alpha080
        
    FROM base_data
    WHERE close_ma20 IS NOT NULL
      AND returns_std20 IS NOT NULL
      AND adv20 IS NOT NULL
)

SELECT * FROM alpha_factors
    );
  
  
[0m15:40:02.256796 [debug] [Thread-1 (]: SQL status: OK in 0.014 seconds
[0m15:40:02.260879 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_061_080"
[0m15:40:02.261155 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_061_080: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_factors_061_080"} */
alter table "quant_features"."main"."alpha_factors_061_080__dbt_tmp" rename to "alpha_factors_061_080"
[0m15:40:02.261725 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:40:02.270239 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_061_080: COMMIT
[0m15:40:02.270520 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_061_080"
[0m15:40:02.270715 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_061_080: COMMIT
[0m15:40:02.272628 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:40:02.276036 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_061_080"
[0m15:40:02.276385 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_061_080: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_factors_061_080"} */

      drop table if exists "quant_features"."main"."alpha_factors_061_080__dbt_backup" cascade
    
[0m15:40:02.276839 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:40:02.278377 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_061_080: Close
[0m15:40:02.280158 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a2740a36-c2df-4cdc-914b-c0b23dc30863', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba7e5f5c10>]}
[0m15:40:02.280653 [info ] [Thread-1 (]: 1 of 1 OK created sql table model main.alpha_factors_061_080 ................... [[32mOK[0m in 0.07s]
[0m15:40:02.281002 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_factors_061_080
[0m15:40:02.282377 [debug] [MainThread]: Using duckdb connection "master"
[0m15:40:02.282608 [debug] [MainThread]: On master: BEGIN
[0m15:40:02.282772 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:40:02.283129 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:40:02.283324 [debug] [MainThread]: On master: COMMIT
[0m15:40:02.283483 [debug] [MainThread]: Using duckdb connection "master"
[0m15:40:02.283640 [debug] [MainThread]: On master: COMMIT
[0m15:40:02.283897 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:40:02.284064 [debug] [MainThread]: On master: Close
[0m15:40:02.284338 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:40:02.284526 [debug] [MainThread]: Connection 'model.quant_features.alpha_factors_061_080' was properly closed.
[0m15:40:02.284696 [info ] [MainThread]: 
[0m15:40:02.284889 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.15 seconds (0.15s).
[0m15:40:02.285285 [debug] [MainThread]: Command end result
[0m15:40:02.309225 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:40:02.310467 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:40:02.314290 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:40:02.314517 [info ] [MainThread]: 
[0m15:40:02.314757 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:40:02.314944 [info ] [MainThread]: 
[0m15:40:02.315141 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m15:40:02.315789 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.87198776, "process_in_blocks": "0", "process_kernel_time": 0.168013, "process_mem_max_rss": "162280", "process_out_blocks": "5424", "process_user_time": 1.632106}
[0m15:40:02.316093 [debug] [MainThread]: Command `dbt run` succeeded at 15:40:02.316023 after 0.87 seconds
[0m15:40:02.316324 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba7f8f7d80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba7e307c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba7e307bb0>]}
[0m15:40:02.316578 [debug] [MainThread]: Flushing usage events
[0m15:40:02.359904 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:40:22.739731 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f01d630b770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f01d7955a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f01d517fd90>]}


============================== 15:40:22.742061 | 2939c3af-29d1-4ef2-9f83-279338dd4d50 ==============================
[0m15:40:22.742061 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:40:22.742399 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'invocation_command': 'dbt run --select alpha_factors_081_101', 'log_format': 'default', 'printer_width': '80', 'indirect_selection': 'eager', 'introspect': 'True', 'empty': 'False', 'version_check': 'True', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'profiles_dir': '/workspace/dbt_project', 'no_print': 'None', 'cache_selected_only': 'False', 'use_experimental_parser': 'False', 'write_json': 'True', 'debug': 'False', 'use_colors': 'True', 'static_parser': 'True', 'partial_parse': 'True', 'log_cache_events': 'False', 'quiet': 'False', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/workspace/dbt_project/logs'}
[0m15:40:22.886790 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2939c3af-29d1-4ef2-9f83-279338dd4d50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f01d5ffb360>]}
[0m15:40:22.929164 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2939c3af-29d1-4ef2-9f83-279338dd4d50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f01d526ebe0>]}
[0m15:40:22.930167 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:40:22.962660 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:40:23.053005 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m15:40:23.053417 [debug] [MainThread]: Partial parsing: added file: quant_features://models/alpha101/alpha_factors_081_101.sql
[0m15:40:23.249494 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2939c3af-29d1-4ef2-9f83-279338dd4d50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f01d1a8da50>]}
[0m15:40:23.348436 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:40:23.349661 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:40:23.360354 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2939c3af-29d1-4ef2-9f83-279338dd4d50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f01d1abcc80>]}
[0m15:40:23.360689 [info ] [MainThread]: Found 23 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:40:23.360897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2939c3af-29d1-4ef2-9f83-279338dd4d50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f01d11f0590>]}
[0m15:40:23.362094 [info ] [MainThread]: 
[0m15:40:23.362356 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:40:23.362525 [info ] [MainThread]: 
[0m15:40:23.362795 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:40:23.363593 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features'
[0m15:40:23.379088 [debug] [ThreadPool]: Using duckdb connection "list_quant_features"
[0m15:40:23.379376 [debug] [ThreadPool]: On list_quant_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"quant_features"'
    
  
  
[0m15:40:23.379559 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:40:23.393985 [debug] [ThreadPool]: SQL status: OK in 0.014 seconds
[0m15:40:23.394848 [debug] [ThreadPool]: On list_quant_features: Close
[0m15:40:23.395501 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_quant_features, now create_quant_features_main)
[0m15:40:23.395796 [debug] [ThreadPool]: Creating schema "database: "quant_features"
schema: "main"
"
[0m15:40:23.399914 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:40:23.400166 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='quant_features'
        and type='sqlite'
    
  
[0m15:40:23.400348 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:40:23.401028 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:40:23.401894 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:40:23.402099 [debug] [ThreadPool]: On create_quant_features_main: BEGIN
[0m15:40:23.402402 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:40:23.402568 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:40:23.402716 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
    
        create schema if not exists "quant_features"."main"
    
[0m15:40:23.403042 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:40:23.403572 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:40:23.403772 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:40:23.403938 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:40:23.404271 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:40:23.404471 [debug] [ThreadPool]: On create_quant_features_main: Close
[0m15:40:23.409301 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_quant_features_main, now list_quant_features_main)
[0m15:40:23.413270 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:40:23.413512 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:40:23.413672 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:40:23.414040 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:40:23.414228 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:40:23.414385 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:40:23.420861 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m15:40:23.421830 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:40:23.422424 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:40:23.422627 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:40:23.424807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2939c3af-29d1-4ef2-9f83-279338dd4d50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f01d02cef70>]}
[0m15:40:23.425108 [debug] [MainThread]: Using duckdb connection "master"
[0m15:40:23.425289 [debug] [MainThread]: On master: BEGIN
[0m15:40:23.425436 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:40:23.425805 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:40:23.425999 [debug] [MainThread]: On master: COMMIT
[0m15:40:23.426151 [debug] [MainThread]: Using duckdb connection "master"
[0m15:40:23.426307 [debug] [MainThread]: On master: COMMIT
[0m15:40:23.426548 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:40:23.426722 [debug] [MainThread]: On master: Close
[0m15:40:23.429151 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_factors_081_101
[0m15:40:23.429551 [info ] [Thread-1 (]: 1 of 1 START sql table model main.alpha_factors_081_101 ........................ [RUN]
[0m15:40:23.429836 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.alpha_factors_081_101)
[0m15:40:23.430039 [debug] [Thread-1 (]: Began compiling node model.quant_features.alpha_factors_081_101
[0m15:40:23.435773 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.alpha_factors_081_101"
[0m15:40:23.436193 [debug] [Thread-1 (]: Began executing node model.quant_features.alpha_factors_081_101
[0m15:40:23.457635 [debug] [Thread-1 (]: Writing runtime sql for node "model.quant_features.alpha_factors_081_101"
[0m15:40:23.458089 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_081_101"
[0m15:40:23.458322 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_081_101: BEGIN
[0m15:40:23.458510 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:40:23.459015 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:40:23.459275 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_081_101"
[0m15:40:23.459572 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_081_101: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_factors_081_101"} */

  
    
    

    create  table
      "quant_features"."main"."alpha_factors_081_101__dbt_tmp"
  
    as (
      

-- Alpha 101 因子计算 (081-101)

WITH base_data AS (
    SELECT * FROM "quant_features"."main"."alpha_base_data"
),

alpha_factors AS (
    SELECT 
        symbol,
        timestamp,
        
        -- 基础数据
        open, high, low, close, volume, vwap, returns,
        
        -- Alpha 081: 成交量相对强度
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY volume) AS alpha081,
        
        -- Alpha 082: 价格排序
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY close) AS alpha082,
        
        -- Alpha 083: 高低价差异
        (high - low) / NULLIF(
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
, 0) AS alpha083,
        
        -- Alpha 084: 价格相对强度
        
    
    CASE 
        WHEN vwap - close > 0 THEN 1
        WHEN vwap - close < 0 THEN -1
        ELSE 0
    END
 * POWER(ABS(vwap - close), )
 AS alpha084,
        
        -- Alpha 085: 成交量排序
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY volume) AS alpha085,
        
        -- Alpha 086: 价格延迟
        CASE 
            WHEN 
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 > close_lag10 THEN -1
            ELSE 1
        END AS alpha086,
        
        -- Alpha 087: 价格相关性
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY close) AS alpha087,
        
        -- Alpha 088: 价格相对强度
        (close - close_lag20) / NULLIF(close_lag20, 0) AS alpha088,
        
        -- Alpha 089: 价格趋势
        (close - close_lag5) / NULLIF(close_lag5, 0) AS alpha089,
        
        -- Alpha 090: 价格排序
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY close) AS alpha090,
        
        -- Alpha 091: 成交量相对强度
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY volume) AS alpha091,
        
        -- Alpha 092: 价格相对强度
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY high + low) AS alpha092,
        
        -- Alpha 093: 成交量排序
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY volume) AS alpha093,
        
        -- Alpha 094: 成交量相关性
        
    CORR(close, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 29 PRECEDING AND CURRENT ROW
    )
 AS alpha094,
        
        -- Alpha 095: 价格标准化
        
    (volume - AVG(volume) OVER (PARTITION BY timestamp)) / 
    NULLIF(STDDEV(volume) OVER (PARTITION BY timestamp), 0)
 AS alpha095,
        
        -- Alpha 096: 价格相对强度
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY close) AS alpha096,
        
        -- Alpha 097: 成交量标准化
        
    (volume - AVG(volume) OVER (PARTITION BY timestamp)) / 
    NULLIF(STDDEV(volume) OVER (PARTITION BY timestamp), 0)
 AS alpha097,
        
        -- Alpha 098: 价格相关性
        
    CORR(vwap, close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS alpha098,
        
        -- Alpha 099: 成交量相对强度
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY volume) AS alpha099,
        
        -- Alpha 100: 价格标准化
        
    (close - AVG(close) OVER (PARTITION BY timestamp)) / 
    NULLIF(STDDEV(close) OVER (PARTITION BY timestamp), 0)
 AS alpha100,
        
        -- Alpha 101: 收盘价相对强度
        (close - open) / NULLIF(high - low, 0) AS alpha101
        
    FROM base_data
    WHERE close_ma20 IS NOT NULL
      AND returns_std20 IS NOT NULL
      AND adv20 IS NOT NULL
)

SELECT * FROM alpha_factors
    );
  
  
[0m15:40:23.460394 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_factors_081_101"} */

  
    
    

    create  table
      "quant_features"."main"."alpha_factors_081_101__dbt_tmp"
  
    as (
      

-- Alpha 101 因子计算 (081-101)

WITH base_data AS (
    SELECT * FROM "quant_features"."main"."alpha_base_data"
),

alpha_factors AS (
    SELECT 
        symbol,
        timestamp,
        
        -- 基础数据
        open, high, low, close, volume, vwap, returns,
        
        -- Alpha 081: 成交量相对强度
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY volume) AS alpha081,
        
        -- Alpha 082: 价格排序
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY close) AS alpha082,
        
        -- Alpha 083: 高低价差异
        (high - low) / NULLIF(
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
, 0) AS alpha083,
        
        -- Alpha 084: 价格相对强度
        
    
    CASE 
        WHEN vwap - close > 0 THEN 1
        WHEN vwap - close < 0 THEN -1
        ELSE 0
    END
 * POWER(ABS(vwap - close), )
 AS alpha084,
        
        -- Alpha 085: 成交量排序
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY volume) AS alpha085,
        
        -- Alpha 086: 价格延迟
        CASE 
            WHEN 
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 > close_lag10 THEN -1
            ELSE 1
        END AS alpha086,
        
        -- Alpha 087: 价格相关性
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY close) AS alpha087,
        
        -- Alpha 088: 价格相对强度
        (close - close_lag20) / NULLIF(close_lag20, 0) AS alpha088,
        
        -- Alpha 089: 价格趋势
        (close - close_lag5) / NULLIF(close_lag5, 0) AS alpha089,
        
        -- Alpha 090: 价格排序
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY close) AS alpha090,
        
        -- Alpha 091: 成交量相对强度
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY volume) AS alpha091,
        
        -- Alpha 092: 价格相对强度
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY high + low) AS alpha092,
        
        -- Alpha 093: 成交量排序
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY volume) AS alpha093,
        
        -- Alpha 094: 成交量相关性
        
    CORR(close, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 29 PRECEDING AND CURRENT ROW
    )
 AS alpha094,
        
        -- Alpha 095: 价格标准化
        
    (volume - AVG(volume) OVER (PARTITION BY timestamp)) / 
    NULLIF(STDDEV(volume) OVER (PARTITION BY timestamp), 0)
 AS alpha095,
        
        -- Alpha 096: 价格相对强度
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY close) AS alpha096,
        
        -- Alpha 097: 成交量标准化
        
    (volume - AVG(volume) OVER (PARTITION BY timestamp)) / 
    NULLIF(STDDEV(volume) OVER (PARTITION BY timestamp), 0)
 AS alpha097,
        
        -- Alpha 098: 价格相关性
        
    CORR(vwap, close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS alpha098,
        
        -- Alpha 099: 成交量相对强度
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY volume) AS alpha099,
        
        -- Alpha 100: 价格标准化
        
    (close - AVG(close) OVER (PARTITION BY timestamp)) / 
    NULLIF(STDDEV(close) OVER (PARTITION BY timestamp), 0)
 AS alpha100,
        
        -- Alpha 101: 收盘价相对强度
        (close - open) / NULLIF(high - low, 0) AS alpha101
        
    FROM base_data
    WHERE close_ma20 IS NOT NULL
      AND returns_std20 IS NOT NULL
      AND adv20 IS NOT NULL
)

SELECT * FROM alpha_factors
    );
  
  
[0m15:40:23.460726 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m15:40:23.460991 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_081_101: ROLLBACK
[0m15:40:23.465992 [debug] [Thread-1 (]: Failed to rollback 'model.quant_features.alpha_factors_081_101'
[0m15:40:23.466295 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_081_101: Close
[0m15:40:23.468655 [debug] [Thread-1 (]: Runtime Error in model alpha_factors_081_101 (models/alpha101/alpha_factors_081_101.sql)
  Parser Error: syntax error at or near ")"
[0m15:40:23.469671 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2939c3af-29d1-4ef2-9f83-279338dd4d50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f01d0102450>]}
[0m15:40:23.470111 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model main.alpha_factors_081_101 ............... [[31mERROR[0m in 0.04s]
[0m15:40:23.470489 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_factors_081_101
[0m15:40:23.471191 [debug] [Thread-7 (]: Marking all children of 'model.quant_features.alpha_factors_081_101' to be skipped because of status 'error'.  Reason: Runtime Error in model alpha_factors_081_101 (models/alpha101/alpha_factors_081_101.sql)
  Parser Error: syntax error at or near ")".
[0m15:40:23.473652 [debug] [MainThread]: Using duckdb connection "master"
[0m15:40:23.473894 [debug] [MainThread]: On master: BEGIN
[0m15:40:23.474056 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:40:23.474420 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:40:23.474602 [debug] [MainThread]: On master: COMMIT
[0m15:40:23.474775 [debug] [MainThread]: Using duckdb connection "master"
[0m15:40:23.474923 [debug] [MainThread]: On master: COMMIT
[0m15:40:23.475171 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:40:23.475370 [debug] [MainThread]: On master: Close
[0m15:40:23.475631 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:40:23.475791 [debug] [MainThread]: Connection 'model.quant_features.alpha_factors_081_101' was properly closed.
[0m15:40:23.475952 [info ] [MainThread]: 
[0m15:40:23.476138 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.11 seconds (0.11s).
[0m15:40:23.476592 [debug] [MainThread]: Command end result
[0m15:40:23.498313 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:40:23.499507 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:40:23.503167 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:40:23.503443 [info ] [MainThread]: 
[0m15:40:23.503656 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:40:23.503830 [info ] [MainThread]: 
[0m15:40:23.504031 [error] [MainThread]: [31mFailure in model alpha_factors_081_101 (models/alpha101/alpha_factors_081_101.sql)[0m
[0m15:40:23.504229 [error] [MainThread]:   Runtime Error in model alpha_factors_081_101 (models/alpha101/alpha_factors_081_101.sql)
  Parser Error: syntax error at or near ")"
[0m15:40:23.504418 [info ] [MainThread]: 
[0m15:40:23.504615 [info ] [MainThread]:   compiled code at target/compiled/quant_features/models/alpha101/alpha_factors_081_101.sql
[0m15:40:23.504759 [info ] [MainThread]: 
[0m15:40:23.504920 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m15:40:23.505446 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.8020329, "process_in_blocks": "0", "process_kernel_time": 0.127379, "process_mem_max_rss": "154820", "process_out_blocks": "5440", "process_user_time": 1.552436}
[0m15:40:23.505737 [debug] [MainThread]: Command `dbt run` failed at 15:40:23.505677 after 0.80 seconds
[0m15:40:23.505954 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f01d0defcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f01d113ec10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f01d113d4f0>]}
[0m15:40:23.506175 [debug] [MainThread]: Flushing usage events
[0m15:40:23.550005 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:40:41.233719 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7f62c3770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7f7911a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7f5133d90>]}


============================== 15:40:41.236052 | 449a2153-970c-43b4-8eb1-726106083bc4 ==============================
[0m15:40:41.236052 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:40:41.236421 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt run --select alpha_factors_081_101', 'profiles_dir': '/workspace/dbt_project', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_cache_events': 'False', 'printer_width': '80', 'quiet': 'False', 'log_path': '/workspace/dbt_project/logs', 'log_format': 'default', 'version_check': 'True', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'fail_fast': 'False', 'warn_error': 'None', 'write_json': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'False', 'introspect': 'True', 'debug': 'False', 'use_colors': 'True', 'target_path': 'None', 'cache_selected_only': 'False', 'no_print': 'None'}
[0m15:40:41.381717 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '449a2153-970c-43b4-8eb1-726106083bc4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7f5faf360>]}
[0m15:40:41.423993 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '449a2153-970c-43b4-8eb1-726106083bc4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7f5222be0>]}
[0m15:40:41.424995 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:40:41.456480 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:40:41.545893 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:40:41.546357 [debug] [MainThread]: Partial parsing: updated file: quant_features://models/alpha101/alpha_factors_081_101.sql
[0m15:40:41.743956 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '449a2153-970c-43b4-8eb1-726106083bc4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7f1a41a50>]}
[0m15:40:41.844294 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:40:41.845546 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:40:41.856288 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '449a2153-970c-43b4-8eb1-726106083bc4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7f1a74c80>]}
[0m15:40:41.856630 [info ] [MainThread]: Found 23 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:40:41.856838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '449a2153-970c-43b4-8eb1-726106083bc4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7f11e1550>]}
[0m15:40:41.858065 [info ] [MainThread]: 
[0m15:40:41.858332 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:40:41.858506 [info ] [MainThread]: 
[0m15:40:41.858788 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:40:41.859630 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features'
[0m15:40:41.875335 [debug] [ThreadPool]: Using duckdb connection "list_quant_features"
[0m15:40:41.875607 [debug] [ThreadPool]: On list_quant_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"quant_features"'
    
  
  
[0m15:40:41.875817 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:40:41.889978 [debug] [ThreadPool]: SQL status: OK in 0.014 seconds
[0m15:40:41.890892 [debug] [ThreadPool]: On list_quant_features: Close
[0m15:40:41.891658 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_quant_features, now create_quant_features_main)
[0m15:40:41.891987 [debug] [ThreadPool]: Creating schema "database: "quant_features"
schema: "main"
"
[0m15:40:41.896140 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:40:41.896444 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='quant_features'
        and type='sqlite'
    
  
[0m15:40:41.896653 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:40:41.897375 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:40:41.898253 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:40:41.898492 [debug] [ThreadPool]: On create_quant_features_main: BEGIN
[0m15:40:41.898853 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:40:41.899051 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:40:41.899247 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
    
        create schema if not exists "quant_features"."main"
    
[0m15:40:41.899613 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:40:41.900156 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:40:41.900398 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:40:41.900585 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:40:41.900918 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:40:41.901117 [debug] [ThreadPool]: On create_quant_features_main: Close
[0m15:40:41.906017 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_quant_features_main, now list_quant_features_main)
[0m15:40:41.909678 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:40:41.909928 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:40:41.910118 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:40:41.910591 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:40:41.910809 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:40:41.910993 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:40:41.917739 [debug] [ThreadPool]: SQL status: OK in 0.007 seconds
[0m15:40:41.918720 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:40:41.919334 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:40:41.919566 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:40:41.921658 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '449a2153-970c-43b4-8eb1-726106083bc4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7f02b6dd0>]}
[0m15:40:41.921961 [debug] [MainThread]: Using duckdb connection "master"
[0m15:40:41.922128 [debug] [MainThread]: On master: BEGIN
[0m15:40:41.922283 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:40:41.922674 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:40:41.922875 [debug] [MainThread]: On master: COMMIT
[0m15:40:41.923044 [debug] [MainThread]: Using duckdb connection "master"
[0m15:40:41.923191 [debug] [MainThread]: On master: COMMIT
[0m15:40:41.923478 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:40:41.923647 [debug] [MainThread]: On master: Close
[0m15:40:41.926365 [debug] [Thread-1 (]: Began running node model.quant_features.alpha_factors_081_101
[0m15:40:41.926748 [info ] [Thread-1 (]: 1 of 1 START sql table model main.alpha_factors_081_101 ........................ [RUN]
[0m15:40:41.927031 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.alpha_factors_081_101)
[0m15:40:41.927241 [debug] [Thread-1 (]: Began compiling node model.quant_features.alpha_factors_081_101
[0m15:40:41.932988 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.alpha_factors_081_101"
[0m15:40:41.933444 [debug] [Thread-1 (]: Began executing node model.quant_features.alpha_factors_081_101
[0m15:40:41.955091 [debug] [Thread-1 (]: Writing runtime sql for node "model.quant_features.alpha_factors_081_101"
[0m15:40:41.955548 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_081_101"
[0m15:40:41.955811 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_081_101: BEGIN
[0m15:40:41.955995 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:40:41.956511 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:40:41.956756 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_081_101"
[0m15:40:41.957044 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_081_101: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_factors_081_101"} */

  
    
    

    create  table
      "quant_features"."main"."alpha_factors_081_101__dbt_tmp"
  
    as (
      

-- Alpha 101 因子计算 (081-101)

WITH base_data AS (
    SELECT * FROM "quant_features"."main"."alpha_base_data"
),

alpha_factors AS (
    SELECT 
        symbol,
        timestamp,
        
        -- 基础数据
        open, high, low, close, volume, vwap, returns,
        
        -- Alpha 081: 成交量相对强度
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY volume) AS alpha081,
        
        -- Alpha 082: 价格排序
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY close) AS alpha082,
        
        -- Alpha 083: 高低价差异
        (high - low) / NULLIF(
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
, 0) AS alpha083,
        
        -- Alpha 084: 价格相对强度
        
    
    CASE 
        WHEN vwap - close > 0 THEN 1
        WHEN vwap - close < 0 THEN -1
        ELSE 0
    END
 * POWER(ABS(vwap - close), 1)
 AS alpha084,
        
        -- Alpha 085: 成交量排序
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY volume) AS alpha085,
        
        -- Alpha 086: 价格延迟
        CASE 
            WHEN 
    AVG(close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
    )
 > close_lag10 THEN -1
            ELSE 1
        END AS alpha086,
        
        -- Alpha 087: 价格相关性
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY close) AS alpha087,
        
        -- Alpha 088: 价格相对强度
        (close - close_lag20) / NULLIF(close_lag20, 0) AS alpha088,
        
        -- Alpha 089: 价格趋势
        (close - close_lag5) / NULLIF(close_lag5, 0) AS alpha089,
        
        -- Alpha 090: 价格排序
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY close) AS alpha090,
        
        -- Alpha 091: 成交量相对强度
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY volume) AS alpha091,
        
        -- Alpha 092: 价格相对强度
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY high + low) AS alpha092,
        
        -- Alpha 093: 成交量排序
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY volume) AS alpha093,
        
        -- Alpha 094: 成交量相关性
        
    CORR(close, volume) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 29 PRECEDING AND CURRENT ROW
    )
 AS alpha094,
        
        -- Alpha 095: 价格标准化
        
    (volume - AVG(volume) OVER (PARTITION BY timestamp)) / 
    NULLIF(STDDEV(volume) OVER (PARTITION BY timestamp), 0)
 AS alpha095,
        
        -- Alpha 096: 价格相对强度
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY close) AS alpha096,
        
        -- Alpha 097: 成交量标准化
        
    (volume - AVG(volume) OVER (PARTITION BY timestamp)) / 
    NULLIF(STDDEV(volume) OVER (PARTITION BY timestamp), 0)
 AS alpha097,
        
        -- Alpha 098: 价格相关性
        
    CORR(vwap, close) OVER (
        PARTITION BY symbol 
        ORDER BY timestamp
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
 AS alpha098,
        
        -- Alpha 099: 成交量相对强度
        PERCENT_RANK() OVER (PARTITION BY timestamp ORDER BY volume) AS alpha099,
        
        -- Alpha 100: 价格标准化
        
    (close - AVG(close) OVER (PARTITION BY timestamp)) / 
    NULLIF(STDDEV(close) OVER (PARTITION BY timestamp), 0)
 AS alpha100,
        
        -- Alpha 101: 收盘价相对强度
        (close - open) / NULLIF(high - low, 0) AS alpha101
        
    FROM base_data
    WHERE close_ma20 IS NOT NULL
      AND returns_std20 IS NOT NULL
      AND adv20 IS NOT NULL
)

SELECT * FROM alpha_factors
    );
  
  
[0m15:40:41.966851 [debug] [Thread-1 (]: SQL status: OK in 0.009 seconds
[0m15:40:41.970881 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_081_101"
[0m15:40:41.971174 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_081_101: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_factors_081_101"} */
alter table "quant_features"."main"."alpha_factors_081_101__dbt_tmp" rename to "alpha_factors_081_101"
[0m15:40:41.971681 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:40:41.979979 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_081_101: COMMIT
[0m15:40:41.980269 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_081_101"
[0m15:40:41.980483 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_081_101: COMMIT
[0m15:40:41.982280 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:40:41.985670 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha_factors_081_101"
[0m15:40:41.985935 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_081_101: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha_factors_081_101"} */

      drop table if exists "quant_features"."main"."alpha_factors_081_101__dbt_backup" cascade
    
[0m15:40:41.986372 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:40:41.987889 [debug] [Thread-1 (]: On model.quant_features.alpha_factors_081_101: Close
[0m15:40:41.989605 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '449a2153-970c-43b4-8eb1-726106083bc4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7f02e5e50>]}
[0m15:40:41.990046 [info ] [Thread-1 (]: 1 of 1 OK created sql table model main.alpha_factors_081_101 ................... [[32mOK[0m in 0.06s]
[0m15:40:41.990396 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha_factors_081_101
[0m15:40:41.991934 [debug] [MainThread]: Using duckdb connection "master"
[0m15:40:41.992274 [debug] [MainThread]: On master: BEGIN
[0m15:40:41.992464 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:40:41.992844 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:40:41.993029 [debug] [MainThread]: On master: COMMIT
[0m15:40:41.993191 [debug] [MainThread]: Using duckdb connection "master"
[0m15:40:41.993354 [debug] [MainThread]: On master: COMMIT
[0m15:40:41.993615 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:40:41.993800 [debug] [MainThread]: On master: Close
[0m15:40:41.994067 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:40:41.994240 [debug] [MainThread]: Connection 'model.quant_features.alpha_factors_081_101' was properly closed.
[0m15:40:41.994414 [info ] [MainThread]: 
[0m15:40:41.994588 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.14 seconds (0.14s).
[0m15:40:41.994967 [debug] [MainThread]: Command end result
[0m15:40:42.016431 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:40:42.017549 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:40:42.021160 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:40:42.021396 [info ] [MainThread]: 
[0m15:40:42.021615 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:40:42.021791 [info ] [MainThread]: 
[0m15:40:42.021977 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m15:40:42.022514 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.82484794, "process_in_blocks": "0", "process_kernel_time": 0.169629, "process_mem_max_rss": "162092", "process_out_blocks": "5464", "process_user_time": 1.538535}
[0m15:40:42.022796 [debug] [MainThread]: Command `dbt run` succeeded at 15:40:42.022738 after 0.83 seconds
[0m15:40:42.023001 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7f0dfbd80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7eb707c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7eb707bb0>]}
[0m15:40:42.023209 [debug] [MainThread]: Flushing usage events
[0m15:40:42.066908 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:41:25.385104 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2170b03770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f217214da90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f216f973d90>]}


============================== 15:41:25.387512 | de0305f4-5b75-4d40-88da-b460c0d3f48c ==============================
[0m15:41:25.387512 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:41:25.387840 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'log_cache_events': 'False', 'log_format': 'default', 'empty': 'False', 'introspect': 'True', 'warn_error': 'None', 'write_json': 'True', 'static_parser': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select alpha101_complete_fixed', 'version_check': 'True', 'use_experimental_parser': 'False', 'log_path': '/workspace/dbt_project/logs', 'target_path': 'None', 'printer_width': '80', 'partial_parse': 'True', 'use_colors': 'True', 'profiles_dir': '/workspace/dbt_project', 'quiet': 'False'}
[0m15:41:25.532403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'de0305f4-5b75-4d40-88da-b460c0d3f48c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f21707ef360>]}
[0m15:41:25.574862 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'de0305f4-5b75-4d40-88da-b460c0d3f48c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f216fa66be0>]}
[0m15:41:25.575859 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:41:25.607533 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:41:25.697284 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m15:41:25.697707 [debug] [MainThread]: Partial parsing: added file: quant_features://models/alpha101/alpha101_complete_fixed.sql
[0m15:41:25.889059 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'de0305f4-5b75-4d40-88da-b460c0d3f48c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f216b551850>]}
[0m15:41:25.987913 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:41:25.989167 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:41:26.000017 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'de0305f4-5b75-4d40-88da-b460c0d3f48c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f216c2b4c80>]}
[0m15:41:26.000438 [info ] [MainThread]: Found 24 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:41:26.000684 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'de0305f4-5b75-4d40-88da-b460c0d3f48c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f216b9e0590>]}
[0m15:41:26.001929 [info ] [MainThread]: 
[0m15:41:26.002183 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:41:26.002373 [info ] [MainThread]: 
[0m15:41:26.002652 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:41:26.003539 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features'
[0m15:41:26.018112 [debug] [ThreadPool]: Using duckdb connection "list_quant_features"
[0m15:41:26.018382 [debug] [ThreadPool]: On list_quant_features: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"quant_features"'
    
  
  
[0m15:41:26.018567 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:41:26.033302 [debug] [ThreadPool]: SQL status: OK in 0.015 seconds
[0m15:41:26.034185 [debug] [ThreadPool]: On list_quant_features: Close
[0m15:41:26.034861 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_quant_features, now create_quant_features_main)
[0m15:41:26.035150 [debug] [ThreadPool]: Creating schema "database: "quant_features"
schema: "main"
"
[0m15:41:26.039236 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:41:26.039502 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='quant_features'
        and type='sqlite'
    
  
[0m15:41:26.039671 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:41:26.040355 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:41:26.041236 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:41:26.041459 [debug] [ThreadPool]: On create_quant_features_main: BEGIN
[0m15:41:26.041779 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:41:26.041958 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:41:26.042119 [debug] [ThreadPool]: On create_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "create_quant_features_main"} */

    
    
        create schema if not exists "quant_features"."main"
    
[0m15:41:26.042424 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:41:26.042940 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:41:26.043126 [debug] [ThreadPool]: Using duckdb connection "create_quant_features_main"
[0m15:41:26.043294 [debug] [ThreadPool]: On create_quant_features_main: COMMIT
[0m15:41:26.043572 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:41:26.043744 [debug] [ThreadPool]: On create_quant_features_main: Close
[0m15:41:26.048589 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_quant_features_main, now list_quant_features_main)
[0m15:41:26.052283 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:41:26.052888 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:41:26.053147 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:41:26.053567 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:41:26.053768 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:41:26.053928 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:41:26.060508 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m15:41:26.061483 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:41:26.062069 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:41:26.062281 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:41:26.064498 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'de0305f4-5b75-4d40-88da-b460c0d3f48c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f216a1061a0>]}
[0m15:41:26.064895 [debug] [MainThread]: Using duckdb connection "master"
[0m15:41:26.065092 [debug] [MainThread]: On master: BEGIN
[0m15:41:26.065248 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:41:26.065652 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:41:26.065841 [debug] [MainThread]: On master: COMMIT
[0m15:41:26.065995 [debug] [MainThread]: Using duckdb connection "master"
[0m15:41:26.066138 [debug] [MainThread]: On master: COMMIT
[0m15:41:26.066392 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:41:26.066577 [debug] [MainThread]: On master: Close
[0m15:41:26.068474 [debug] [Thread-1 (]: Began running node model.quant_features.alpha101_complete_fixed
[0m15:41:26.068845 [info ] [Thread-1 (]: 1 of 1 START sql table model main.alpha101_complete_fixed ...................... [RUN]
[0m15:41:26.069130 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.alpha101_complete_fixed)
[0m15:41:26.069342 [debug] [Thread-1 (]: Began compiling node model.quant_features.alpha101_complete_fixed
[0m15:41:26.075024 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.alpha101_complete_fixed"
[0m15:41:26.075466 [debug] [Thread-1 (]: Began executing node model.quant_features.alpha101_complete_fixed
[0m15:41:26.097520 [debug] [Thread-1 (]: Writing runtime sql for node "model.quant_features.alpha101_complete_fixed"
[0m15:41:26.097991 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha101_complete_fixed"
[0m15:41:26.098215 [debug] [Thread-1 (]: On model.quant_features.alpha101_complete_fixed: BEGIN
[0m15:41:26.098410 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:41:26.098905 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:41:26.099145 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha101_complete_fixed"
[0m15:41:26.099604 [debug] [Thread-1 (]: On model.quant_features.alpha101_complete_fixed: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha101_complete_fixed"} */

  
    
    

    create  table
      "quant_features"."main"."alpha101_complete_fixed__dbt_tmp"
  
    as (
      

-- Alpha 101 完整因子汇总（修正版）
-- 整合所有101个Alpha因子

WITH factors_basic AS (
    SELECT * FROM "quant_features"."main"."alpha_factors_basic"
),

factors_021_040 AS (
    SELECT * FROM "quant_features"."main"."alpha_factors_021_040"
),

factors_041_060 AS (
    SELECT * FROM "quant_features"."main"."alpha_factors_041_060"
),

factors_061_080 AS (
    SELECT * FROM "quant_features"."main"."alpha_factors_061_080"
),

factors_081_101 AS (
    SELECT * FROM "quant_features"."main"."alpha_factors_081_101"
),

-- 合并所有Alpha因子
all_alpha_factors AS (
    SELECT 
        a.symbol,
        a.timestamp,
        
        -- 基础市场数据
        a.open, a.high, a.low, a.close, a.volume, a.vwap, a.returns,
        
        -- Alpha 001-020 (来自基础模型)
        a.alpha001, a.alpha002, a.alpha003, a.alpha004, a.alpha005,
        a.alpha006, a.alpha007, a.alpha008, a.alpha009, a.alpha010,
        a.alpha011, a.alpha012, a.alpha013, a.alpha014, a.alpha015,
        a.alpha016, a.alpha017, a.alpha018, a.alpha019, a.alpha020,
        
        -- Alpha 021-040
        b.alpha021, b.alpha022, b.alpha023, b.alpha024, b.alpha025,
        b.alpha026, b.alpha027, b.alpha028, b.alpha029, b.alpha030,
        b.alpha031, b.alpha032, b.alpha033, b.alpha034, b.alpha035,
        b.alpha036, b.alpha037, b.alpha038, b.alpha039, b.alpha040,
        
        -- Alpha 041-060
        c.alpha041, c.alpha042, c.alpha043, c.alpha044, c.alpha045,
        c.alpha046, c.alpha047, c.alpha048, c.alpha049, c.alpha050,
        c.alpha051, c.alpha052, c.alpha053, c.alpha054, c.alpha055,
        c.alpha056, c.alpha057, c.alpha058, c.alpha059, c.alpha060,
        
        -- Alpha 061-080
        d.alpha061, d.alpha062, d.alpha063, d.alpha064, d.alpha065,
        d.alpha066, d.alpha067, d.alpha068, d.alpha069, d.alpha070,
        d.alpha071, d.alpha072, d.alpha073, d.alpha074, d.alpha075,
        d.alpha076, d.alpha077, d.alpha078, d.alpha079, d.alpha080,
        
        -- Alpha 081-101
        e.alpha081, e.alpha082, e.alpha083, e.alpha084, e.alpha085,
        e.alpha086, e.alpha087, e.alpha088, e.alpha089, e.alpha090,
        e.alpha091, e.alpha092, e.alpha093, e.alpha094, e.alpha095,
        e.alpha096, e.alpha097, e.alpha098, e.alpha099, e.alpha100,
        e.alpha101
        
    FROM factors_basic a
    LEFT JOIN factors_021_040 b ON a.symbol = b.symbol AND a.timestamp = b.timestamp
    LEFT JOIN factors_041_060 c ON a.symbol = c.symbol AND a.timestamp = c.timestamp
    LEFT JOIN factors_061_080 d ON a.symbol = d.symbol AND a.timestamp = d.timestamp
    LEFT JOIN factors_081_101 e ON a.symbol = e.symbol AND a.timestamp = e.timestamp
),

-- 因子统计和组合
factor_summary AS (
    SELECT 
        *,
        
        -- 计算有效因子数量
        (
            CASE WHEN alpha001 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha002 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha003 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha004 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha005 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha006 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha007 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha008 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha009 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha010 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha011 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha012 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha013 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha014 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha015 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha016 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha017 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha018 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha019 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha020 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha021 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha022 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha023 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha024 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha025 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha026 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha027 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha028 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha029 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha030 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha031 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha032 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha033 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha034 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha035 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha036 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha037 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha038 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha039 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha040 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha041 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha042 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha043 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha044 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha045 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha046 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha047 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha048 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha049 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha050 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha051 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha052 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha053 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha054 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha055 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha056 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha057 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha058 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha059 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha060 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha061 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha062 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha063 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha064 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha065 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha066 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha067 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha068 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha069 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha070 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha071 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha072 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha073 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha074 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha075 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha076 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha077 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha078 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha079 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha080 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha081 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha082 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha083 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha084 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha085 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha086 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha087 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha088 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha089 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha090 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha091 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha092 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha093 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha094 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha095 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha096 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha097 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha098 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha099 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha100 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha101 IS NOT NULL THEN 1 ELSE 0 END
        ) AS total_valid_factors,
        
        -- 因子组合
        (COALESCE(alpha001, 0) + COALESCE(alpha012, 0) + COALESCE(alpha019, 0) +
         COALESCE(alpha037, 0) + COALESCE(alpha065, 0)) / 5 AS momentum_composite,
        
        (COALESCE(alpha003, 0) + COALESCE(alpha004, 0) + COALESCE(alpha009, 0) +
         COALESCE(alpha023, 0) + COALESCE(alpha051, 0)) / 5 AS reversal_composite,
        
        (COALESCE(alpha006, 0) + COALESCE(alpha013, 0) + COALESCE(alpha025, 0) +
         COALESCE(alpha044, 0) + COALESCE(alpha075, 0)) / 5 AS volume_composite,
        
        -- Feast集成字段
        timestamp AS event_timestamp,
        CURRENT_TIMESTAMP AS created_at
        
    FROM all_alpha_factors
)

SELECT * FROM factor_summary
    );
  
  
[0m15:41:26.122414 [debug] [Thread-1 (]: SQL status: OK in 0.022 seconds
[0m15:41:26.126431 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha101_complete_fixed"
[0m15:41:26.126718 [debug] [Thread-1 (]: On model.quant_features.alpha101_complete_fixed: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha101_complete_fixed"} */
alter table "quant_features"."main"."alpha101_complete_fixed__dbt_tmp" rename to "alpha101_complete_fixed"
[0m15:41:26.127252 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:41:26.135741 [debug] [Thread-1 (]: On model.quant_features.alpha101_complete_fixed: COMMIT
[0m15:41:26.136021 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha101_complete_fixed"
[0m15:41:26.136214 [debug] [Thread-1 (]: On model.quant_features.alpha101_complete_fixed: COMMIT
[0m15:41:26.139098 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m15:41:26.142505 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha101_complete_fixed"
[0m15:41:26.142777 [debug] [Thread-1 (]: On model.quant_features.alpha101_complete_fixed: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha101_complete_fixed"} */

      drop table if exists "quant_features"."main"."alpha101_complete_fixed__dbt_backup" cascade
    
[0m15:41:26.143177 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:41:26.144700 [debug] [Thread-1 (]: On model.quant_features.alpha101_complete_fixed: Close
[0m15:41:26.146546 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'de0305f4-5b75-4d40-88da-b460c0d3f48c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f216a2ede50>]}
[0m15:41:26.146990 [info ] [Thread-1 (]: 1 of 1 OK created sql table model main.alpha101_complete_fixed ................. [[32mOK[0m in 0.08s]
[0m15:41:26.147342 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha101_complete_fixed
[0m15:41:26.148695 [debug] [MainThread]: Using duckdb connection "master"
[0m15:41:26.148950 [debug] [MainThread]: On master: BEGIN
[0m15:41:26.149120 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:41:26.149493 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:41:26.149682 [debug] [MainThread]: On master: COMMIT
[0m15:41:26.149849 [debug] [MainThread]: Using duckdb connection "master"
[0m15:41:26.149998 [debug] [MainThread]: On master: COMMIT
[0m15:41:26.150319 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:41:26.150510 [debug] [MainThread]: On master: Close
[0m15:41:26.150781 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:41:26.150941 [debug] [MainThread]: Connection 'model.quant_features.alpha101_complete_fixed' was properly closed.
[0m15:41:26.151105 [info ] [MainThread]: 
[0m15:41:26.151288 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.15 seconds (0.15s).
[0m15:41:26.151651 [debug] [MainThread]: Command end result
[0m15:41:26.173060 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:41:26.174225 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:41:26.177908 [debug] [MainThread]: Wrote artifact RunExecutionResult to /workspace/dbt_project/target/run_results.json
[0m15:41:26.178139 [info ] [MainThread]: 
[0m15:41:26.178374 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:41:26.178543 [info ] [MainThread]: 
[0m15:41:26.178718 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m15:41:26.179264 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.83010715, "process_in_blocks": "0", "process_kernel_time": 0.120653, "process_mem_max_rss": "164788", "process_out_blocks": "5672", "process_user_time": 1.58866}
[0m15:41:26.179567 [debug] [MainThread]: Command `dbt run` succeeded at 15:41:26.179507 after 0.83 seconds
[0m15:41:26.179779 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f216b403e30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f216a007f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f216a007ed0>]}
[0m15:41:26.179988 [debug] [MainThread]: Flushing usage events
[0m15:41:26.235085 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:41:33.394784 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65384c3770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6539b09a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6537337d90>]}


============================== 15:41:33.397048 | 1f691fa1-e356-462d-8e36-c8258ceb1f07 ==============================
[0m15:41:33.397048 [info ] [MainThread]: Running with dbt=1.10.9
[0m15:41:33.397380 [debug] [MainThread]: running dbt with arguments {'empty': 'None', 'profiles_dir': '/workspace/dbt_project', 'target_path': 'None', 'cache_selected_only': 'False', 'send_anonymous_usage_stats': 'True', 'log_path': '/workspace/dbt_project/logs', 'use_experimental_parser': 'False', 'debug': 'False', 'log_format': 'default', 'version_check': 'True', 'fail_fast': 'False', 'introspect': 'True', 'quiet': 'False', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'no_print': 'None', 'invocation_command': 'dbt show --select alpha101_complete_fixed --limit 3', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'printer_width': '80', 'write_json': 'True', 'warn_error': 'None', 'partial_parse': 'True'}
[0m15:41:33.541735 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1f691fa1-e356-462d-8e36-c8258ceb1f07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65381b7360>]}
[0m15:41:33.583936 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1f691fa1-e356-462d-8e36-c8258ceb1f07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f653742abe0>]}
[0m15:41:33.591896 [info ] [MainThread]: Registered adapter: duckdb=1.9.4
[0m15:41:33.624558 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m15:41:33.714506 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:41:33.714746 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:41:33.751973 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1f691fa1-e356-462d-8e36-c8258ceb1f07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65332d5150>]}
[0m15:41:33.822407 [debug] [MainThread]: Wrote artifact WritableManifest to /workspace/dbt_project/target/manifest.json
[0m15:41:33.823602 [debug] [MainThread]: Wrote artifact SemanticManifest to /workspace/dbt_project/target/semantic_manifest.json
[0m15:41:33.829980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1f691fa1-e356-462d-8e36-c8258ceb1f07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65332c59a0>]}
[0m15:41:33.830327 [info ] [MainThread]: Found 24 models, 2 seeds, 37 data tests, 1 source, 565 macros
[0m15:41:33.830536 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1f691fa1-e356-462d-8e36-c8258ceb1f07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6537383cb0>]}
[0m15:41:33.831744 [info ] [MainThread]: 
[0m15:41:33.832002 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:41:33.832175 [info ] [MainThread]: 
[0m15:41:33.832499 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:41:33.836598 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_quant_features_main'
[0m15:41:33.881844 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:41:33.882103 [debug] [ThreadPool]: On list_quant_features_main: BEGIN
[0m15:41:33.882288 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:41:33.897366 [debug] [ThreadPool]: SQL status: OK in 0.015 seconds
[0m15:41:33.897626 [debug] [ThreadPool]: Using duckdb connection "list_quant_features_main"
[0m15:41:33.897810 [debug] [ThreadPool]: On list_quant_features_main: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "connection_name": "list_quant_features_main"} */
select
      'quant_features' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'quant_features'
  
[0m15:41:33.904963 [debug] [ThreadPool]: SQL status: OK in 0.007 seconds
[0m15:41:33.905994 [debug] [ThreadPool]: On list_quant_features_main: ROLLBACK
[0m15:41:33.906588 [debug] [ThreadPool]: Failed to rollback 'list_quant_features_main'
[0m15:41:33.906794 [debug] [ThreadPool]: On list_quant_features_main: Close
[0m15:41:33.908989 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1f691fa1-e356-462d-8e36-c8258ceb1f07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6532d18a10>]}
[0m15:41:33.911523 [debug] [Thread-1 (]: Began running node model.quant_features.alpha101_complete_fixed
[0m15:41:33.911892 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_quant_features_main, now model.quant_features.alpha101_complete_fixed)
[0m15:41:33.912118 [debug] [Thread-1 (]: Began compiling node model.quant_features.alpha101_complete_fixed
[0m15:41:33.918242 [debug] [Thread-1 (]: Writing injected SQL for node "model.quant_features.alpha101_complete_fixed"
[0m15:41:33.918718 [debug] [Thread-1 (]: Began executing node model.quant_features.alpha101_complete_fixed
[0m15:41:33.922894 [debug] [Thread-1 (]: Using duckdb connection "model.quant_features.alpha101_complete_fixed"
[0m15:41:33.923449 [debug] [Thread-1 (]: On model.quant_features.alpha101_complete_fixed: /* {"app": "dbt", "dbt_version": "1.10.9", "profile_name": "quant_features", "target_name": "dev", "node_id": "model.quant_features.alpha101_complete_fixed"} */

  
  

-- Alpha 101 完整因子汇总（修正版）
-- 整合所有101个Alpha因子

WITH factors_basic AS (
    SELECT * FROM "quant_features"."main"."alpha_factors_basic"
),

factors_021_040 AS (
    SELECT * FROM "quant_features"."main"."alpha_factors_021_040"
),

factors_041_060 AS (
    SELECT * FROM "quant_features"."main"."alpha_factors_041_060"
),

factors_061_080 AS (
    SELECT * FROM "quant_features"."main"."alpha_factors_061_080"
),

factors_081_101 AS (
    SELECT * FROM "quant_features"."main"."alpha_factors_081_101"
),

-- 合并所有Alpha因子
all_alpha_factors AS (
    SELECT 
        a.symbol,
        a.timestamp,
        
        -- 基础市场数据
        a.open, a.high, a.low, a.close, a.volume, a.vwap, a.returns,
        
        -- Alpha 001-020 (来自基础模型)
        a.alpha001, a.alpha002, a.alpha003, a.alpha004, a.alpha005,
        a.alpha006, a.alpha007, a.alpha008, a.alpha009, a.alpha010,
        a.alpha011, a.alpha012, a.alpha013, a.alpha014, a.alpha015,
        a.alpha016, a.alpha017, a.alpha018, a.alpha019, a.alpha020,
        
        -- Alpha 021-040
        b.alpha021, b.alpha022, b.alpha023, b.alpha024, b.alpha025,
        b.alpha026, b.alpha027, b.alpha028, b.alpha029, b.alpha030,
        b.alpha031, b.alpha032, b.alpha033, b.alpha034, b.alpha035,
        b.alpha036, b.alpha037, b.alpha038, b.alpha039, b.alpha040,
        
        -- Alpha 041-060
        c.alpha041, c.alpha042, c.alpha043, c.alpha044, c.alpha045,
        c.alpha046, c.alpha047, c.alpha048, c.alpha049, c.alpha050,
        c.alpha051, c.alpha052, c.alpha053, c.alpha054, c.alpha055,
        c.alpha056, c.alpha057, c.alpha058, c.alpha059, c.alpha060,
        
        -- Alpha 061-080
        d.alpha061, d.alpha062, d.alpha063, d.alpha064, d.alpha065,
        d.alpha066, d.alpha067, d.alpha068, d.alpha069, d.alpha070,
        d.alpha071, d.alpha072, d.alpha073, d.alpha074, d.alpha075,
        d.alpha076, d.alpha077, d.alpha078, d.alpha079, d.alpha080,
        
        -- Alpha 081-101
        e.alpha081, e.alpha082, e.alpha083, e.alpha084, e.alpha085,
        e.alpha086, e.alpha087, e.alpha088, e.alpha089, e.alpha090,
        e.alpha091, e.alpha092, e.alpha093, e.alpha094, e.alpha095,
        e.alpha096, e.alpha097, e.alpha098, e.alpha099, e.alpha100,
        e.alpha101
        
    FROM factors_basic a
    LEFT JOIN factors_021_040 b ON a.symbol = b.symbol AND a.timestamp = b.timestamp
    LEFT JOIN factors_041_060 c ON a.symbol = c.symbol AND a.timestamp = c.timestamp
    LEFT JOIN factors_061_080 d ON a.symbol = d.symbol AND a.timestamp = d.timestamp
    LEFT JOIN factors_081_101 e ON a.symbol = e.symbol AND a.timestamp = e.timestamp
),

-- 因子统计和组合
factor_summary AS (
    SELECT 
        *,
        
        -- 计算有效因子数量
        (
            CASE WHEN alpha001 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha002 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha003 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha004 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha005 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha006 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha007 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha008 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha009 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha010 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha011 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha012 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha013 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha014 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha015 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha016 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha017 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha018 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha019 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha020 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha021 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha022 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha023 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha024 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha025 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha026 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha027 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha028 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha029 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha030 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha031 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha032 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha033 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha034 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha035 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha036 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha037 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha038 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha039 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha040 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha041 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha042 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha043 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha044 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha045 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha046 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha047 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha048 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha049 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha050 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha051 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha052 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha053 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha054 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha055 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha056 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha057 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha058 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha059 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha060 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha061 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha062 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha063 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha064 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha065 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha066 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha067 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha068 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha069 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha070 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha071 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha072 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha073 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha074 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha075 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha076 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha077 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha078 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha079 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha080 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha081 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha082 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha083 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha084 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha085 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha086 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha087 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha088 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha089 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha090 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha091 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha092 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha093 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha094 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha095 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha096 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha097 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha098 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha099 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha100 IS NOT NULL THEN 1 ELSE 0 END +
            CASE WHEN alpha101 IS NOT NULL THEN 1 ELSE 0 END
        ) AS total_valid_factors,
        
        -- 因子组合
        (COALESCE(alpha001, 0) + COALESCE(alpha012, 0) + COALESCE(alpha019, 0) +
         COALESCE(alpha037, 0) + COALESCE(alpha065, 0)) / 5 AS momentum_composite,
        
        (COALESCE(alpha003, 0) + COALESCE(alpha004, 0) + COALESCE(alpha009, 0) +
         COALESCE(alpha023, 0) + COALESCE(alpha051, 0)) / 5 AS reversal_composite,
        
        (COALESCE(alpha006, 0) + COALESCE(alpha013, 0) + COALESCE(alpha025, 0) +
         COALESCE(alpha044, 0) + COALESCE(alpha075, 0)) / 5 AS volume_composite,
        
        -- Feast集成字段
        timestamp AS event_timestamp,
        CURRENT_TIMESTAMP AS created_at
        
    FROM all_alpha_factors
)

SELECT * FROM factor_summary
  
  limit 3

[0m15:41:33.923960 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:41:33.943771 [debug] [Thread-1 (]: SQL status: OK in 0.020 seconds
[0m15:41:33.951351 [debug] [Thread-1 (]: On model.quant_features.alpha101_complete_fixed: Close
[0m15:41:33.952336 [error] [Thread-1 (]: [31mUnhandled error while executing [0m
'/UTC'
[0m15:41:33.953926 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "/workspace/dbt_env/lib/python3.13/site-packages/dbt/task/base.py", line 363, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/workspace/dbt_env/lib/python3.13/site-packages/dbt/task/base.py", line 310, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/workspace/dbt_env/lib/python3.13/site-packages/dbt/task/base.py", line 410, in run
    return self.execute(compiled_node, manifest)
           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/dbt_env/lib/python3.13/site-packages/dbt/task/show.py", line 42, in execute
    adapter_response, execute_result = self.adapter.execute(
                                       ~~~~~~~~~~~~~~~~~~~~^
        compiled_node.compiled_code, fetch=True
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/workspace/dbt_env/lib/python3.13/site-packages/dbt_common/record.py", line 507, in record_replay_wrapper
    return func_to_record(*call_args, **kwargs)
  File "/workspace/dbt_env/lib/python3.13/site-packages/dbt/adapters/base/impl.py", line 449, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/dbt_env/lib/python3.13/site-packages/dbt/adapters/duckdb/connections.py", line 133, in execute
    return super().execute(sql, auto_begin, fetch, limit)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/dbt_env/lib/python3.13/site-packages/dbt/adapters/sql/connections.py", line 222, in execute
    table = self.get_result_from_cursor(cursor, limit)
  File "/workspace/dbt_env/lib/python3.13/site-packages/dbt/adapters/sql/connections.py", line 204, in get_result_from_cursor
    rows = cursor.fetchall()
  File "/workspace/dbt_env/lib/python3.13/site-packages/pytz/__init__.py", line 188, in timezone
    raise UnknownTimeZoneError(zone)
pytz.exceptions.UnknownTimeZoneError: '/UTC'

[0m15:41:33.954417 [debug] [Thread-1 (]: Finished running node model.quant_features.alpha101_complete_fixed
[0m15:41:33.955111 [debug] [Thread-7 (]: Marking all children of 'model.quant_features.alpha101_complete_fixed' to be skipped because of status 'error'.  Reason: '/UTC'.
[0m15:41:33.956002 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:41:33.956310 [debug] [MainThread]: Connection 'model.quant_features.alpha101_complete_fixed' was properly closed.
[0m15:41:33.956620 [error] [MainThread]: Encountered an error:
Runtime Error
  '/UTC'
[0m15:41:33.957444 [debug] [MainThread]: Resource report: {"command_name": "show", "command_success": false, "command_wall_clock_time": 0.5985954, "process_in_blocks": "0", "process_kernel_time": 0.154882, "process_mem_max_rss": "160784", "process_out_blocks": "2176", "process_user_time": 1.314498}
[0m15:41:33.957781 [debug] [MainThread]: Command `dbt show` failed at 15:41:33.957717 after 0.60 seconds
[0m15:41:33.958000 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65376cb110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6532eaa360>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6532eaa2b0>]}
[0m15:41:33.958217 [debug] [MainThread]: Flushing usage events
[0m15:41:34.007570 [debug] [MainThread]: An error was encountered while trying to flush usage events
